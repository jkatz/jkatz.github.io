<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Postgres on Jonathan Katz</title><link>https://jkatz.github.io/post/postgres/</link><description>Recent content in Posts on Postgres on Jonathan Katz</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 30 Apr 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://jkatz.github.io/post/postgres/index.xml" rel="self" type="application/rss+xml"/><item><title>The 150x pgvector speedup: a year-in-review</title><link>https://jkatz.github.io/post/postgres/pgvector-performance-150x-speedup/</link><pubDate>Tue, 30 Apr 2024 00:00:00 +0000</pubDate><guid>https://jkatz.github.io/post/postgres/pgvector-performance-150x-speedup/</guid><description>&lt;p>I wanted to write a &amp;ldquo;&lt;a href="https://jkatz.github.io/post/postgres/vectors-json-postgresql/">year-in-review&lt;/a>&amp;rdquo; covering all the performance &lt;a href="https://github.com/pgvector/pgvector/">pgvector&lt;/a> has made (with significant credit to &lt;a href="https://github.com/ankane">Andrew Kane&lt;/a>), highlighting specific areas where pgvector has improved (including one 150x improvement!) and areas where we can continue to do better.&lt;/p>
&lt;p>A few weeks ago, I started outlining this post and began my data collection. While I was working on this over a two week period, no fewer than three competitive benchmarks against pgvector published. To me, this is a testament both how well pgvector is at handling vector workloads (and by extension, PostgreSQL too) that people are using it as the baseline to compare it to their vector search systems.&lt;/p>
&lt;p>Some of these benchmarks did contain info that identified areas we can continue to improve both PostgreSQL and pgvector, but I was generally disappointed in the methodology used to make these comparisons. Of course I&amp;rsquo;d like to see pgvector perform well in benchmarks, but it&amp;rsquo;s important to position technologies fairly and be vocally self-critical on where your system can improve to build trust in what you&amp;rsquo;re building.&lt;/p>
&lt;p>I have a separate blog post planned for how to best present benchmark studies between different systems for vector similarity search (&lt;a href="https://jkatz.github.io/post/postgres/pgvector-hnsw-performance/">it&amp;rsquo;s a topic I&amp;rsquo;m interested in&lt;/a>). Today though, I want to compare pgvector against itself, and highlight areas it&amp;rsquo;s improved over the past year, and where the project can continue to go and grow.&lt;/p>
&lt;h2 id="how-i-ran-these-tests">How I ran these tests&lt;/h2>
&lt;p>An important aspect of any benchmark is transparency. First, I&amp;rsquo;ll discuss the test methodology I used, describe the test environment setup (instances, storage, database configuration), and then discuss the results. If you&amp;rsquo;re not interested in this part, you can skip ahead to &amp;ldquo;The 150x pgvector speedup&amp;rdquo;, but this information can help you with your own testing!&lt;/p>
&lt;p>First, what are testing for? We&amp;rsquo;ll be looking at these specific attributes in these tests:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://en.wikipedia.org/wiki/Precision_and_recall">&lt;strong>Recall&lt;/strong>&lt;/a>: A measurement of the relevancy of our results - what percentage of the expected results are returned during a vector search? Arguably, this is the most important measurement - it doesn&amp;rsquo;t matter if you have the highest query throughput if your recall is poor.&lt;/li>
&lt;li>&lt;strong>Storage size&lt;/strong>: This could be related to storing your original vector/associated data, and any data you store in a vector index. Because PostgreSQL is a database, at a minimum you&amp;rsquo;ll have to store the vector in the table, and pay additional storage costs for building a vector index.&lt;/li>
&lt;li>&lt;strong>Load time / index build time&lt;/strong>: How long does it take to load your vector data into an existing index? If your data is preloaded, how long does it take to build an index? Spending more time building your index can help improve both recall and query performance, but this is often the most expensive part of a vector database and can impact overall system performance.&lt;/li>
&lt;li>&lt;strong>Latency (p99)&lt;/strong>: Specifically, how long it takes to return a single result, but representing the 99th percentile (&amp;ldquo;very slow&amp;rdquo;) queries. This serves as an &amp;ldquo;upper bound&amp;rdquo; on latency times.&lt;/li>
&lt;li>&lt;strong>Single-connection Throughput / queries per second (QPS)&lt;/strong>: How many queries can be executed each second? This impacts how much load you can put on a single system.&lt;/li>
&lt;/ul>
&lt;p>(More on the &amp;ldquo;single-connection&amp;rdquo; distinction in a future blog post).&lt;/p>
&lt;p>This is a &amp;ldquo;year-in-review&amp;rdquo; post, so I ran tests against the following releases and configurations of pgvector. I&amp;rsquo;m including the shorthand that I&amp;rsquo;ll show in the tests results.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>pgvector version&lt;/th>
&lt;th>Index type&lt;/th>
&lt;th>Test name (r7gd)&lt;/th>
&lt;th>Test name (r7i)&lt;/th>
&lt;th>Notes&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0.4.1&lt;/td>
&lt;td>IVFFlat&lt;/td>
&lt;td>&lt;code>r7gd.041&lt;/code>&lt;/td>
&lt;td>&lt;code>r7i.041&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>0.4.4&lt;/td>
&lt;td>IVFFlat&lt;/td>
&lt;td>&lt;code>r7gd.044&lt;/code>&lt;/td>
&lt;td>&lt;code>r7i.044&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>0.5.0&lt;/td>
&lt;td>HNSW&lt;/td>
&lt;td>&lt;code>r7gd.050&lt;/code>&lt;/td>
&lt;td>&lt;code>r7i.050&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>0.5.1&lt;/td>
&lt;td>HNSW&lt;/td>
&lt;td>&lt;code>r7gd.051&lt;/code>&lt;/td>
&lt;td>&lt;code>r7i.051&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>0.6.0&lt;/td>
&lt;td>HNSW&lt;/td>
&lt;td>&lt;code>r7gd.060&lt;/code>&lt;/td>
&lt;td>&lt;code>r7i.060&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>0.6.2&lt;/td>
&lt;td>HNSW&lt;/td>
&lt;td>&lt;code>r7gd.062&lt;/code>&lt;/td>
&lt;td>&lt;code>r7i.062&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>0.7.0&lt;/td>
&lt;td>HNSW&lt;/td>
&lt;td>&lt;code>r7gd.070&lt;/code>&lt;/td>
&lt;td>&lt;code>r7i.070&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>0.7.0&lt;/td>
&lt;td>HNSW - SQ16&lt;/td>
&lt;td>&lt;code>r7gd.070.fp16&lt;/code>&lt;/td>
&lt;td>&lt;code>r7i.070.fp16&lt;/code>&lt;/td>
&lt;td>Stores 2-byte float representation of vectors in the index&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>0.7.0&lt;/td>
&lt;td>HNSW - BQ + Jaccard rerank&lt;/td>
&lt;td>&lt;code>r7gd.070.bq-jaccard-rerank&lt;/code>&lt;/td>
&lt;td>&lt;code>r7i.070.bq-jaccard-rerank&lt;/code>&lt;/td>
&lt;td>Stores binary representation of vectors in index using Jaccard distance; results are re-ranked using original vector after the index search&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>0.7.0&lt;/td>
&lt;td>HNSW - BQ + Hamming rerank&lt;/td>
&lt;td>&lt;code>r7gd.070.bq-jaccard-rerank&lt;/code>&lt;/td>
&lt;td>&lt;code>r7i.070.bq-jaccard-rerank&lt;/code>&lt;/td>
&lt;td>Stores binary representation of vectors in index using Hamming distance; results are re-ranked using original vector after the index search&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="test-setup">Test setup&lt;/h3>
&lt;p>To simplify the comparison, I kept the index build parameters the same for all of the tests. Adjusting build parameters can impact all five of the key metrics (please see &lt;a href="https://jkatz.github.io/tags/pgvector/">previous posts&lt;/a> and &lt;a href="https://jkatz.github.io/talks/">talks&lt;/a>), but the purpose of this blog post is to show how pgvector has evolved over the past year and choosing a fixed set of parameters does serve to show how it&amp;rsquo;s improved and where it can grow. Below are the build parameters used for each index type:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Index type&lt;/th>
&lt;th>Build parameters&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>IVFFlat&lt;/td>
&lt;td>&lt;code>lists&lt;/code>: &lt;code>1000&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>HNSW&lt;/td>
&lt;td>&lt;code>m&lt;/code>: &lt;code>16&lt;/code>; &lt;code>ef_construction&lt;/code>: &lt;code>256&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>For the testing, I used a &lt;code>r7gd.16xlarge&lt;/code> and a &lt;code>r7i.16xlarge&lt;/code>, both of which have 64 vCPU and 512GiB of RAM. I stored the data on the local NVMe on the &lt;code>r7gd&lt;/code>, and on &lt;code>gp3&lt;/code> storage for the &lt;code>r7i&lt;/code>. If this test was looking at behaviors around storage, that fact would matter heavily, but these tests focused specifically on CPU and memory characteristics.&lt;/p>
&lt;p>For these tests, I used PostgreSQL 16.2 (aside: the upcoming PostgreSQL 17 release is expected to have the ability to utilize AVX-512 SIMD instructions for the &lt;code>pg_popcount&lt;/code> function, used by the Jaccard distance; this doesn&amp;rsquo;t account for those optimizations) with the following configurations, using parallelism where available:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>checkpoint_timeout&lt;/td>
&lt;td>2h&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>effective_cache_size&lt;/td>
&lt;td>256GB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>jit&lt;/td>
&lt;td>off&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>maintenance_work_mem&lt;/td>
&lt;td>64GB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>max_parallel_maintenance_workers&lt;/td>
&lt;td>63&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>max_parallel_workers&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>max_parallel_workers_per_gather&lt;/td>
&lt;td>64&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>max_wal_size&lt;/td>
&lt;td>20GB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>max_worker_processes&lt;/td>
&lt;td>128&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>shared_buffers&lt;/td>
&lt;td>128GB&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>wal_compression&lt;/td>
&lt;td>zstd&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>work_mem&lt;/td>
&lt;td>64MB&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>I used the &lt;a href="https://github.com/erikbern/ann-benchmarks">ANN Benchmark&lt;/a> framework to run the tests. I made the following modifications to the &lt;code>pgvector&lt;/code> module:&lt;/p>
&lt;ul>
&lt;li>I commented out the &lt;a href="https://github.com/erikbern/ann-benchmarks/blob/75043ab482d91afae82ac4033cbdd98997121d58/ann_benchmarks/algorithms/pgvector/module.py#L25">&lt;code>subprocess.run&lt;/code>&lt;/a> line so I could run the test in a local process, not a container&lt;/li>
&lt;li>I added modules to run both the new work &lt;a href="https://jkatz.github.io/post/postgres/pgvector-scalar-binary-quantization/">scalar/binary quantization&lt;/a> code in v0.7.0 (&lt;a href="https://jkatz.github.io/post/postgres/pgvector-scalar-binary-quantization/">see previous blog post for more details&lt;/a>). This followed the same format as the existing test.&lt;/li>
&lt;li>Additionally, I revived the &lt;a href="https://github.com/erikbern/ann-benchmarks/blob/fd389848e5706fdb0fbfc0e58bcb584e2ce7d4ee/ann_benchmarks/algorithms/pgvector/module.py">IVFFlat code&lt;/a> to run those tests. I did &lt;a href="https://github.com/erikbern/ann-benchmarks/commit/c09127170291301c9da759cfe41294ade0d22652">include a recent commit to accelerate the loading of data into the table&lt;/a>.&lt;/li>
&lt;li>I did make a small tweak to the timing of the index build. Instead of considering both the data load and the index build time, I only timed the index build. I&amp;rsquo;m planning to propose this as a contribution to the upstream project, as the primary goal of the &lt;code>fit&lt;/code> portion of ANN Benchmarks is to test the index build time.&lt;/li>
&lt;/ul>
&lt;p>For each index type I used the following search parameters, which are the defaults for what&amp;rsquo;s in the pgvector module for ANN Benchmarks:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Index type&lt;/th>
&lt;th>Search parameters&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>IVFFlat&lt;/td>
&lt;td>&lt;code>ivfflat.probes&lt;/code>: &lt;code>[1, 2, 4, 10, 20, 40, 100]&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>HNSW&lt;/td>
&lt;td>&lt;code>hnsw.ef_search&lt;/code>: &lt;code>[10, 20, 40, 80, 120, 200, 400, 800]&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Finally, the test results below will show the recall target (e.g. &lt;code>0.90&lt;/code> or &lt;code>90%&lt;/code>). The results are shown at the threshold that each test passed that recall level (if it passed that recall level). I could probably have fine tuned this further to find the exact &lt;code>hnsw.ef_search&lt;/code> value where the test crossed the threshold, which would give a more accurate representation of the performance characteristics at a recall target, but again, the main goal is to show the growth and growth areas of pgvector over the past year.&lt;/p>
&lt;p>And now it&amp;rsquo;s time for&amp;hellip;&lt;/p>
&lt;h2 id="the-150x-pgvector-speedup">The 150x pgvector speedup&lt;/h2>
&lt;p>For the first test, we&amp;rsquo;ll review the results from the &lt;code>dbpedia-openai-1000k-angular&lt;/code> benchmark at 99% recall. The results are below:&lt;/p>
&lt;h3 id="dbpedia-openai-1000k-angular--99-recall-on-a-r7gd16xlarge">&lt;code>dbpedia-openai-1000k-angular&lt;/code> @ 99% recall on a r7gd.16xlarge&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Test&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>Single Connection Throughput (QPS)&lt;/th>
&lt;th>QPS Speedup&lt;/th>
&lt;th>p99 Latency (ms)&lt;/th>
&lt;th>p99 Speedup&lt;/th>
&lt;th>Index Build (s)&lt;/th>
&lt;th>Index Build Speedup&lt;/th>
&lt;th>Index Size (GiB)&lt;/th>
&lt;th>Size Improvement&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>r7gd.041&lt;/td>
&lt;td>0.994&lt;/td>
&lt;td>8&lt;/td>
&lt;td>1&lt;/td>
&lt;td>150.16&lt;/td>
&lt;td>1&lt;/td>
&lt;td>474&lt;/td>
&lt;td>16&lt;/td>
&lt;td>7.56&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.044&lt;/td>
&lt;td>0.994&lt;/td>
&lt;td>8&lt;/td>
&lt;td>1&lt;/td>
&lt;td>155.25&lt;/td>
&lt;td>1&lt;/td>
&lt;td>476&lt;/td>
&lt;td>16&lt;/td>
&lt;td>7.56&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.050&lt;/td>
&lt;td>0.993&lt;/td>
&lt;td>243&lt;/td>
&lt;td>30.4&lt;/td>
&lt;td>5.74&lt;/td>
&lt;td>27&lt;/td>
&lt;td>7479&lt;/td>
&lt;td>1&lt;/td>
&lt;td>7.55&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.051&lt;/td>
&lt;td>0.992&lt;/td>
&lt;td>247&lt;/td>
&lt;td>30.9&lt;/td>
&lt;td>5.67&lt;/td>
&lt;td>27.4&lt;/td>
&lt;td>5088&lt;/td>
&lt;td>2&lt;/td>
&lt;td>7.55&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.060&lt;/td>
&lt;td>0.992&lt;/td>
&lt;td>252&lt;/td>
&lt;td>31.5&lt;/td>
&lt;td>5.52&lt;/td>
&lt;td>28.1&lt;/td>
&lt;td>253&lt;/td>
&lt;td>30&lt;/td>
&lt;td>7.55&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.062&lt;/td>
&lt;td>0.992&lt;/td>
&lt;td>253&lt;/td>
&lt;td>31.6&lt;/td>
&lt;td>5.54&lt;/td>
&lt;td>28&lt;/td>
&lt;td>252&lt;/td>
&lt;td>30&lt;/td>
&lt;td>7.55&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.070&lt;/td>
&lt;td>0.992&lt;/td>
&lt;td>253&lt;/td>
&lt;td>31.6&lt;/td>
&lt;td>5.51&lt;/td>
&lt;td>28.2&lt;/td>
&lt;td>250&lt;/td>
&lt;td>30&lt;/td>
&lt;td>7.55&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.070.fp16&lt;/td>
&lt;td>0.993&lt;/td>
&lt;td>263&lt;/td>
&lt;td>32.9&lt;/td>
&lt;td>5.3&lt;/td>
&lt;td>29.3&lt;/td>
&lt;td>146&lt;/td>
&lt;td>51&lt;/td>
&lt;td>3.78&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.070.bq-hamming-rerank&lt;/td>
&lt;td>0.99&lt;/td>
&lt;td>236&lt;/td>
&lt;td>29.5&lt;/td>
&lt;td>5.4&lt;/td>
&lt;td>28.8&lt;/td>
&lt;td>49&lt;/td>
&lt;td>153&lt;/td>
&lt;td>0.46&lt;/td>
&lt;td>16.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.070.bq-jaccard-rerank&lt;/td>
&lt;td>0.99&lt;/td>
&lt;td>234&lt;/td>
&lt;td>29.3&lt;/td>
&lt;td>5.38&lt;/td>
&lt;td>28.9&lt;/td>
&lt;td>50&lt;/td>
&lt;td>150&lt;/td>
&lt;td>0.46&lt;/td>
&lt;td>16.4&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>And there it is: between pgvector 0.5.0 (where HNSW was introduced) and pgvector 0.7.0, we see that we can get a 150x speedup in the index build time when we use the &amp;ldquo;&lt;a href="https://jkatz.github.io/post/postgres/pgvector-scalar-binary-quantization/">binary quantization&lt;/a>&amp;rdquo; methods. Note that &lt;a href="https://jkatz.github.io/post/postgres/pgvector-scalar-binary-quantization/">we can&amp;rsquo;t always use binary quantization&lt;/a> with our data, but we can see we can that scalar quantization to 2-byte floats show over a 50x speedup from the initial HNSW implementation in pgvector 0.5.0. A lot of this speedup is attributed to the use of parallel workers (in this case, 64) during the index build process. For fun, here&amp;rsquo;s how this looks in a bar chart:&lt;/p>
&lt;p>&lt;img src="https://jkatz.github.io/images/pgvector-150x-r7gd-dbpedia.png" alt="pgvector-150x-r7gd-dbpedia.png">&lt;/p>
&lt;p>(Note: I do chuckle a bit, as it reminds of a time I fixed a query I wrote to get a 100x speedup. It was a recursive query, but I used &lt;code>UNION ALL&lt;/code> when instead I wanted &lt;code>UNION&lt;/code>. Unlike my goofy mistake, this I do take this work in pgvector to be a bona fide speedup due to all of the improvements in the pgvector implementation).&lt;/p>
&lt;p>Additionally, we see that the addition of HNSW allows us to get a 30x QPS boost and an almost 30x p99 latency boost over IVFFlat at 99% recall. Queries were executed serially; we&amp;rsquo;d need to run additional tests to see how pgvector scales with client concurrently querying the data.&lt;/p>
&lt;h3 id="dbpedia-openai-1000k-angular--99-recall-on-a-r7i16xlarge">&lt;code>dbpedia-openai-1000k-angular&lt;/code> @ 99% recall on a r7i.16xlarge&lt;/h3>
&lt;p>Different CPU families can impact the results from a test based upon the availability of acceleration instructions (e.g. SIMD). pgvector 0.7.0 added support for SIMD disaptching functions on x86-64 architecture, so it&amp;rsquo;s important to test what impact this has on our test runs. For these tests, I used an Ubuntu 22.04, with the pgvector code compiled with gcc 12.3 and clang-15, and am showing the results from the &lt;code>dbpedia-openai-1000k-angular&lt;/code> benchmark at 99% recall:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Test&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>Single Connection Throughput (QPS)&lt;/th>
&lt;th>QPS Speedup&lt;/th>
&lt;th>p99 Latency (ms)&lt;/th>
&lt;th>p99 Speedup&lt;/th>
&lt;th>Index Build (s)&lt;/th>
&lt;th>Index Build Speedup&lt;/th>
&lt;th>Index Size (GiB)&lt;/th>
&lt;th>Size Improvement&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>r7i.041&lt;/td>
&lt;td>0.994&lt;/td>
&lt;td>8&lt;/td>
&lt;td>1&lt;/td>
&lt;td>153.01&lt;/td>
&lt;td>1&lt;/td>
&lt;td>496&lt;/td>
&lt;td>15.0&lt;/td>
&lt;td>7.56&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.044&lt;/td>
&lt;td>0.994&lt;/td>
&lt;td>8&lt;/td>
&lt;td>1&lt;/td>
&lt;td>156.58&lt;/td>
&lt;td>1&lt;/td>
&lt;td>494&lt;/td>
&lt;td>15.1&lt;/td>
&lt;td>7.56&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.050&lt;/td>
&lt;td>0.992&lt;/td>
&lt;td>255&lt;/td>
&lt;td>31.9&lt;/td>
&lt;td>5.42&lt;/td>
&lt;td>28.9&lt;/td>
&lt;td>7443&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>7.55&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.051&lt;/td>
&lt;td>0.992&lt;/td>
&lt;td>245&lt;/td>
&lt;td>30.6&lt;/td>
&lt;td>5.66&lt;/td>
&lt;td>27.7&lt;/td>
&lt;td>5201&lt;/td>
&lt;td>1.4&lt;/td>
&lt;td>7.55&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.060&lt;/td>
&lt;td>0.992&lt;/td>
&lt;td>261&lt;/td>
&lt;td>32.6&lt;/td>
&lt;td>5.28&lt;/td>
&lt;td>29.7&lt;/td>
&lt;td>773&lt;/td>
&lt;td>9.6&lt;/td>
&lt;td>7.55&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.062&lt;/td>
&lt;td>0.992&lt;/td>
&lt;td>265&lt;/td>
&lt;td>33.1&lt;/td>
&lt;td>5.22&lt;/td>
&lt;td>30.0&lt;/td>
&lt;td>382&lt;/td>
&lt;td>19.5&lt;/td>
&lt;td>7.55&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.070&lt;/td>
&lt;td>0.993&lt;/td>
&lt;td>255&lt;/td>
&lt;td>31.9&lt;/td>
&lt;td>5.40&lt;/td>
&lt;td>29.0&lt;/td>
&lt;td>388&lt;/td>
&lt;td>19.2&lt;/td>
&lt;td>7.55&lt;/td>
&lt;td>1&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.070.fp16&lt;/td>
&lt;td>0.993&lt;/td>
&lt;td>282&lt;/td>
&lt;td>35.3&lt;/td>
&lt;td>4.87&lt;/td>
&lt;td>32.2&lt;/td>
&lt;td>227&lt;/td>
&lt;td>32.8&lt;/td>
&lt;td>3.78&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.070.bq-hamming-rerank&lt;/td>
&lt;td>0.99&lt;/td>
&lt;td>269&lt;/td>
&lt;td>33.6&lt;/td>
&lt;td>4.78&lt;/td>
&lt;td>32.8&lt;/td>
&lt;td>64&lt;/td>
&lt;td>116.3&lt;/td>
&lt;td>0.46&lt;/td>
&lt;td>16.4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.070.bq-jaccard-rerank&lt;/td>
&lt;td>0.99&lt;/td>
&lt;td>267&lt;/td>
&lt;td>33.4&lt;/td>
&lt;td>4.77&lt;/td>
&lt;td>32.8&lt;/td>
&lt;td>66&lt;/td>
&lt;td>112.8&lt;/td>
&lt;td>0.46&lt;/td>
&lt;td>16.4&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Again, we see a 100x+ speedup in index build time when using the &amp;ldquo;&lt;a href="https://jkatz.github.io/post/postgres/pgvector-scalar-binary-quantization/">binary quantization&lt;/a>&amp;rdquo; methods, and comparable performance results overall to what we had with the r7gd family. We can also see a more than 30x improvement in both throughput and latency as well. Here is a chart that shows how the index build times have decreased on the r7i:&lt;/p>
&lt;p>&lt;img src="https://jkatz.github.io/images/pgvector-150x-r7gd-dbpedia.png" alt="pgvector-150x-r7i-dbpedia.png">&lt;/p>
&lt;p>(I&amp;rsquo;ll note here I really need to level up my matplotlib skills; likely Excel too, as it was taking me awhile to get the data charted there. Anyway, this is all the charting I&amp;rsquo;m doing in this blog post).&lt;/p>
&lt;p>As explored in the previous blog post on &lt;a href="post/postgres/pgvector-scalar-binary-quantization/">scalar and binary quantization&lt;/a>, we can&amp;rsquo;t always use binary quantization and achieve our recall target due to lack of bit diversity in the indexed vectors. We saw this with both the &lt;code>sift-128-euclidean&lt;/code> and &lt;code>gist-960-euclidean&lt;/code> datasets. However, both still have nice speedups over the course of the year.&lt;/p>
&lt;p>Below are the results from the &lt;code>sift-128-euclidean&lt;/code> benchmark @ 99% recall on both architectures:&lt;/p>
&lt;h3 id="sift-128-euclidean--99-recall-on-a-r7gd16xlarge">&lt;code>sift-128-euclidean&lt;/code> @ 99% recall on a r7gd.16xlarge&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Test&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>Single Connection Throughput (QPS)&lt;/th>
&lt;th>QPS Speedup&lt;/th>
&lt;th>p99 Latency (ms)&lt;/th>
&lt;th>p99 Speedup&lt;/th>
&lt;th>Index Build (s)&lt;/th>
&lt;th>Index Build Speedup&lt;/th>
&lt;th>Index Size (GiB)&lt;/th>
&lt;th>Size Improvement&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>r7gd.041&lt;/td>
&lt;td>0.999&lt;/td>
&lt;td>33&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>44.05&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>58&lt;/td>
&lt;td>41.6&lt;/td>
&lt;td>0.51&lt;/td>
&lt;td>1.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.044&lt;/td>
&lt;td>0.999&lt;/td>
&lt;td>33&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>42.39&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>59&lt;/td>
&lt;td>40.9&lt;/td>
&lt;td>0.51&lt;/td>
&lt;td>1.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.050&lt;/td>
&lt;td>0.994&lt;/td>
&lt;td>432&lt;/td>
&lt;td>13.1&lt;/td>
&lt;td>2.98&lt;/td>
&lt;td>14.8&lt;/td>
&lt;td>2411&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>0.76&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.051&lt;/td>
&lt;td>0.994&lt;/td>
&lt;td>432&lt;/td>
&lt;td>13.1&lt;/td>
&lt;td>2.98&lt;/td>
&lt;td>14.8&lt;/td>
&lt;td>1933&lt;/td>
&lt;td>1.2&lt;/td>
&lt;td>0.76&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.060&lt;/td>
&lt;td>0.994&lt;/td>
&lt;td>453&lt;/td>
&lt;td>13.7&lt;/td>
&lt;td>2.84&lt;/td>
&lt;td>15.5&lt;/td>
&lt;td>67&lt;/td>
&lt;td>36.0&lt;/td>
&lt;td>0.76&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.062&lt;/td>
&lt;td>0.994&lt;/td>
&lt;td>458&lt;/td>
&lt;td>13.9&lt;/td>
&lt;td>2.81&lt;/td>
&lt;td>15.7&lt;/td>
&lt;td>57&lt;/td>
&lt;td>42.3&lt;/td>
&lt;td>0.76&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.070&lt;/td>
&lt;td>0.994&lt;/td>
&lt;td>487&lt;/td>
&lt;td>14.8&lt;/td>
&lt;td>2.65&lt;/td>
&lt;td>16.6&lt;/td>
&lt;td>56&lt;/td>
&lt;td>43.1&lt;/td>
&lt;td>0.76&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.070.fp16&lt;/td>
&lt;td>0.994&lt;/td>
&lt;td>482&lt;/td>
&lt;td>14.6&lt;/td>
&lt;td>2.68&lt;/td>
&lt;td>16.4&lt;/td>
&lt;td>48&lt;/td>
&lt;td>50.2&lt;/td>
&lt;td>0.52&lt;/td>
&lt;td>1.5&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="sift-128-euclidean--99-recall-on-a-r7i16xlarge">&lt;code>sift-128-euclidean&lt;/code> @ 99% recall on a r7i.16xlarge&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Test&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>Single Connection Throughput (QPS)&lt;/th>
&lt;th>QPS Speedup&lt;/th>
&lt;th>p99 Latency (ms)&lt;/th>
&lt;th>p99 Speedup&lt;/th>
&lt;th>Index Build (s)&lt;/th>
&lt;th>Index Build Speedup&lt;/th>
&lt;th>Index Size (GiB)&lt;/th>
&lt;th>Size Improvement&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>r7i.041&lt;/td>
&lt;td>0.999&lt;/td>
&lt;td>31&lt;/td>
&lt;td>1.1&lt;/td>
&lt;td>48.57&lt;/td>
&lt;td>1.1&lt;/td>
&lt;td>43&lt;/td>
&lt;td>51.3&lt;/td>
&lt;td>0.51&lt;/td>
&lt;td>1.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.044&lt;/td>
&lt;td>0.999&lt;/td>
&lt;td>29&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>51.34&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>44&lt;/td>
&lt;td>50.2&lt;/td>
&lt;td>0.51&lt;/td>
&lt;td>1.5&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.050&lt;/td>
&lt;td>0.994&lt;/td>
&lt;td>436&lt;/td>
&lt;td>15.0&lt;/td>
&lt;td>2.96&lt;/td>
&lt;td>17.3&lt;/td>
&lt;td>2208&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>0.76&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.051&lt;/td>
&lt;td>0.994&lt;/td>
&lt;td>426&lt;/td>
&lt;td>14.7&lt;/td>
&lt;td>3.06&lt;/td>
&lt;td>16.8&lt;/td>
&lt;td>1722&lt;/td>
&lt;td>1.3&lt;/td>
&lt;td>0.76&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.060&lt;/td>
&lt;td>0.994&lt;/td>
&lt;td>503&lt;/td>
&lt;td>17.3&lt;/td>
&lt;td>2.57&lt;/td>
&lt;td>20.0&lt;/td>
&lt;td>581&lt;/td>
&lt;td>3.8&lt;/td>
&lt;td>0.76&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.062&lt;/td>
&lt;td>0.994&lt;/td>
&lt;td>497&lt;/td>
&lt;td>17.1&lt;/td>
&lt;td>2.57&lt;/td>
&lt;td>20.0&lt;/td>
&lt;td>74&lt;/td>
&lt;td>29.8&lt;/td>
&lt;td>0.76&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.070&lt;/td>
&lt;td>0.994&lt;/td>
&lt;td>492&lt;/td>
&lt;td>17.0&lt;/td>
&lt;td>2.60&lt;/td>
&lt;td>19.7&lt;/td>
&lt;td>74&lt;/td>
&lt;td>29.8&lt;/td>
&lt;td>0.76&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.070.fp16&lt;/td>
&lt;td>0.994&lt;/td>
&lt;td>544&lt;/td>
&lt;td>18.8&lt;/td>
&lt;td>2.36&lt;/td>
&lt;td>21.8&lt;/td>
&lt;td>62&lt;/td>
&lt;td>35.6&lt;/td>
&lt;td>0.52&lt;/td>
&lt;td>1.5&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Across the board, there are some nice speedups, including the 50x index build time improvement for the quantized &lt;code>halfvec&lt;/code> test (&lt;code>r7gd.070.fp16&lt;/code>), similar to the &lt;code>dbpedia-openai-1000k-angular&lt;/code> test.&lt;/p>
&lt;p>Let&amp;rsquo;s take a quick look at the &lt;code>gist-960-euclidean&lt;/code> data. With the previous tests, we looked at the results targeting 99% recall, as the QPS/p99 speedups were more pronounced with those. However, &lt;code>gist-960-euclidean&lt;/code> tends to be particularly challenging to get good throughput/performance results at high recall (though with binary quantization, I can get over 6,000 QPS at 0% recall!), and interestingly I observed the best speedups at 90% recall.&lt;/p>
&lt;h3 id="gist-960-euclidean--90-recall-on-a-r7gd16xlarge">&lt;code>gist-960-euclidean&lt;/code> @ 90% recall on a r7gd.16xlarge&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Test&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>Single Connection Throughput (QPS)&lt;/th>
&lt;th>QPS Speedup&lt;/th>
&lt;th>p99 Latency (ms)&lt;/th>
&lt;th>p99 Speedup&lt;/th>
&lt;th>Index Build (s)&lt;/th>
&lt;th>Index Build Speedup&lt;/th>
&lt;th>Index Size (GiB)&lt;/th>
&lt;th>Size Improvement&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>r7gd.041&lt;/td>
&lt;td>0.965&lt;/td>
&lt;td>13&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>128.91&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>300&lt;/td>
&lt;td>22.6&lt;/td>
&lt;td>3.82&lt;/td>
&lt;td>2.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.044&lt;/td>
&lt;td>0.968&lt;/td>
&lt;td>14&lt;/td>
&lt;td>1.1&lt;/td>
&lt;td>123.66&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>297&lt;/td>
&lt;td>22.9&lt;/td>
&lt;td>3.82&lt;/td>
&lt;td>2.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.050&lt;/td>
&lt;td>0.923&lt;/td>
&lt;td>215&lt;/td>
&lt;td>16.5&lt;/td>
&lt;td>5.53&lt;/td>
&lt;td>23.3&lt;/td>
&lt;td>6787&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>7.50&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.051&lt;/td>
&lt;td>0.924&lt;/td>
&lt;td>215&lt;/td>
&lt;td>16.5&lt;/td>
&lt;td>5.59&lt;/td>
&lt;td>23.1&lt;/td>
&lt;td>4687&lt;/td>
&lt;td>1.4&lt;/td>
&lt;td>7.50&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.060&lt;/td>
&lt;td>0.924&lt;/td>
&lt;td>229&lt;/td>
&lt;td>17.6&lt;/td>
&lt;td>5.16&lt;/td>
&lt;td>25.0&lt;/td>
&lt;td>204&lt;/td>
&lt;td>33.3&lt;/td>
&lt;td>7.50&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.062&lt;/td>
&lt;td>0.923&lt;/td>
&lt;td>224&lt;/td>
&lt;td>17.2&lt;/td>
&lt;td>5.31&lt;/td>
&lt;td>24.3&lt;/td>
&lt;td>198&lt;/td>
&lt;td>34.3&lt;/td>
&lt;td>7.50&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.070&lt;/td>
&lt;td>0.922&lt;/td>
&lt;td>229&lt;/td>
&lt;td>17.6&lt;/td>
&lt;td>5.18&lt;/td>
&lt;td>24.9&lt;/td>
&lt;td>197&lt;/td>
&lt;td>34.5&lt;/td>
&lt;td>7.50&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.070.fp16&lt;/td>
&lt;td>0.921&lt;/td>
&lt;td>248&lt;/td>
&lt;td>19.1&lt;/td>
&lt;td>4.83&lt;/td>
&lt;td>26.7&lt;/td>
&lt;td>137&lt;/td>
&lt;td>49.5&lt;/td>
&lt;td>2.50&lt;/td>
&lt;td>3.0&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="gist-960-euclidean--90-recall-on-a-r7i16xlarge">&lt;code>gist-960-euclidean&lt;/code> @ 90% recall on a r7i.16xlarge&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Test&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>Single Connection Throughput (QPS)&lt;/th>
&lt;th>QPS Speedup&lt;/th>
&lt;th>p99 Latency (ms)&lt;/th>
&lt;th>p99 Speedup&lt;/th>
&lt;th>Index Build (s)&lt;/th>
&lt;th>Index Build Speedup&lt;/th>
&lt;th>Index Size (GiB)&lt;/th>
&lt;th>Size Improvement&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>r7i.041&lt;/td>
&lt;td>0.966&lt;/td>
&lt;td>16&lt;/td>
&lt;td>1.1&lt;/td>
&lt;td>111.47&lt;/td>
&lt;td>1.1&lt;/td>
&lt;td>282&lt;/td>
&lt;td>22.2&lt;/td>
&lt;td>3.82&lt;/td>
&lt;td>2.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.044&lt;/td>
&lt;td>0.965&lt;/td>
&lt;td>15&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>120.90&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>289&lt;/td>
&lt;td>21.7&lt;/td>
&lt;td>3.82&lt;/td>
&lt;td>2.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.050&lt;/td>
&lt;td>0.923&lt;/td>
&lt;td>226&lt;/td>
&lt;td>15.1&lt;/td>
&lt;td>5.20&lt;/td>
&lt;td>23.3&lt;/td>
&lt;td>6273&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>7.50&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.051&lt;/td>
&lt;td>0.925&lt;/td>
&lt;td>228&lt;/td>
&lt;td>15.2&lt;/td>
&lt;td>5.26&lt;/td>
&lt;td>23.0&lt;/td>
&lt;td>4212&lt;/td>
&lt;td>1.5&lt;/td>
&lt;td>7.50&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.060&lt;/td>
&lt;td>0.924&lt;/td>
&lt;td>246&lt;/td>
&lt;td>16.4&lt;/td>
&lt;td>4.84&lt;/td>
&lt;td>25.0&lt;/td>
&lt;td>1109&lt;/td>
&lt;td>5.7&lt;/td>
&lt;td>7.50&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.062&lt;/td>
&lt;td>0.923&lt;/td>
&lt;td>245&lt;/td>
&lt;td>16.3&lt;/td>
&lt;td>4.88&lt;/td>
&lt;td>24.8&lt;/td>
&lt;td>301&lt;/td>
&lt;td>20.8&lt;/td>
&lt;td>7.50&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.070&lt;/td>
&lt;td>0.924&lt;/td>
&lt;td>238&lt;/td>
&lt;td>15.9&lt;/td>
&lt;td>4.97&lt;/td>
&lt;td>24.3&lt;/td>
&lt;td>295&lt;/td>
&lt;td>21.3&lt;/td>
&lt;td>7.50&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.070.fp16&lt;/td>
&lt;td>0.921&lt;/td>
&lt;td>271&lt;/td>
&lt;td>18.1&lt;/td>
&lt;td>4.33&lt;/td>
&lt;td>27.9&lt;/td>
&lt;td>180&lt;/td>
&lt;td>34.9&lt;/td>
&lt;td>2.50&lt;/td>
&lt;td>3.0&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Again, we can see the effects of parallelism on speeding up the HNSW builds, as well as the effects on shrinking the index size by using 2-byte floats. Also, similar to the &lt;code>sift-128-euclidean&lt;/code> test, we&amp;rsquo;re unable to use binary quantization to achieve 90% recall.&lt;/p>
&lt;p>For completeness, here are a few more sets of results. I chose the &amp;ldquo;recall&amp;rdquo; values to optimize for where I saw the biggest performance gains:&lt;/p>
&lt;h3 id="glove-25-angular--99-recall-on-a-r7gd16xlarge">&lt;code>glove-25-angular&lt;/code> @ 99% recall on a r7gd.16xlarge&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Test&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>Single Connection Throughput (QPS)&lt;/th>
&lt;th>QPS Speedup&lt;/th>
&lt;th>p99 Latency (ms)&lt;/th>
&lt;th>p99 Speedup&lt;/th>
&lt;th>Index Build (s)&lt;/th>
&lt;th>Index Build Speedup&lt;/th>
&lt;th>Index Size (GiB)&lt;/th>
&lt;th>Size Improvement&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>r7gd.041&lt;/td>
&lt;td>0.997&lt;/td>
&lt;td>26&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>53.50&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>31&lt;/td>
&lt;td>81.9&lt;/td>
&lt;td>0.14&lt;/td>
&lt;td>3.2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.044&lt;/td>
&lt;td>0.997&lt;/td>
&lt;td>26&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>53.98&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>33&lt;/td>
&lt;td>76.9&lt;/td>
&lt;td>0.14&lt;/td>
&lt;td>3.2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.050&lt;/td>
&lt;td>0.995&lt;/td>
&lt;td>493&lt;/td>
&lt;td>19.0&lt;/td>
&lt;td>2.64&lt;/td>
&lt;td>20.4&lt;/td>
&lt;td>2538&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>0.45&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.051&lt;/td>
&lt;td>0.995&lt;/td>
&lt;td>495&lt;/td>
&lt;td>19.0&lt;/td>
&lt;td>2.64&lt;/td>
&lt;td>20.4&lt;/td>
&lt;td>1922&lt;/td>
&lt;td>1.3&lt;/td>
&lt;td>0.45&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.060&lt;/td>
&lt;td>0.995&lt;/td>
&lt;td>514&lt;/td>
&lt;td>19.8&lt;/td>
&lt;td>2.55&lt;/td>
&lt;td>21.2&lt;/td>
&lt;td>53&lt;/td>
&lt;td>47.9&lt;/td>
&lt;td>0.45&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.062&lt;/td>
&lt;td>0.995&lt;/td>
&lt;td>470&lt;/td>
&lt;td>18.1&lt;/td>
&lt;td>2.79&lt;/td>
&lt;td>19.3&lt;/td>
&lt;td>49&lt;/td>
&lt;td>51.8&lt;/td>
&lt;td>0.45&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.070&lt;/td>
&lt;td>0.995&lt;/td>
&lt;td>522&lt;/td>
&lt;td>20.1&lt;/td>
&lt;td>2.50&lt;/td>
&lt;td>21.6&lt;/td>
&lt;td>48&lt;/td>
&lt;td>52.9&lt;/td>
&lt;td>0.45&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.070.fp16&lt;/td>
&lt;td>0.995&lt;/td>
&lt;td>521&lt;/td>
&lt;td>20.0&lt;/td>
&lt;td>2.51&lt;/td>
&lt;td>21.5&lt;/td>
&lt;td>48&lt;/td>
&lt;td>52.9&lt;/td>
&lt;td>0.40&lt;/td>
&lt;td>1.1&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="glove-25-angular--99-recall-on-a-r7i16xlarge">&lt;code>glove-25-angular&lt;/code> @ 99% recall on a r7i.16xlarge&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Test&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>Single Connection Throughput (QPS)&lt;/th>
&lt;th>QPS Speedup&lt;/th>
&lt;th>p99 Latency (ms)&lt;/th>
&lt;th>p99 Speedup&lt;/th>
&lt;th>Index Build (s)&lt;/th>
&lt;th>Index Build Speedup&lt;/th>
&lt;th>Index Size (GiB)&lt;/th>
&lt;th>Size Improvement&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>r7i.041&lt;/td>
&lt;td>0.997&lt;/td>
&lt;td>23&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>59.08&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>38&lt;/td>
&lt;td>63.5&lt;/td>
&lt;td>0.14&lt;/td>
&lt;td>3.2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.044&lt;/td>
&lt;td>0.997&lt;/td>
&lt;td>24&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>59.59&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>30&lt;/td>
&lt;td>80.5&lt;/td>
&lt;td>0.14&lt;/td>
&lt;td>3.2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.050&lt;/td>
&lt;td>0.995&lt;/td>
&lt;td>539&lt;/td>
&lt;td>23.4&lt;/td>
&lt;td>2.41&lt;/td>
&lt;td>24.7&lt;/td>
&lt;td>2414&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>0.45&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.051&lt;/td>
&lt;td>0.995&lt;/td>
&lt;td>545&lt;/td>
&lt;td>23.7&lt;/td>
&lt;td>2.39&lt;/td>
&lt;td>24.9&lt;/td>
&lt;td>1827&lt;/td>
&lt;td>1.3&lt;/td>
&lt;td>0.45&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.060&lt;/td>
&lt;td>0.995&lt;/td>
&lt;td>557&lt;/td>
&lt;td>24.2&lt;/td>
&lt;td>2.34&lt;/td>
&lt;td>25.5&lt;/td>
&lt;td>471&lt;/td>
&lt;td>5.1&lt;/td>
&lt;td>0.45&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.062&lt;/td>
&lt;td>0.995&lt;/td>
&lt;td>574&lt;/td>
&lt;td>25.0&lt;/td>
&lt;td>2.27&lt;/td>
&lt;td>26.3&lt;/td>
&lt;td>64&lt;/td>
&lt;td>37.7&lt;/td>
&lt;td>0.45&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.070&lt;/td>
&lt;td>0.995&lt;/td>
&lt;td>569&lt;/td>
&lt;td>24.7&lt;/td>
&lt;td>2.28&lt;/td>
&lt;td>26.1&lt;/td>
&lt;td>63&lt;/td>
&lt;td>38.3&lt;/td>
&lt;td>0.45&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.070.fp16&lt;/td>
&lt;td>0.995&lt;/td>
&lt;td>569&lt;/td>
&lt;td>24.7&lt;/td>
&lt;td>2.28&lt;/td>
&lt;td>26.1&lt;/td>
&lt;td>60&lt;/td>
&lt;td>40.2&lt;/td>
&lt;td>0.40&lt;/td>
&lt;td>1.1&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The interesting thing about both of these tests is that the IVFFlat index builds are both faster and smaller than the HNSW index builds - and that is without using any parallelism during the IVFFlat build. However, the HNSW numbers show a sigificant boost in throughput and p99 latency.&lt;/p>
&lt;p>Finally, here are the results from the &lt;code>glove-100-angular&lt;/code> test. In my test, I wasn&amp;rsquo;t able to get much above 95% recall. I would likely need to increase the &lt;code>m&lt;/code> build parameter to get towards 99% recall, but as mentioned earlier, the goal of this testing was primarily to see how pgvector has improved over the course of the year and not optimize parameters for a particular dataset:&lt;/p>
&lt;h3 id="glove-100-angular--95-recall-on-a-r7gd16xlarge">&lt;code>glove-100-angular&lt;/code> @ 95% recall on a r7gd.16xlarge&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Test&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>Single Connection Throughput (QPS)&lt;/th>
&lt;th>QPS Speedup&lt;/th>
&lt;th>p99 Latency (ms)&lt;/th>
&lt;th>p99 Speedup&lt;/th>
&lt;th>Index Build (s)&lt;/th>
&lt;th>Index Build Speedup&lt;/th>
&lt;th>Index Size (GiB)&lt;/th>
&lt;th>Size Improvement&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>r7gd.041&lt;/td>
&lt;td>0.963&lt;/td>
&lt;td>29&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>47.10&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>68&lt;/td>
&lt;td>58.0&lt;/td>
&lt;td>0.48&lt;/td>
&lt;td>1.7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.044&lt;/td>
&lt;td>0.963&lt;/td>
&lt;td>29&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>46.20&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>69&lt;/td>
&lt;td>57.1&lt;/td>
&lt;td>0.48&lt;/td>
&lt;td>1.7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.050&lt;/td>
&lt;td>0.965&lt;/td>
&lt;td>65&lt;/td>
&lt;td>2.2&lt;/td>
&lt;td>21.05&lt;/td>
&lt;td>2.2&lt;/td>
&lt;td>3941&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>0.82&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.051&lt;/td>
&lt;td>0.965&lt;/td>
&lt;td>65&lt;/td>
&lt;td>2.2&lt;/td>
&lt;td>20.90&lt;/td>
&lt;td>2.3&lt;/td>
&lt;td>2965&lt;/td>
&lt;td>1.3&lt;/td>
&lt;td>0.82&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.060&lt;/td>
&lt;td>0.965&lt;/td>
&lt;td>63&lt;/td>
&lt;td>2.2&lt;/td>
&lt;td>21.22&lt;/td>
&lt;td>2.2&lt;/td>
&lt;td>83&lt;/td>
&lt;td>47.5&lt;/td>
&lt;td>0.82&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.062&lt;/td>
&lt;td>0.965&lt;/td>
&lt;td>62&lt;/td>
&lt;td>2.1&lt;/td>
&lt;td>21.68&lt;/td>
&lt;td>2.2&lt;/td>
&lt;td>78&lt;/td>
&lt;td>50.5&lt;/td>
&lt;td>0.82&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.070&lt;/td>
&lt;td>0.965&lt;/td>
&lt;td>66&lt;/td>
&lt;td>2.3&lt;/td>
&lt;td>20.07&lt;/td>
&lt;td>2.3&lt;/td>
&lt;td>77&lt;/td>
&lt;td>51.2&lt;/td>
&lt;td>0.82&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7gd.070.fp16&lt;/td>
&lt;td>0.965&lt;/td>
&lt;td>67&lt;/td>
&lt;td>2.3&lt;/td>
&lt;td>19.97&lt;/td>
&lt;td>2.4&lt;/td>
&lt;td>68&lt;/td>
&lt;td>58.0&lt;/td>
&lt;td>0.57&lt;/td>
&lt;td>1.4&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="glove-100-angular--95-recall-on-a-r7i16xlarge">&lt;code>glove-100-angular&lt;/code> @ 95% recall on a r7i.16xlarge&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Test&lt;/th>
&lt;th>Recall&lt;/th>
&lt;th>Single Connection Throughput (QPS)&lt;/th>
&lt;th>QPS Speedup&lt;/th>
&lt;th>p99 Latency (ms)&lt;/th>
&lt;th>p99 Speedup&lt;/th>
&lt;th>Index Build (s)&lt;/th>
&lt;th>Index Build Speedup&lt;/th>
&lt;th>Index Size (GiB)&lt;/th>
&lt;th>Size Improvement&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>r7i.041&lt;/td>
&lt;td>0.963&lt;/td>
&lt;td>27&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>50.43&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>53&lt;/td>
&lt;td>66.8&lt;/td>
&lt;td>0.48&lt;/td>
&lt;td>1.7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.044&lt;/td>
&lt;td>0.962&lt;/td>
&lt;td>26&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>52.13&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>56&lt;/td>
&lt;td>63.3&lt;/td>
&lt;td>0.48&lt;/td>
&lt;td>1.7&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.050&lt;/td>
&lt;td>0.965&lt;/td>
&lt;td>81&lt;/td>
&lt;td>3.1&lt;/td>
&lt;td>16.70&lt;/td>
&lt;td>3.1&lt;/td>
&lt;td>3543&lt;/td>
&lt;td>1.0&lt;/td>
&lt;td>0.82&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.051&lt;/td>
&lt;td>0.965&lt;/td>
&lt;td>82&lt;/td>
&lt;td>3.2&lt;/td>
&lt;td>16.49&lt;/td>
&lt;td>3.2&lt;/td>
&lt;td>2517&lt;/td>
&lt;td>1.4&lt;/td>
&lt;td>0.82&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.060&lt;/td>
&lt;td>0.965&lt;/td>
&lt;td>79&lt;/td>
&lt;td>3.0&lt;/td>
&lt;td>16.64&lt;/td>
&lt;td>3.1&lt;/td>
&lt;td>692&lt;/td>
&lt;td>5.1&lt;/td>
&lt;td>0.82&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.062&lt;/td>
&lt;td>0.965&lt;/td>
&lt;td>83&lt;/td>
&lt;td>3.2&lt;/td>
&lt;td>15.90&lt;/td>
&lt;td>3.3&lt;/td>
&lt;td>98&lt;/td>
&lt;td>36.2&lt;/td>
&lt;td>0.82&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.070&lt;/td>
&lt;td>0.965&lt;/td>
&lt;td>81&lt;/td>
&lt;td>3.1&lt;/td>
&lt;td>16.27&lt;/td>
&lt;td>3.2&lt;/td>
&lt;td>95&lt;/td>
&lt;td>37.3&lt;/td>
&lt;td>0.82&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>r7i.070.fp16&lt;/td>
&lt;td>0.965&lt;/td>
&lt;td>86&lt;/td>
&lt;td>3.3&lt;/td>
&lt;td>15.27&lt;/td>
&lt;td>3.4&lt;/td>
&lt;td>84&lt;/td>
&lt;td>42.2&lt;/td>
&lt;td>0.57&lt;/td>
&lt;td>1.4&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Overall with &lt;code>glove-100-angular&lt;/code> on the selected build parameters, there are definite speedups on build times for HNSW indexes, and we do see improvements in throughput/latency. For this specific dataset, I&amp;rsquo;d recommend rerunning it with different HNSW build parameters to see if we can improve query performance numbers at higher levels of recall, but that&amp;rsquo;s an experiment for another day.&lt;/p>
&lt;h2 id="where-do-we-go-from-here">Where do we go from here?&lt;/h2>
&lt;p>It&amp;rsquo;s been quite a year for pgvector on many fronts, not to say the least the many people who are already building amazing apps with it today! A &amp;ldquo;billion-scale&amp;rdquo; vector storage problem is attainable with pgvector today, much of this attributed to the work of the last year. And while I can&amp;rsquo;t say enough about the work &lt;a href="https://github.com/ankane">Andrew Kane&lt;/a> has done on pgvector, I do want to give mentions to &lt;a href="https://github.com/hlinnaka">Heikki Linnakangas&lt;/a>, &lt;a href="https://github.com/nathan-bossart">Nathan Bossart&lt;/a>, &lt;a href="https://github.com/pashkinelfe">Pavel Borisov&lt;/a>, and &lt;a href="https://github.com/aytekinar">Arda Aytekin&lt;/a> who all made contributions to improve pgvector performance (and apologies if I missed someone).&lt;/p>
&lt;p>However, much like the &lt;a href="https://jkatz05.com/post/postgres/postgresql-2024/">&lt;em>almost&lt;/em> 40-year-old database PostgreSQL&lt;/a>, there are still ways pgvector can continue to grow. I&amp;rsquo;m going to talk more in depth about some &lt;a href="https://www.pgevents.ca/events/pgconfdev2024/schedule/session/1-vectors-how-to-better-support-a-nasty-data-type-in-postgresql/">longer term goals to better support vector workloads with pgvector and PostgreSQL&lt;/a> at &lt;a href="https://2024.pgconf.dev/">PGConf.dev 2024&lt;/a>, but I&amp;rsquo;ll give a brief preview here.&lt;/p>
&lt;p>Over the past year, pgvector has made significant gains across the board in index build times, index sizes, throughput, and latency, particularly on vector queries over an entire vector data set. Simplifying &lt;a href="https://github.com/pgvector/pgvector/?tab=readme-ov-file#filtering">filtering&lt;/a> (aka the &lt;code>WHERE&lt;/code> clause) - pgvector and PostgreSQL already support this, but there are some areas we can make it easier and more efficient. Additionally, there are other search patterns that are gaining popularity, such as &amp;ldquo;hybrid search&amp;rdquo; like using simultaneously vector similarity search and fulltext search to return results. Again, this is something already supported by PostgreSQL natively, but there are areas we can simplify this process with pgvector. We&amp;rsquo;re seeing more work in pgvector to support hardware acceleration; this combined with further optimizations on And finally, there are some areas of PostgresQL we can prove to better support &lt;a href="https://jkatz.github.io/post/postgres/distributed-pgvector/">distributed pgvector workloads&lt;/a>, but I&amp;rsquo;ll still emphasize that most workloads that involve PostgreSQL and pgvector will scale vertically (which means showing more concurrency testing!).&lt;/p>
&lt;p>We&amp;rsquo;ll also have to see how vector search workloads evolve, as that will also dictate what new features we&amp;rsquo;ll see in pgvector. Please keep giving feedback on what you&amp;rsquo;re building with pgvector and how your experience is - as that is how we can continue to make the project better!&lt;/p></description></item><item><title>Scalar and binary quantization for pgvector vector search and storage</title><link>https://jkatz.github.io/post/postgres/pgvector-scalar-binary-quantization/</link><pubDate>Tue, 09 Apr 2024 00:00:00 +0000</pubDate><guid>https://jkatz.github.io/post/postgres/pgvector-scalar-binary-quantization/</guid><description>&lt;p>While many AI/ML embedding models generate vectors that provide large amounts of information by using high dimensionality, this can come at the cost of using more memory for searches and more overall storage. Both of these can have an impact on the cost and performance of a system that&amp;rsquo;s storing vectors, including when using &lt;a href="https://www.postgresql.org/">PostgreSQL&lt;/a> with the &lt;a href="https://github.com/pgvector/pgvector/">pgvector&lt;/a> for these use cases.&lt;/p>
&lt;p>When I &lt;a href="https://jkatz05.com/talks/">talk&lt;/a> about &lt;a href="https://www.youtube.com/watch?v=PhIC4JlYg7A&amp;amp;ab_channel=AWSEvents">vector search in PostgreSQL&lt;/a>, I have a slide that I like to call &amp;ldquo;no shortcuts without tradeoffs&amp;rdquo; that calls out the different challenges around searching vectors in a database. I first like to highlight that a 1,536 dimensional vector with 4-byte floating point (&lt;code>fp32&lt;/code>) dimensions requires 6KiB of storage. That may not seem like a lot, but storing 1,000,000 of these vectors in a PostgreSQL database requires 5.7GB of storage without any indexing. And again, that may not seem like a lot, but a single database row is closer to 600B of data (and typically even less than that) than 6KB.&lt;/p>
&lt;p>The next point I talk about is compression: unfortunately, you just can&amp;rsquo;t compress a bunch of seemingly random floating point numbers. However, there are techniques to reduce the amount of information that you can store, from &lt;a href="https://en.wikipedia.org/wiki/Principal_component_analysis">principal component analysis&lt;/a> (aka &lt;a href="https://en.wikipedia.org/wiki/Principal_component_analysis">PCA&lt;/a>), which can reduce the overall dimensionality, to quantization techniques that can reduce the size or overall number of dimensions of a vector. These can reduce your overall storage and memory requirements &amp;ndash; and perhaps even improve performance in certain areas, but recall that I mentioned there are no shortcuts without tradeoffs. To use quantization for vector searches, we&amp;rsquo;ll have to give something up, whether it&amp;rsquo;s in the overall relevancy of our searches (&lt;a href="https://en.wikipedia.org/wiki/Precision_and_recall">recall&lt;/a>) or in some area of performance in our system.&lt;/p>
&lt;p>There are three common quantization techniques around vector databases:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Scalar quantization&lt;/strong>, which reduces the overall size of the dimensions to a smaller data type (e.g. a 4-byte float to a 2-byte float or 1-byte integer).&lt;/li>
&lt;li>&lt;strong>Binary quantization&lt;/strong>, which is a subset of scalar quantization that reduces the dimensions to a single bit (e.g. &lt;code>&amp;gt; 0&lt;/code> to &lt;code>1&lt;/code>, &lt;code>&amp;lt;=0&lt;/code> to &lt;code>0&lt;/code>).&lt;/li>
&lt;li>&lt;strong>Product quantization&lt;/strong>, which uses a clustering technique to effectively remaps the original vector to a vector with smaller dimensionality and indexes that (e.g. reduce a vector from 128-dim to 8-dim).&lt;/li>
&lt;/ul>
&lt;p>There is already a lot of content available that does better exploration and explanation of these techniques than I will; instead I&amp;rsquo;ll focus on how to use 2 of these 3 techniques in the context of pgvector.&lt;/p>
&lt;p>The &lt;a href="https://github.com/pgvector/pgvector/issues/508">upcoming pgvector 0.7.0&lt;/a> release is planning to include functionality that allows for you to use both scalar and binary quantization as part of your indexing strategy. Specifically, pgvector 0.7.0 adds support for indexing 2-byte floats (&lt;code>halfvec&lt;/code>) and bit/binary vectors (using the PostgreSQL &lt;a href="https://www.postgresql.org/docs/current/datatype-bit.html">&lt;code>bit&lt;/code>&lt;/a> data type). The 0.7.0 will also add support for sparse vectors via &lt;code>sparsevec&lt;/code>, but that will be for a future blog post.&lt;/p>
&lt;p>&lt;strong>ASIDE&lt;/strong>: These quantization techniques also let you index vectors that are &lt;strong>larger than 2,000 dimensions&lt;/strong>, which has been a major request for pgvector. &lt;code>halfvec&lt;/code> can store up to 4,000 dimensions, &lt;code>bit&lt;/code> can store up to 64,000 dimensions, and &lt;code>sparsevec&lt;/code> can store up to 1,000 nonzero elements (which means it can store vectors with very high dimensionality).&lt;/p>
&lt;p>Using existing pgvector functionality, such as &lt;a href="https://jkatz.github.io/post/postgres/pgvector-hnsw-performance/">HNSW indexing&lt;/a>, along with PostgreSQL features like &lt;a href="https://www.postgresql.org/docs/current/indexes-expressional.html">expression indexes&lt;/a>, we can explore how scalar and binary quantization can help reduce both space and memory consumption &amp;ndash; letting us scale vector workloads even further on PostgreSQL, understand what performance gains and tradeoffs we must make, how embedding models can impact results, and determine when it makes sense to use quantization.&lt;/p>
&lt;h2 id="test-setup-and-system-configuration">Test setup and system configuration&lt;/h2>
&lt;p>Before diving in, let&amp;rsquo;s do a quick review of how we want to test these techniques. I &lt;a href="https://jkatz.github.io/post/postgres/pgvector-hnsw-performance/">previously provided guidance on testing approximate nearest neighbor (ANN)&lt;/a> algorithms over vector indexes; below is a quick summary of what we will look at over the course of this testing:&lt;/p>
&lt;ul>
&lt;li>&lt;code>Index size&lt;/code>: This is a key metric with a quantization test, as we should be building indexes that are smaller than a full (or &amp;ldquo;flat&amp;rdquo;) vector index. The interesting data is &amp;ldquo;how much smaller,&amp;rdquo; and the impacts to the other metrics we&amp;rsquo;ll review.&lt;/li>
&lt;li>&lt;code>Index build time&lt;/code>: How does quantization impact overall build time? If the build time is longer, do we gain enough in index size / recall / query performance that it&amp;rsquo;s OK to make that tradeoff? If the build time is shorter, do we lose anything in recall / query performance such that we should avoid quantization?&lt;/li>
&lt;li>&lt;code>Queries per second&lt;/code> (QPS): How does quantization impact how many queries we can execute per second, or our overall throughput?&lt;/li>
&lt;li>&lt;code>Latency&lt;/code> (p99): The time it takes to return a single result, but using a measurement that represents the 99th percentile (&amp;ldquo;very slow&amp;rdquo;) timing. This serves as an &amp;ldquo;upper bound&amp;rdquo; on our latency times.&lt;/li>
&lt;li>&lt;code>Recall&lt;/code>: A measurement of the relevancy of our results - what percentage of the expected results do we return?&lt;/li>
&lt;/ul>
&lt;p>For the test itself, I used a &lt;code>r7gd.16xlarge&lt;/code> and stored the data on the local NVMe to rule out the impact of network latency. For these tests, I only ran on the ARM-based architecture of the Graviton3; as pgvector does leverage SIMD, exact timings may vary on different architectures.&lt;/p>
&lt;p>I used PostgreSQL 16.2, with the following (relevant to this test) non-default configuration (in alphabetical order - thanks &lt;code>\dconfig&lt;/code>!):&lt;/p>
&lt;ul>
&lt;li>&lt;code>checkpoint_timeout&lt;/code>: 2h&lt;/li>
&lt;li>&lt;code>effective_cache_size&lt;/code>: 256GB&lt;/li>
&lt;li>&lt;code>jit&lt;/code>: off&lt;/li>
&lt;li>&lt;code>maintenance_work_mem&lt;/code>: 64GB&lt;/li>
&lt;li>&lt;code>max_parallel_maintenance_workers&lt;/code>: 63&lt;/li>
&lt;li>&lt;code>max_parallel_workers&lt;/code>: 64&lt;/li>
&lt;li>&lt;code>max_parallel_workers_per_gather&lt;/code>: 64&lt;/li>
&lt;li>&lt;code>max_wal_size&lt;/code>: 20GB&lt;/li>
&lt;li>&lt;code>max_worker_processes&lt;/code>: 128&lt;/li>
&lt;li>&lt;code>shared_buffers&lt;/code>: 128GB&lt;/li>
&lt;li>&lt;code>wal_compression&lt;/code>: zstd&lt;/li>
&lt;li>&lt;code>work_mem&lt;/code>: 64MB&lt;/li>
&lt;/ul>
&lt;p>For testing, I used &lt;a href="https://github.com/erikbern/ann-benchmarks">ANN Benchmarks&lt;/a> with some modifications to the pgvector module to work with scalar and binary quantization. The modifications were purely to support scalar/binary quantization and would not make a material difference in performance values. I used the following datasets, though I won&amp;rsquo;t necessarily show the data from all of them:&lt;/p>
&lt;ul>
&lt;li>mnist-784-euclidean (60K, 784-dim)&lt;/li>
&lt;li>sift-128-euclidean (1M, 128-dim)&lt;/li>
&lt;li>gist-960-euclidean (1M, 960-dim)&lt;/li>
&lt;li>dbpedia-openai-1000k-angular (1M, 1536-dim)&lt;/li>
&lt;li>glove-25-angular (1.1M, 25-dim)&lt;/li>
&lt;li>glove-100-angular (1.1M, 100-dim)&lt;/li>
&lt;/ul>
&lt;p>For this blog post, I&amp;rsquo;ll focus on results from &lt;code>sift-128-euclidean&lt;/code>, &lt;code>gist-960-euclidean&lt;/code>, and &lt;code>dbpedia-openai-1000k-angular&lt;/code>.&lt;/p>
&lt;p>For the tests, I used HNSW indexing and used the following values:&lt;/p>
&lt;ul>
&lt;li>I fixed &lt;code>m&lt;/code> at &lt;code>16&lt;/code>, but ran tests with &lt;code>ef_construction&lt;/code> at 32, 64, 128, 256, 512&lt;/li>
&lt;li>For each test, I varied &lt;code>ef_search&lt;/code> with 10, 20, 40, 80, 120, 200, 400, 800&lt;/li>
&lt;/ul>
&lt;p>Finally, for each test, I maintained the original table structure (i.e. storing the original / flat vector in the table) while quantizing the indexed vector.&lt;/p>
&lt;p>We&amp;rsquo;ll first look individually at scalar and binary quantization with pgvector, followed by a summarization of findings and some recommendations.&lt;/p>
&lt;h2 id="scalar-quantization-with-2-byte-fp16-floats-halfvec">Scalar quantization with 2-byte (fp16) floats (&lt;code>halfvec&lt;/code>)&lt;/h2>
&lt;p>Scalar quantization is often the simplest technique to use to shrink vector index storage, as it&amp;rsquo;s just a matter of converting dimensions to a smaller data type (e.g. a 4-byte float to a 2-byte float). In some ways, quantize to a 2-byte float makes sense: when performing a distance operation, the biggest &amp;ldquo;differentiator&amp;rdquo; between two dimensions is in the more significant bits of data. If we marginally reduce the information to only care about those bits, we shouldn&amp;rsquo;t see much difference in recall.&lt;/p>
&lt;p>The first thing needed for this test is to add support for 2-byte float scalar quantization when building the HNSW index. As mentioned, we can do this in PostgreSQL using an expression index. Below is an example of a table and HNSW index using cosine distance that uses scalar quantization to store a 3,072 dimensional vector, such as the ones found in the &lt;code>text-embedding-3-large&lt;/code> model:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">TABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">documents&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">bigint&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">GENERATED&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">BY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">DEFAULT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">AS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">IDENTITY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">PRIMARY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">KEY&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">content&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">text&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">3072&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">INDEX&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">ON&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">documents&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">USING&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">hnsw&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="p">::&lt;/span>&lt;span class="n">halfvec&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">3072&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">halfvec_cosine_ops&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that &lt;code>hnsw ((embedding::halfvec(3072)) halfvec_cosine_ops)&lt;/code> contains the expression (&lt;code>(embedding::halfvec(3072))&lt;/code>) that casts the full, 4-byte vector to become a 2-byte vector. Also note that we need to use a different operator class for the &lt;code>halfvec&lt;/code> data type.&lt;/p>
&lt;p>For completeness, if you want to use scalar quantization with Euclidean / L2 distance, you&amp;rsquo;d use the following query:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">INDEX&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">ON&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">documents&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">USING&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">hnsw&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="p">::&lt;/span>&lt;span class="n">halfvec&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">3072&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">halfvec_l2_ops&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>A query that would use scalar quantization would look similar to this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">FROM&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">documents&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">ORDER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">BY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="p">::&lt;/span>&lt;span class="n">halfvec&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">3072&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">&amp;lt;=&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="err">$&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">::&lt;/span>&lt;span class="n">halfvec&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">3072&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">LIMIT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>where &lt;code>$1&lt;/code> represented a parameter that&amp;rsquo;s a full vector being quantized to a 2-byte float.&lt;/p>
&lt;p>Now, let&amp;rsquo;s see how scalar quantization impacts vector search in pgvector. We&amp;rsquo;ll first see the impact on index build times. In the table below, &lt;code>vector&lt;/code> represents the index with no quantization, whereas &lt;code>halfvec&lt;/code> contains the quantized vector. We&amp;rsquo;ll only show the &lt;code>ef_construction&lt;/code> value, as &lt;code>m&lt;/code> is fixed to &lt;code>16&lt;/code>:&lt;/p>
&lt;h3 id="sift-128-euclidean">&lt;code>sift-128-euclidean&lt;/code>&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;code>ef_construction&lt;/code>&lt;/th>
&lt;th>&lt;code>vector&lt;/code> size (MB)&lt;/th>
&lt;th>&lt;code>halfvec&lt;/code> size (MB)&lt;/th>
&lt;th>Space reduction&lt;/th>
&lt;th>&lt;code>vector&lt;/code> build-time (s)&lt;/th>
&lt;th>&lt;code>halfvec&lt;/code> build-time (s)&lt;/th>
&lt;th>Speedup&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>32&lt;/td>
&lt;td>793&lt;/td>
&lt;td>544&lt;/td>
&lt;td>1.46x&lt;/td>
&lt;td>33&lt;/td>
&lt;td>31&lt;/td>
&lt;td>1.06x&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>64&lt;/td>
&lt;td>782&lt;/td>
&lt;td>538&lt;/td>
&lt;td>1.45x&lt;/td>
&lt;td>37&lt;/td>
&lt;td>34&lt;/td>
&lt;td>1.09x&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>128&lt;/td>
&lt;td>782&lt;/td>
&lt;td>538&lt;/td>
&lt;td>1.45x&lt;/td>
&lt;td>44&lt;/td>
&lt;td>40&lt;/td>
&lt;td>1.10x&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>256&lt;/td>
&lt;td>782&lt;/td>
&lt;td>538&lt;/td>
&lt;td>1.45x&lt;/td>
&lt;td>58&lt;/td>
&lt;td>51&lt;/td>
&lt;td>1.14x&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>512&lt;/td>
&lt;td>782&lt;/td>
&lt;td>536&lt;/td>
&lt;td>1.46x&lt;/td>
&lt;td>88&lt;/td>
&lt;td>74&lt;/td>
&lt;td>1.19x&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="gist-960-euclidean">&lt;code>gist-960-euclidean&lt;/code>&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;code>ef_construction&lt;/code>&lt;/th>
&lt;th>&lt;code>vector&lt;/code> size (MB)&lt;/th>
&lt;th>&lt;code>halfvec&lt;/code> size (MB)&lt;/th>
&lt;th>Space reduction&lt;/th>
&lt;th>&lt;code>vector&lt;/code> build-time (s)&lt;/th>
&lt;th>&lt;code>halfvec&lt;/code> build-time (s)&lt;/th>
&lt;th>Speedup&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>32&lt;/td>
&lt;td>7,811&lt;/td>
&lt;td>2,603&lt;/td>
&lt;td>3.00x&lt;/td>
&lt;td>145&lt;/td>
&lt;td>68&lt;/td>
&lt;td>2.13x&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>64&lt;/td>
&lt;td>7,684&lt;/td>
&lt;td>2,561&lt;/td>
&lt;td>3.00x&lt;/td>
&lt;td>161&lt;/td>
&lt;td>77&lt;/td>
&lt;td>2.09x&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>128&lt;/td>
&lt;td>7,680&lt;/td>
&lt;td>2,560&lt;/td>
&lt;td>3.00x&lt;/td>
&lt;td>190&lt;/td>
&lt;td>101&lt;/td>
&lt;td>1.88x&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>256&lt;/td>
&lt;td>7,678&lt;/td>
&lt;td>2,559&lt;/td>
&lt;td>3.00x&lt;/td>
&lt;td>247&lt;/td>
&lt;td>147&lt;/td>
&lt;td>1.68x&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>512&lt;/td>
&lt;td>7,678&lt;/td>
&lt;td>2,559&lt;/td>
&lt;td>3.00x&lt;/td>
&lt;td>349&lt;/td>
&lt;td>229&lt;/td>
&lt;td>1.52x&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="dbpedia-openai-1000k-angular">&lt;code>dbpedia-openai-1000k-angular&lt;/code>&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;code>ef_construction&lt;/code>&lt;/th>
&lt;th>&lt;code>vector&lt;/code> size (MB)&lt;/th>
&lt;th>&lt;code>halfvec&lt;/code> size (MB)&lt;/th>
&lt;th>Space reduction&lt;/th>
&lt;th>&lt;code>vector&lt;/code> build-time (s)&lt;/th>
&lt;th>&lt;code>halfvec&lt;/code> build-time (s)&lt;/th>
&lt;th>Speedup&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>32&lt;/td>
&lt;td>7,734&lt;/td>
&lt;td>3,867&lt;/td>
&lt;td>2.00x&lt;/td>
&lt;td>244&lt;/td>
&lt;td>77&lt;/td>
&lt;td>3.17x&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>64&lt;/td>
&lt;td>7,734&lt;/td>
&lt;td>3,867&lt;/td>
&lt;td>2.00x&lt;/td>
&lt;td>264&lt;/td>
&lt;td>90&lt;/td>
&lt;td>2.93x&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>128&lt;/td>
&lt;td>7,734&lt;/td>
&lt;td>3,867&lt;/td>
&lt;td>2.00x&lt;/td>
&lt;td>301&lt;/td>
&lt;td>115&lt;/td>
&lt;td>2.62x&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>256&lt;/td>
&lt;td>7,734&lt;/td>
&lt;td>3,867&lt;/td>
&lt;td>2.00x&lt;/td>
&lt;td>377&lt;/td>
&lt;td>163&lt;/td>
&lt;td>2.31x&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>512&lt;/td>
&lt;td>7,734&lt;/td>
&lt;td>3,867&lt;/td>
&lt;td>2.00x&lt;/td>
&lt;td>508&lt;/td>
&lt;td>254&lt;/td>
&lt;td>2.00x&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The above shows that using &lt;code>halfvec&lt;/code> to index a vector has both space savings and index build time benefits for vectors with larger dimensions. Without quantization, &lt;code>gist-960-euclidean&lt;/code> (960-dim) and &lt;code>dbpedia-openai-1000k-angular&lt;/code> (1536-dim) vectors can respectively only fit 2 and 1 vectors on an index page. Quantizing to a &lt;code>halfvec&lt;/code> allows to store up to respectively store 4 and 2 on a page, which lines up with th space saving numbers we see. With less pages required, it also makes sense that we&amp;rsquo;d see a build speedup, as pgvector doesn&amp;rsquo;t need to look up as many pages when placing a new vector into the index. The &lt;code>sift-128-euclidean&lt;/code> (128-dim) tests did showed space savings, the overall changes in build times were minimal, though importantly they did not show regression.&lt;/p>
&lt;p>It seems like this means we should start using 2-byte float scalar quantization, right? We can&amp;rsquo;t make that determination just yet &amp;ndash; we need to understand the impact of these changes on query performance and recall. for the purposes of this blog, we&amp;rsquo;ll look at the query performance values at &lt;code>hnsw.ef_search&lt;/code> set to &lt;code>10&lt;/code>, &lt;code>40&lt;/code>, &lt;code>200&lt;/code>, and &lt;code>800&lt;/code>. 10 is the minimum value we can set to for this test (given the &lt;code>LIMIT 10&lt;/code>) and will give us a lower bound. 40 is the &lt;code>hnsw.ef_search&lt;/code> default for pgvector. At 200, we&amp;rsquo;d expect to see high recall, and at 800 we should be close to converging towards perfect recall (if we haven&amp;rsquo;t done so already).&lt;/p>
&lt;p>Visualizing these results is a bit challenging, as there are several variables to consider (dataset, quantization, &lt;code>ef_construction&lt;/code>, &lt;code>ef_search&lt;/code>) along with several outputs (QPS, p99 latency, recall). For simplicity, we&amp;rsquo;ll consider the data at &lt;code>ef_construction=256&lt;/code>, as that is the build value I recommend for folks to use for index builds when using the parallel build functionality. You can see the results below:&lt;/p>
&lt;h3 id="sift-128-euclidean--ef_construction256">&lt;code>sift-128-euclidean&lt;/code> @ &lt;code>ef_construction=256&lt;/code>&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;code>hnsw.ef_search&lt;/code>&lt;/th>
&lt;th>&lt;code>vector&lt;/code> recall&lt;/th>
&lt;th>&lt;code>halfvec&lt;/code> recall&lt;/th>
&lt;th>&lt;code>vector&lt;/code> QPS&lt;/th>
&lt;th>&lt;code>halfvec&lt;/code> QPS&lt;/th>
&lt;th>&lt;code>vector&lt;/code> p99 latency (ms)&lt;/th>
&lt;th>&lt;code>halfvec&lt;/code> p99 latency (ms)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>77.7%&lt;/td>
&lt;td>77.5%&lt;/td>
&lt;td>2,100&lt;/td>
&lt;td>2,161&lt;/td>
&lt;td>0.71&lt;/td>
&lt;td>0.68&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>40&lt;/td>
&lt;td>95.4%&lt;/td>
&lt;td>95.4%&lt;/td>
&lt;td>1,020&lt;/td>
&lt;td>1,097&lt;/td>
&lt;td>1.19&lt;/td>
&lt;td>1.20&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>200&lt;/td>
&lt;td>99.8%&lt;/td>
&lt;td>99.8%&lt;/td>
&lt;td>268&lt;/td>
&lt;td>293&lt;/td>
&lt;td>4.60&lt;/td>
&lt;td>4.38&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>800&lt;/td>
&lt;td>100.0%&lt;/td>
&lt;td>100.0%&lt;/td>
&lt;td>84&lt;/td>
&lt;td>91&lt;/td>
&lt;td>15.07&lt;/td>
&lt;td>14.80&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="gist-960-euclidean--ef_construction256">&lt;code>gist-960-euclidean&lt;/code> @ &lt;code>ef_construction=256&lt;/code>&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;code>hnsw.ef_search&lt;/code>&lt;/th>
&lt;th>&lt;code>vector&lt;/code> recall&lt;/th>
&lt;th>&lt;code>halfvec&lt;/code> recall&lt;/th>
&lt;th>&lt;code>vector&lt;/code> QPS&lt;/th>
&lt;th>&lt;code>halfvec&lt;/code> QPS&lt;/th>
&lt;th>&lt;code>vector&lt;/code> p99 latency (ms)&lt;/th>
&lt;th>&lt;code>halfvec&lt;/code> p99 latency (ms)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>50.4%&lt;/td>
&lt;td>50.1%&lt;/td>
&lt;td>1,114&lt;/td>
&lt;td>1,207&lt;/td>
&lt;td>1.36&lt;/td>
&lt;td>1.21&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>40&lt;/td>
&lt;td>78.0%&lt;/td>
&lt;td>78.1%&lt;/td>
&lt;td>513&lt;/td>
&lt;td>579&lt;/td>
&lt;td>2.64&lt;/td>
&lt;td>2.30&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>200&lt;/td>
&lt;td>96.0%&lt;/td>
&lt;td>96.0%&lt;/td>
&lt;td>135&lt;/td>
&lt;td>156&lt;/td>
&lt;td>8.70&lt;/td>
&lt;td>7.49&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>800&lt;/td>
&lt;td>99.6%&lt;/td>
&lt;td>99.6%&lt;/td>
&lt;td>41&lt;/td>
&lt;td>47&lt;/td>
&lt;td>29.20&lt;/td>
&lt;td>25.44&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="dbpedia-openai-1000k-angular--ef_construction256">&lt;code>dbpedia-openai-1000k-angular&lt;/code> @ &lt;code>ef_construction=256&lt;/code>&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;code>hnsw.ef_search&lt;/code>&lt;/th>
&lt;th>&lt;code>vector&lt;/code> recall&lt;/th>
&lt;th>&lt;code>halfvec&lt;/code> recall&lt;/th>
&lt;th>&lt;code>vector&lt;/code> QPS&lt;/th>
&lt;th>&lt;code>halfvec&lt;/code> QPS&lt;/th>
&lt;th>&lt;code>vector&lt;/code> p99 latency (ms)&lt;/th>
&lt;th>&lt;code>halfvec&lt;/code> p99 latency (ms)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>85.1%&lt;/td>
&lt;td>85.2%&lt;/td>
&lt;td>1,162&lt;/td>
&lt;td>1,163&lt;/td>
&lt;td>1.40&lt;/td>
&lt;td>1.21&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>40&lt;/td>
&lt;td>96.8%&lt;/td>
&lt;td>96.8%&lt;/td>
&lt;td>567&lt;/td>
&lt;td>578&lt;/td>
&lt;td>2.70&lt;/td>
&lt;td>2.61&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>200&lt;/td>
&lt;td>99.6%&lt;/td>
&lt;td>99.6%&lt;/td>
&lt;td>156&lt;/td>
&lt;td>163&lt;/td>
&lt;td>9.01&lt;/td>
&lt;td>8.59&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>800&lt;/td>
&lt;td>99.9%&lt;/td>
&lt;td>99.9%&lt;/td>
&lt;td>48&lt;/td>
&lt;td>51&lt;/td>
&lt;td>30.50&lt;/td>
&lt;td>28.49&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>These are very good results for 2-byte float quantization with &lt;code>halfvec&lt;/code>. In addition to the improved build performance, we see that (at least at &lt;code>ef_construction=256&lt;/code>) the recall values between &lt;code>vector&lt;/code> and &lt;code>halfvec&lt;/code> are nearly idenical, and query performance is identical or slightly better (particularly with latency) with &lt;code>halfvec&lt;/code>.&lt;/p>
&lt;p>Across all the tests I ran with the ANN Benchmark datasets listed above, I saw similar results. This makes a strong case that moving forward, you should first attempt to use HNSW with 2-byte float quantization, as you&amp;rsquo;ll save space and improve index build times while maintaining comparable performance to storing the full vector! If you&amp;rsquo;re not seeing the recall that you want, you can increase &lt;code>m&lt;/code>/&lt;code>ef_construction&lt;/code>, or fall back to using the full 4-byte float vector in your index.&lt;/p>
&lt;p>Let&amp;rsquo;s now explore binary quantization.&lt;/p>
&lt;h2 id="binary-quantization">Binary quantization&lt;/h2>
&lt;p>Binary quantization is a more extreme quantization technique: it can reduce the full value of the dimension of a vector to a single bit of information. Specifically, binary quantization will reduce any positive value to &lt;code>1&lt;/code>, and any zero or negative value to &lt;code>0&lt;/code>. This can certainly lead to a huge reduction in memory/storage utilization, and likely query performance, as you&amp;rsquo;re storing much more data on a single page. However, this can have a notable impact on recall, as we&amp;rsquo;ll see in the experiments below.&lt;/p>
&lt;p>I&amp;rsquo;m going to spoil one of the experiments: using binary quantization on its own can lead to poor recall if there&amp;rsquo;s not enough bit-diversity in your vectors. Typically, you&amp;rsquo;ll see more diversity amongst vectors with higher dimensionality, as these vectors are more likely to have varied bits. However, you can improve recall by &amp;ldquo;reranking,&amp;rdquo; i.e. you use binary quantization to narrow down your set of vectors, and then you reorder the reduced set of vectors by using the original (flat) vectors stored in the table.&lt;/p>
&lt;p>First, let&amp;rsquo;s see how we can set up a table to support binary quantization. We&amp;rsquo;ll first look at the results of building a HNSW index that stores binary quantized vectors, and then compare results between using binary quantization with and without re-ranking.&lt;/p>
&lt;p>Recall that pgvector ultimately gets support for binary quantization through supporting vector operations through the PostgreSQL &lt;code>bit&lt;/code> type. The pgvector 0.7.0 release provides two different bitwise distance functions: &lt;a href="https://en.wikipedia.org/wiki/Jaccard_index">Jaccard&lt;/a> (&lt;code>&amp;lt;%&amp;gt;&lt;/code>) and &lt;a href="https://en.wikipedia.org/wiki/Hamming_distance">Hamming distance&lt;/a> (&lt;code>&amp;lt;~&amp;gt;&lt;/code>). For these tests, I chose the Hamming distance function due to its performance characteristics, though I plan/hope to go into a deeper dive on these two options in a future blog post.&lt;/p>
&lt;p>Below is an example of how you can create a table and index that supports binary quantization and Hamming distance:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">TABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">documents&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">bigint&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">GENERATED&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">BY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">DEFAULT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">AS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">IDENTITY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">PRIMARY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">KEY&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">content&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">text&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">3072&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">INDEX&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">ON&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">documents&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">USING&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">hnsw&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">binary_quantize&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="p">)::&lt;/span>&lt;span class="nb">bit&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">3072&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">bit_hamming_ops&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Again, notice we use a PostgreSQL expression index to quantize the embedding, and explicitly cast the bit dimensionality. That final cast is important: &lt;code>binary_quantize&lt;/code> just returns a bit string, and pgvector index building needs to detect and explicit dimension.&lt;/p>
&lt;p>The following query is how to search the index using binary quantization:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">FROM&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">documents&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">ORDER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">BY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">binary_quantize&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="p">)::&lt;/span>&lt;span class="nb">bit&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">3072&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">&amp;lt;~&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">binary_quantize&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="err">$&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">LIMIT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>where $1 represents a parameterized value that is a &lt;code>vector&lt;/code> type.&lt;/p>
&lt;p>As mentioned earlier, we&amp;rsquo;ll also run an experiment that involves re-ranking the results before returning them. The following query does exactly that; note that the subquery has a higher limit, but it will ultimately be governed by the value of &lt;code>hnsw.ef_search&lt;/code> (you can always set the &lt;code>LIMIT&lt;/code> to be the value of &lt;code>hnsw.ef_search&lt;/code>).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">FROM&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">&amp;lt;=&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="err">$&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">AS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">distance&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">FROM&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">items&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">ORDER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">BY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">binary_quantize&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="p">)::&lt;/span>&lt;span class="nb">bit&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">3072&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">&amp;lt;~&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">binary_quantize&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="err">$&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">LIMIT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">800&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="c1">-- bound by hnsw.ef_search
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">ORDER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">BY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">distance&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">LIMIT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>where $1 represents a parameterized value that is a &lt;code>vector&lt;/code> type. Note that in the SELECT list of the subqery, there is the &lt;code>embedding &amp;lt;=&amp;gt; $1 AS distance&lt;/code> expression: this performs the distance calculation that is ultimately used in the rerank. This example uses cosine distance, but you can choose your preferred distance metric.&lt;/p>
&lt;p>Now, let&amp;rsquo;s see how binary quantization with and without reanking impacts vector search in pgvector. As before, we&amp;rsquo;ll first see the impact on index build times. In the table below, &lt;code>vector&lt;/code> represents the index with no quantization, whereas &lt;code>bit&lt;/code> contains the binary quantized vector. Note that reranking only impacts the query itself, so we don&amp;rsquo;t have to show multiple sets of build times. We&amp;rsquo;ll only show the &lt;code>ef_construction&lt;/code> value, as &lt;code>m&lt;/code> is fixed to &lt;code>16&lt;/code>:&lt;/p>
&lt;h3 id="sift-128-euclidean-1">&lt;code>sift-128-euclidean&lt;/code>&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;code>ef_construction&lt;/code>&lt;/th>
&lt;th>&lt;code>vector&lt;/code> size (MB)&lt;/th>
&lt;th>&lt;code>bit&lt;/code> size (MB)&lt;/th>
&lt;th>Space reduction&lt;/th>
&lt;th>&lt;code>vector&lt;/code> build-time (s)&lt;/th>
&lt;th>&lt;code>bit&lt;/code> build-time (s)&lt;/th>
&lt;th>Speedup&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>32&lt;/td>
&lt;td>793&lt;/td>
&lt;td>304&lt;/td>
&lt;td>2.61x&lt;/td>
&lt;td>33&lt;/td>
&lt;td>28&lt;/td>
&lt;td>1.18x&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>64&lt;/td>
&lt;td>782&lt;/td>
&lt;td>298&lt;/td>
&lt;td>2.62x&lt;/td>
&lt;td>37&lt;/td>
&lt;td>31&lt;/td>
&lt;td>1.19x&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>128&lt;/td>
&lt;td>782&lt;/td>
&lt;td>298&lt;/td>
&lt;td>2.62x&lt;/td>
&lt;td>44&lt;/td>
&lt;td>37&lt;/td>
&lt;td>1.19x&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>256&lt;/td>
&lt;td>782&lt;/td>
&lt;td>298&lt;/td>
&lt;td>2.62x&lt;/td>
&lt;td>58&lt;/td>
&lt;td>46&lt;/td>
&lt;td>1.26x&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>512&lt;/td>
&lt;td>782&lt;/td>
&lt;td>298&lt;/td>
&lt;td>2.62x&lt;/td>
&lt;td>88&lt;/td>
&lt;td>67&lt;/td>
&lt;td>1.31x&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="gist-960-euclidean-1">&lt;code>gist-960-euclidean&lt;/code>&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;code>ef_construction&lt;/code>&lt;/th>
&lt;th>&lt;code>vector&lt;/code> size (MB)&lt;/th>
&lt;th>&lt;code>bit&lt;/code> size (MB)&lt;/th>
&lt;th>Space reduction&lt;/th>
&lt;th>&lt;code>vector&lt;/code> build-time (s)&lt;/th>
&lt;th>&lt;code>bit&lt;/code> build-time (s)&lt;/th>
&lt;th>Speedup&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>32&lt;/td>
&lt;td>7,811&lt;/td>
&lt;td>405&lt;/td>
&lt;td>19.29x&lt;/td>
&lt;td>145&lt;/td>
&lt;td>77&lt;/td>
&lt;td>1.88x&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>64&lt;/td>
&lt;td>7,684&lt;/td>
&lt;td>405&lt;/td>
&lt;td>18.97x&lt;/td>
&lt;td>161&lt;/td>
&lt;td>76&lt;/td>
&lt;td>2.12x&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>128&lt;/td>
&lt;td>7,680&lt;/td>
&lt;td>405&lt;/td>
&lt;td>18.96x&lt;/td>
&lt;td>190&lt;/td>
&lt;td>76&lt;/td>
&lt;td>2.50x&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>256&lt;/td>
&lt;td>7,678&lt;/td>
&lt;td>405&lt;/td>
&lt;td>18.96x&lt;/td>
&lt;td>247&lt;/td>
&lt;td>77&lt;/td>
&lt;td>3.20x&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>512&lt;/td>
&lt;td>7,678&lt;/td>
&lt;td>405&lt;/td>
&lt;td>18.96x&lt;/td>
&lt;td>349&lt;/td>
&lt;td>76&lt;/td>
&lt;td>4.59x&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="dbpedia-openai-1000k-angular-1">&lt;code>dbpedia-openai-1000k-angular&lt;/code>&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;code>ef_construction&lt;/code>&lt;/th>
&lt;th>&lt;code>vector&lt;/code> size (MB)&lt;/th>
&lt;th>&lt;code>bit&lt;/code> size (MB)&lt;/th>
&lt;th>Space reduction&lt;/th>
&lt;th>&lt;code>vector&lt;/code> build-time (s)&lt;/th>
&lt;th>&lt;code>bit&lt;/code> build-time (s)&lt;/th>
&lt;th>Speedup&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>32&lt;/td>
&lt;td>7,734&lt;/td>
&lt;td>473&lt;/td>
&lt;td>16.35x&lt;/td>
&lt;td>244&lt;/td>
&lt;td>151&lt;/td>
&lt;td>1.62x&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>64&lt;/td>
&lt;td>7,734&lt;/td>
&lt;td>473&lt;/td>
&lt;td>16.35x&lt;/td>
&lt;td>264&lt;/td>
&lt;td>155&lt;/td>
&lt;td>1.70x&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>128&lt;/td>
&lt;td>7,734&lt;/td>
&lt;td>473&lt;/td>
&lt;td>16.35x&lt;/td>
&lt;td>301&lt;/td>
&lt;td>162&lt;/td>
&lt;td>1.86x&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>256&lt;/td>
&lt;td>7,734&lt;/td>
&lt;td>473&lt;/td>
&lt;td>16.35x&lt;/td>
&lt;td>377&lt;/td>
&lt;td>187&lt;/td>
&lt;td>2.02x&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>512&lt;/td>
&lt;td>7,734&lt;/td>
&lt;td>473&lt;/td>
&lt;td>16.35x&lt;/td>
&lt;td>508&lt;/td>
&lt;td>227&lt;/td>
&lt;td>2.24x&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>From these tests, we see that the space savings becomes significantly more pronounced as the dimensions of the vectors becomes larger, but we do see the reduction trailing off for the 1536-dim vector. This comes back earlier to the fact that a PostgreSQL page is 8KB: we can fit about 68 &lt;code>bit(960)&lt;/code> on an index page, whereas we can only fit about 42 &lt;code>bit(1536)&lt;/code> on the same page. However, notice that we don&amp;rsquo;t have much space savings for the 128-dim vector: this makes sense, as we&amp;rsquo;re only transforming 128 bytes (1024 bits) into 128 bits, and while it&amp;rsquo;s an 8x reduction in information, we&amp;rsquo;re already efficiently clustering the flat 128-dim vectors in the index pages. The other interesting element is that the speedups, while at least faster for the larger vectors, don&amp;rsquo;t correlate with the space reduction. There is likely still some optimization work we can do in pgvector to speed up the Hamming distance function.&lt;/p>
&lt;p>Now let&amp;rsquo;s at the impact on query performance and recall. First, let&amp;rsquo;s evaluate performance and recall without any reranking:&lt;/p>
&lt;h3 id="sift-128-euclidean--ef_construction256-no-rerank">&lt;code>sift-128-euclidean&lt;/code> @ &lt;code>ef_construction=256&lt;/code> (no rerank)&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;code>hnsw.ef_search&lt;/code>&lt;/th>
&lt;th>&lt;code>vector&lt;/code> recall&lt;/th>
&lt;th>&lt;code>bit&lt;/code> recall&lt;/th>
&lt;th>&lt;code>vector&lt;/code> QPS&lt;/th>
&lt;th>&lt;code>bit&lt;/code> QPS&lt;/th>
&lt;th>&lt;code>vector&lt;/code> p99 latency (ms)&lt;/th>
&lt;th>&lt;code>bit&lt;/code> p99 latency (ms)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>77.7%&lt;/td>
&lt;td>2.18%&lt;/td>
&lt;td>2,100&lt;/td>
&lt;td>2,412&lt;/td>
&lt;td>0.71&lt;/td>
&lt;td>0.61&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>40&lt;/td>
&lt;td>95.4%&lt;/td>
&lt;td>2.42%&lt;/td>
&lt;td>1,020&lt;/td>
&lt;td>1,095&lt;/td>
&lt;td>1.19&lt;/td>
&lt;td>1.25&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>200&lt;/td>
&lt;td>99.8%&lt;/td>
&lt;td>2.52%&lt;/td>
&lt;td>268&lt;/td>
&lt;td>280&lt;/td>
&lt;td>4.60&lt;/td>
&lt;td>4.91&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>800&lt;/td>
&lt;td>100.0%&lt;/td>
&lt;td>2.52%&lt;/td>
&lt;td>84&lt;/td>
&lt;td>86&lt;/td>
&lt;td>15.07&lt;/td>
&lt;td>16.05&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="gist-960-euclidean--ef_construction256-no-rerank">&lt;code>gist-960-euclidean&lt;/code> @ &lt;code>ef_construction=256&lt;/code> (no rerank)&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;code>hnsw.ef_search&lt;/code>&lt;/th>
&lt;th>&lt;code>vector&lt;/code> recall&lt;/th>
&lt;th>&lt;code>bit&lt;/code> recall&lt;/th>
&lt;th>&lt;code>vector&lt;/code> QPS&lt;/th>
&lt;th>&lt;code>bit&lt;/code> QPS&lt;/th>
&lt;th>&lt;code>vector&lt;/code> p99 latency (ms)&lt;/th>
&lt;th>&lt;code>bit&lt;/code> p99 latency (ms)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>50.4%&lt;/td>
&lt;td>0.00%&lt;/td>
&lt;td>1,114&lt;/td>
&lt;td>6,050&lt;/td>
&lt;td>1.36&lt;/td>
&lt;td>0.18&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>40&lt;/td>
&lt;td>78.0%&lt;/td>
&lt;td>0.00%&lt;/td>
&lt;td>513&lt;/td>
&lt;td>4,847&lt;/td>
&lt;td>2.64&lt;/td>
&lt;td>0.24&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>200&lt;/td>
&lt;td>96.0%&lt;/td>
&lt;td>0.00%&lt;/td>
&lt;td>135&lt;/td>
&lt;td>4,057&lt;/td>
&lt;td>8.70&lt;/td>
&lt;td>0.27&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>800&lt;/td>
&lt;td>99.6%&lt;/td>
&lt;td>0.00%&lt;/td>
&lt;td>41&lt;/td>
&lt;td>3,871&lt;/td>
&lt;td>29.20&lt;/td>
&lt;td>0.28&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="dbpedia-openai-1000k-angular--ef_construction256-no-rerank">&lt;code>dbpedia-openai-1000k-angular&lt;/code> @ &lt;code>ef_construction=256&lt;/code> (no rerank)&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;code>hnsw.ef_search&lt;/code>&lt;/th>
&lt;th>&lt;code>vector&lt;/code> recall&lt;/th>
&lt;th>&lt;code>bit&lt;/code> recall&lt;/th>
&lt;th>&lt;code>vector&lt;/code> QPS&lt;/th>
&lt;th>&lt;code>bit&lt;/code> QPS&lt;/th>
&lt;th>&lt;code>vector&lt;/code> p99 latency (ms)&lt;/th>
&lt;th>&lt;code>bit&lt;/code> p99 latency (ms)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>85.1%&lt;/td>
&lt;td>60.1%&lt;/td>
&lt;td>1,162&lt;/td>
&lt;td>1,556&lt;/td>
&lt;td>1.40&lt;/td>
&lt;td>1.02&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>40&lt;/td>
&lt;td>96.8%&lt;/td>
&lt;td>66.8%&lt;/td>
&lt;td>567&lt;/td>
&lt;td>848&lt;/td>
&lt;td>2.70&lt;/td>
&lt;td>1.79&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>200&lt;/td>
&lt;td>99.6%&lt;/td>
&lt;td>68.3%&lt;/td>
&lt;td>156&lt;/td>
&lt;td>251&lt;/td>
&lt;td>9.01&lt;/td>
&lt;td>5.61&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>800&lt;/td>
&lt;td>99.9%&lt;/td>
&lt;td>68.6%&lt;/td>
&lt;td>48&lt;/td>
&lt;td>71&lt;/td>
&lt;td>30.50&lt;/td>
&lt;td>19.57&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The above shows why with approximate nearest neighbor search, you &lt;strong>must&lt;/strong> measure performance and recall. Without doing so, you&amp;rsquo;d walk away thinking that using binary quantization for the &lt;code>gist-960-euclidean&lt;/code> set is a clear, high performant winner, but you&amp;rsquo;d be returning garbage results. Similarly, &lt;code>sift-128-euclidean&lt;/code> has terrible recall with binary quantization, but does not show much difference in query performance between the flat value.&lt;/p>
&lt;p>However, &lt;code>dbpedia-openai-1000k-angular&lt;/code> looks promising: while the recall numbers are not great, they&amp;rsquo;re significantly higher than the others, likely due to the bit-diversity between the values. We may be able to improve our results if we re-rank. Remember that our rerank query looks something like this (using a 3072-dim vector):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">FROM&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">&amp;lt;=&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="err">$&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">AS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">distance&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">FROM&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">items&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">ORDER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">BY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">binary_quantize&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="p">)::&lt;/span>&lt;span class="nb">bit&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">3072&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">&amp;lt;~&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">binary_quantize&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="err">$&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">LIMIT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">800&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="c1">-- bound by hnsw.ef_search
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">ORDER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">BY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">distance&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">LIMIT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Let&amp;rsquo;s look at the same results with reranking:&lt;/p>
&lt;h3 id="sift-128-euclidean--ef_construction256-with-rerank">&lt;code>sift-128-euclidean&lt;/code> @ &lt;code>ef_construction=256&lt;/code> (with rerank)&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;code>hnsw.ef_search&lt;/code>&lt;/th>
&lt;th>&lt;code>vector&lt;/code> recall&lt;/th>
&lt;th>&lt;code>bit&lt;/code> recall&lt;/th>
&lt;th>&lt;code>vector&lt;/code> QPS&lt;/th>
&lt;th>&lt;code>bit&lt;/code> QPS&lt;/th>
&lt;th>&lt;code>vector&lt;/code> p99 latency (ms)&lt;/th>
&lt;th>&lt;code>bit&lt;/code> p99 latency (ms)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>77.7%&lt;/td>
&lt;td>2.31%&lt;/td>
&lt;td>2,100&lt;/td>
&lt;td>2,194&lt;/td>
&lt;td>0.71&lt;/td>
&lt;td>0.65&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>40&lt;/td>
&lt;td>95.4%&lt;/td>
&lt;td>4.19%&lt;/td>
&lt;td>1,020&lt;/td>
&lt;td>971&lt;/td>
&lt;td>1.19&lt;/td>
&lt;td>1.34&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>200&lt;/td>
&lt;td>99.8%&lt;/td>
&lt;td>8.88%&lt;/td>
&lt;td>268&lt;/td>
&lt;td>246&lt;/td>
&lt;td>4.60&lt;/td>
&lt;td>5.32&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>800&lt;/td>
&lt;td>100.0%&lt;/td>
&lt;td>15.69%&lt;/td>
&lt;td>84&lt;/td>
&lt;td>76&lt;/td>
&lt;td>15.07&lt;/td>
&lt;td>17.28&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="gist-960-euclidean--ef_construction256-with-rerank">&lt;code>gist-960-euclidean&lt;/code> @ &lt;code>ef_construction=256&lt;/code> (with rerank)&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;code>hnsw.ef_search&lt;/code>&lt;/th>
&lt;th>&lt;code>vector&lt;/code> recall&lt;/th>
&lt;th>&lt;code>bit&lt;/code> recall&lt;/th>
&lt;th>&lt;code>vector&lt;/code> QPS&lt;/th>
&lt;th>&lt;code>bit&lt;/code> QPS&lt;/th>
&lt;th>&lt;code>vector&lt;/code> p99 latency (ms)&lt;/th>
&lt;th>&lt;code>bit&lt;/code> p99 latency (ms)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>50.4%&lt;/td>
&lt;td>0.00%&lt;/td>
&lt;td>1,114&lt;/td>
&lt;td>2,744&lt;/td>
&lt;td>1.36&lt;/td>
&lt;td>0.40&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>40&lt;/td>
&lt;td>78.0%&lt;/td>
&lt;td>0.00%&lt;/td>
&lt;td>513&lt;/td>
&lt;td>1,030&lt;/td>
&lt;td>2.64&lt;/td>
&lt;td>1.00&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>200&lt;/td>
&lt;td>96.0%&lt;/td>
&lt;td>0.00%&lt;/td>
&lt;td>135&lt;/td>
&lt;td>569&lt;/td>
&lt;td>8.70&lt;/td>
&lt;td>1.79&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>800&lt;/td>
&lt;td>99.6%&lt;/td>
&lt;td>0.00%&lt;/td>
&lt;td>41&lt;/td>
&lt;td>562&lt;/td>
&lt;td>29.20&lt;/td>
&lt;td>1.82&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="dbpedia-openai-1000k-angular--ef_construction256-with-rerank">&lt;code>dbpedia-openai-1000k-angular&lt;/code> @ &lt;code>ef_construction=256&lt;/code> (with rerank)&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;code>hnsw.ef_search&lt;/code>&lt;/th>
&lt;th>&lt;code>vector&lt;/code> recall&lt;/th>
&lt;th>&lt;code>bit&lt;/code> recall&lt;/th>
&lt;th>&lt;code>vector&lt;/code> QPS&lt;/th>
&lt;th>&lt;code>bit&lt;/code> QPS&lt;/th>
&lt;th>&lt;code>vector&lt;/code> p99 latency (ms)&lt;/th>
&lt;th>&lt;code>bit&lt;/code> p99 latency (ms)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>85.1%&lt;/td>
&lt;td>60.1%&lt;/td>
&lt;td>1,162&lt;/td>
&lt;td>1,503&lt;/td>
&lt;td>1.40&lt;/td>
&lt;td>1.03&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>40&lt;/td>
&lt;td>96.8%&lt;/td>
&lt;td>91.6%&lt;/td>
&lt;td>567&lt;/td>
&lt;td>760&lt;/td>
&lt;td>2.70&lt;/td>
&lt;td>1.91&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>200&lt;/td>
&lt;td>99.6%&lt;/td>
&lt;td>99.0%&lt;/td>
&lt;td>156&lt;/td>
&lt;td>222&lt;/td>
&lt;td>9.01&lt;/td>
&lt;td>6.01&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>800&lt;/td>
&lt;td>99.9%&lt;/td>
&lt;td>99.8%&lt;/td>
&lt;td>48&lt;/td>
&lt;td>62&lt;/td>
&lt;td>30.50&lt;/td>
&lt;td>21.34&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>So, what do we learn with binary quantization and reranking? With &lt;code>sift-128-euclidean&lt;/code> and re-ranking, we&amp;rsquo;re able to improve on recall as we increase &lt;code>hnsw.ef_search&lt;/code>, but the recall is still very poor. The &lt;code>gist-960-euclidean&lt;/code> test is still returning garbage, but not as quickly as before.&lt;/p>
&lt;p>But the &lt;code>dbpedia-openai-1000k-angular&lt;/code> results are very interesting. Once we expand our search radius vis-a-vis &lt;code>hnsw.ef_search=40&lt;/code>, we see that we get a 1.34x boost in QPS and a 29% reduction in p99 latency with only sacrificing 5% in recall. This is using an index that&amp;rsquo;s 16x smaller and builds twice as fast! Moving up to &lt;code>hnsw.ef_search=200&lt;/code>, we get 1.42x faster QPS with a 33% decrease in p99 latency with comparable recall!&lt;/p>
&lt;p>There are a few takeaways from this. First, given how binary quantization works, the experiments show that to ensure your results have meaning, you most certainly will need to perform a reranking. Additionally, bit-diversity matters: the most distinct values we have in the index, the more likely we&amp;rsquo;ll be able to get distinguishable results that lead to higher recall. This leads to a decision though: how much recall matters? If you&amp;rsquo;re doing k=10 / top-10 queries, 90% should be good enough, which means in the above example, you can use &lt;code>hnsw.ef_search=40&lt;/code> on the &lt;code>dbpedia-openai-1000k-angular&lt;/code> dataset to achieve low latency queries that meet your recall target.&lt;/p>
&lt;p>One indirect observation I have from these tests is around how the overall size of a dataset impacts recall for binary quantization. We see good results on the &lt;code>dbpedia-openai-1000k-angular&lt;/code> dataset because the overall dataset size is small (1,000,000) and has good diversity amongst the bit vectors. However, looking at the results of the other two data sets, I would be willing to bet that recall degrades as the 1536-dim data set gets larger (e.g. 1,000,000,000) and requires a larger &lt;code>hnsw.ef_search&lt;/code> + reranking to achieve the same results.&lt;/p>
&lt;p>Finally, for all of these experiments, note that the workload was able to fit entirely into memory (the benefit of using a r7gd.16xlarge) to ensure we&amp;rsquo;re getting a fair comparison between methods. However, in the &amp;ldquo;real-world,&amp;rdquo; binary quantization lets you keep your index entirely in memory on much smaller systems - for example, with &lt;code>dbpedia-openai-1000k-angular&lt;/code> the binary quantized index takes up 473MB vs. 7,734MB (7.5GB) for the full index. If you&amp;rsquo;re getting the recall results you want from an index using binary quantization, this is an effective technique to reduce your overall storage and memory footprint in your system.&lt;/p>
&lt;h2 id="future-experiments">Future experiments&lt;/h2>
&lt;p>While the results are really interesting, there is still more to test (and develop!) around quantization. A few more experiments I&amp;rsquo;d like to run in the future:&lt;/p>
&lt;ul>
&lt;li>With &lt;code>dbpedia-openai-1000k-angular&lt;/code>, quantize the &lt;code>vector&lt;/code> to &lt;code>halfvec&lt;/code> in the table, and then use binary quantization on the &lt;code>halfvec&lt;/code>. I suspect that we reduce the storage/memory footprint even further, get slightly higher QPS/better p99 latency, and don&amp;rsquo;t impact recall.&lt;/li>
&lt;li>Similarly, test storing everything as &lt;code>halfvec&lt;/code> in the table and then building the &lt;code>halfvec&lt;/code> index. I also suspect comparable recall, but possibly better query performance numbers. This does lose precision in the vector stored in the table, so I think the results will be model dependent.&lt;/li>
&lt;li>Test the explicit impact of CPU, i.e. SIMD, acceleration. The unreleased PostgreSQL 17 has planned support for AVX-512 some functions used for computing binary distances; we may be able to further speed up those distance functions.&lt;/li>
&lt;li>Test the hypothesis around recall with binary quantization degrading on much larger datasets as there&amp;rsquo;s less bit diversity amongst the values.&lt;/li>
&lt;/ul>
&lt;h2 id="conclusion-what-if-any-quantization-technique-should-i-use">Conclusion: What (if any) quantization technique should I use?&lt;/h2>
&lt;p>When I give a &lt;a href="https://jkatz.github.io/talks/">talk&lt;/a> on pgvector or vector search in general, my concluding slide always ends with &amp;ldquo;plan for today and tomorrow.&amp;rdquo; I think these quantization techniques are a great example of that.&lt;/p>
&lt;p>First and foremost, pgvector maintains its changes as additive, meaning that as you upgrade, you will not need to make costly on-disk changes. Adopting a new feature may mean reindexing, though fortunately PostgreSQL and pgvector support the nonblocking &lt;code>REINDEX CONCURRENTLY&lt;/code> operation.&lt;/p>
&lt;p>That said, I think there are a few clear takeaways and tradeoffs to consider when determining what quantization technique to use:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Scalar quantization from 4-byte to 2-byte floats looks like a clear winner&lt;/strong>, at least amongst these datasets. I feel comfortable recommending storing the &lt;code>vector&lt;/code> in the table and quantizing to &lt;code>halfvec&lt;/code> in the index. I&amp;rsquo;d like to see the results over larger datasets, but I still think this is a really good starting point.&lt;/li>
&lt;li>&lt;strong>Binary quantization works better for datasets that produce larger vectors that can be distinguished by their bits&lt;/strong>. The more distinguished bits, the better recall we&amp;rsquo;ll achieve.
&lt;ul>
&lt;li>I also suspect that as these datasets get very large, we start to see degraded recall results.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Effective quantization allows you to reduce your storage/memory footprint, which on one hand means you can use smaller instances, but it also means you can scale your workload even further&lt;/strong>. This is great news for pgvector, particularly as we see more &lt;a href="https://jkatz.github.io/post/postgres/distributed-pgvector/">&amp;ldquo;billion-scale&amp;rdquo; and beyond datasets&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>While ANN Benchmarks and other ANN testing tools for databases do provide a good set of datasets to evaluate quantization techniques, this may not reflect the results you see with your actual workload. You still need to test to see if scalar or binary quantization make sense. However, one of my goals is to give you a guide on how to make that evaluation, and hopefully save some time in the process.&lt;/p></description></item><item><title>Will PostgreSQL ever change its license?</title><link>https://jkatz.github.io/post/postgres/postgres-license-2024/</link><pubDate>Wed, 20 Mar 2024 00:00:00 +0000</pubDate><guid>https://jkatz.github.io/post/postgres/postgres-license-2024/</guid><description>&lt;p>(Disclosure: I&amp;rsquo;m on the &lt;a href="https://www.postgresql.org/developer/core/">PostgreSQL Core Team&lt;/a>, but what&amp;rsquo;s written in this post are my personal views and not official project statements&amp;hellip;unless I link to something that&amp;rsquo;s an official project statement ;)&lt;/p>
&lt;p>I was very sad to learn today that the &lt;a href="https://redis.com/blog/redis-adopts-dual-source-available-licensing/">Redis project will no longer be released under an open source license&lt;/a>. Sad for two reasons: as a longtime Redis user and pretty early adopter, and as an open source contributor. I&amp;rsquo;ll preface that I&amp;rsquo;m empathetic to the challenges of building businesses around open source, having been on multiple sides of this equation. I&amp;rsquo;m also cognizant of the downstream effects of these changes that can completely flip how a user adopts and uses a piece of technology.&lt;/p>
&lt;p>Whenever there&amp;rsquo;s a shakeup in open source licensing, particularly amongst databases and related systems (MySQL =&amp;gt; Sun =&amp;gt; Oracle being the one that first springs to mind), I&amp;rsquo;ll hear the question &amp;ldquo;Will PostgreSQL ever change its license?&amp;rdquo;&lt;/p>
&lt;p>The PostgreSQL website &lt;a href="https://www.postgresql.org/about/licence/">has an answer&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>Will PostgreSQL ever be released under a different license?
The PostgreSQL Global Development Group remains committed to making PostgreSQL available as free and open &amp;gt; source software in perpetuity. There are no plans to change the PostgreSQL License or release PostgreSQL under a different license.&lt;/p>
&lt;/blockquote>
&lt;p>(Disclosure: I did help write the above paragraph).&lt;/p>
&lt;p>&lt;a href="https://www.postgresql.org/about/licence/">The PostgreSQL Licence&lt;/a> (aka &amp;ldquo;License&amp;rdquo; &amp;ndash; &lt;a href="https://pgsnake.blogspot.com/">Dave Page&lt;/a> and I have fun going back and forth on this) is an &lt;a href="https://opensource.org/license/postgresql">Open Source Initiative (OSI) recognized license&lt;/a>, and has a very permissive model. In terms of which license it&amp;rsquo;s most similar to, I defer to this email that &lt;a href="https://www.postgresql.org/message-id/1776.1256525282@sss.pgh.pa.us">Tom Lane wrote in 2009&lt;/a>.&lt;/p>
&lt;p>That said, there are a few reasons why PostgreSQL won&amp;rsquo;t change it&amp;rsquo;s license:&lt;/p>
&lt;ul>
&lt;li>It&amp;rsquo;s &amp;ldquo;&lt;a href="https://www.postgresql.org/about/licence/">The PostgreSQL Licence&lt;/a>&amp;rdquo; &amp;ndash; why change license when you have it named after the project?&lt;/li>
&lt;li>The PostgreSQL Project began as a collaborative open source effort and is set up to prevent a single entity to take control. This carries through in the project&amp;rsquo;s ethos almost 30 years later, and is even codified throughout the &lt;a href="https://www.postgresql.org/about/policies/">project policies&lt;/a>.&lt;/li>
&lt;li>&lt;a href="https://www.postgresql.org/message-id/937d27e10910260840s1d28aab2o799f2c58d14dfb1e%40mail.gmail.com">Dave Page explicitly said so in this email&lt;/a> :)&lt;/li>
&lt;/ul>
&lt;p>The question then becomes - is there a reason that PostgreSQL would change its license? Typically these changes happen as part of a business decision - but it seems that business around PostgreSQL is as robust as its feature set. Ruohang Feng (Vonng) recently &lt;a href="https://medium.com/@fengruohang/postgres-is-eating-the-database-world-157c204dcfc4">wrote a blog post&lt;/a> that highlighted just a slice of the PostgreSQL software and business ecosystem that&amp;rsquo;s been built around it, which is only possible through the PostgreSQL Licence. I say &amp;ldquo;just a slice&amp;rdquo; because there&amp;rsquo;s even more, both historically and current, projects and business that are built up around some portion of the PostgreSQL codebase. While many of these projects may be released under different licenses or be closed source, they have helped drive, both directly and indirectly, PostgreSQL adoption, and have helped make the PostgreSQL protocol ubiquitous.&lt;/p>
&lt;p>But the biggest reason why PostgreSQL would not change its license is the disservice it would do to all PostgreSQL users. It takes a long time to build trust in a technology that is often used for the most critical part of an application: storage and retrieval of data. &lt;a href="https://www.postgresql.org/about/">PostgreSQL has earned a strong reputation for its proven architecture, reliability, data integrity, robust feature set, extensibility, and the dedication of the open source community behind the software to consistently deliver performant and innovative solutions&lt;/a>. Changing the license of PostgreSQL would shatter all of the goodwill the project has built up through the past (nearly) 30 years.&lt;/p>
&lt;p>While there are definitely parts of the PostgreSQL project that are imperfect (and I certainly contribute to those imperfections), the PostgreSQL Licence is a true gift to the PostgreSQL community and open source in general that we&amp;rsquo;ll continue to cherish and help keep PostgreSQL truly free and open source. After all, it says &lt;a href="https://www.postgresql.org/about/licence/">so on the website&lt;/a> ;)&lt;/p></description></item><item><title>Distributed queries for pgvector</title><link>https://jkatz.github.io/post/postgres/distributed-pgvector/</link><pubDate>Mon, 18 Mar 2024 00:00:00 +0000</pubDate><guid>https://jkatz.github.io/post/postgres/distributed-pgvector/</guid><description>&lt;p>The past few releases of &lt;a href="https://github.com/pgvector/pgvector">pgvector&lt;/a> have emphasized features that help to vertically scale, particularly around index build parallelism. Scaling vertically is convenient for many reasons, especially because it&amp;rsquo;s simpler to continue managing data that&amp;rsquo;s located within a single instance.&lt;/p>
&lt;p>Performance of querying vector data tends to be memory-bound, meaning that the more vector data you can keep in memory, the faster your database will return queries. It&amp;rsquo;s also completely acceptable to not have your entire vector workload contained within memory, as long as you&amp;rsquo;re meeting your latency requirements.&lt;/p>
&lt;p>However, they may be a point that you can&amp;rsquo;t vertically scale any further, such as not having an instance large enough to keep your entire vector dataset in memory. However, there may be a way to combine &lt;a href="https://www.postgresql.org">PostgreSQL&lt;/a> features with pgvector to create a multi-node system to run distributed, performant queries across multiple instances.&lt;/p>
&lt;p>To see how this works, we&amp;rsquo;ll need to explore several features in PostgreSQL that help with segmenting and distributing data, including &lt;a href="https://www.postgresql.org/docs/current/ddl-partitioning.html">partitioning&lt;/a> and &lt;a href="https://www.postgresql.org/docs/current/ddl-foreign-data.html">foreign data wrappers&lt;/a>. We&amp;rsquo;ll see how we can use these features to run distributed queries with pgvector, and explore the &amp;ldquo;can we&amp;rdquo; / &amp;ldquo;should we&amp;rdquo; questions.&lt;/p>
&lt;h2 id="partitioning-and-pgvector">Partitioning and pgvector&lt;/h2>
&lt;p>Partitioning is a general database technique that lets you divide data in a single table over multiple tables, and is used for purposes such as archiving, segmenting by time, and reducing the overall portion of a data set that you need to search over. PostgreSQL supports three types of partitioning: &lt;a href="https://www.postgresql.org/docs/current/ddl-partitioning.html#DDL-PARTITIONING-OVERVIEW-RANGE">range&lt;/a>, &lt;a href="https://www.postgresql.org/docs/current/ddl-partitioning.html#DDL-PARTITIONING-OVERVIEW-LIST">list&lt;/a>, and &lt;a href="https://www.postgresql.org/docs/current/ddl-partitioning.html#DDL-PARTITIONING-OVERVIEW-HASH">hash&lt;/a>. You use list and range partitioning when you have a defined partition key (e.g. &lt;code>company_id&lt;/code> or &lt;code>start_date BETWEEN '2024-03-01' AND '2024-03-31&lt;/code>), whereas you use hash partitioning when you want to evenly distribute your data across partitions.&lt;/p>
&lt;p>There are many considerations you must make before adopting a partitioning strategy, including understanding how your application will interact with your partitioned table and your partition management strategy. You also want to ensure you don&amp;rsquo;t create &amp;ldquo;too many partitions,&amp;rdquo; which is an upper bound that&amp;rsquo;s increased over the past several PostgreSQL releases (1000s is acceptable, depending on strategy).&lt;/p>
&lt;p>&lt;a href="https://github.com/pgvector/pgvector/?tab=readme-ov-file#filtering">pgvector works natively with PostgreSQL partitioning&lt;/a>. Based upon the structure of your data and query patterns, you may choose to partition your data and build individual indexes over each partition as a way to enable &amp;ldquo;prefiltering&amp;rdquo; (your &lt;code>WHERE&lt;/code> clause) of your queries, for example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">TABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">documents&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">category_id&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1536&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">PARTITION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">BY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">HASH&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">category_id&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">TABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">documents_0&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">PARTITION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">OF&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">documents&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">FOR&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">VALUES&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">WITH&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">MODULUS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">REMAINDER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">TABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">documents_1&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">PARTITION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">OF&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">documents&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">FOR&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">VALUES&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">WITH&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">MODULUS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">REMAINDER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="c1">-- recursively creates an index on each partition
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">INDEX&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">ON&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">documents&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">USING&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">hnsw&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embeddings&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vector_cosine_ops&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>However, the above example also shows one of the challenges with partitioning if you&amp;rsquo;re searching over a partition with either mixed data in your filter, or with multiple filters. Based on your search parameter value (&lt;code>hnsw.ef_search&lt;/code> or &lt;code>ivfflat.probes&lt;/code>), you may not return enough results from the index that match all of the filters. You can remedy this by increasing the values of &lt;code>hnsw.ef_search&lt;/code> or &lt;code>ivfflat.probes&lt;/code>; as of this writing, there is also work going into pgvector to add more prefiltering options.&lt;/p>
&lt;p>Now that we&amp;rsquo;ve had a brief overview of partitioning, let&amp;rsquo;s look at foreign data wrappers.&lt;/p>
&lt;h2 id="foreign-data-wrappers-and-postgres_fdw">Foreign data wrappers and &lt;code>postgres_fdw&lt;/code>&lt;/h2>
&lt;p>&lt;a href="https://wiki.postgresql.org/wiki/Foreign_data_wrappers">Foreign data wrappers&lt;/a> (&lt;a href="https://wiki.postgresql.org/wiki/Foreign_data_wrappers">FDWs&lt;/a>), part of the &lt;a href="https://wiki.postgresql.org/wiki/SQL/MED">SQL/MED&lt;/a> standard, let you work with remote data sources from SQL. PostgreSQL has &lt;a href="https://wiki.postgresql.org/wiki/Foreign_data_wrappers">many different FDWs&lt;/a>, from the built-in &lt;a href="https://www.postgresql.org/docs/current/postgres-fdw.html">&lt;code>postgres_fdw&lt;/code>&lt;/a>, to others that can read/write to other databases or remote sources. Foreign data wrappers are used to run federated queries (queries across instances), push data into remote systems, and can be used to migrate data to PostgreSQL.&lt;/p>
&lt;p>The &lt;code>postgres_fdw&lt;/code> lets you work with data across PostgreSQL databases. Since it&amp;rsquo;s introduction in &lt;a href="https://www.postgresql.org/docs/release/9.3.0/">PostgreSQL 9.3&lt;/a>, the &lt;code>postgres_fdw&lt;/code> has added features to make it possible to run distributed workloads, including different query pushdowns (executing queries on remote instances), convenience functions for loading remote schemas, and running queries asynchronous across remote instances. While the &lt;code>postgres_fdw&lt;/code> is a good solution for certain types of data federation, but is not yet a full solution for sharding, as sharding requires additional data management techniques (e.g., node management / rebalancing).&lt;/p>
&lt;p>Using the &lt;code>postgres_fdw&lt;/code> in production requires security considerations, including authentication/authorization managemet, including on both the local and remote servers, and network management. I won&amp;rsquo;t be getting into these techniques here, but if you decide you want to use this in production, you&amp;rsquo;ll want to follow security best practices. At &lt;a href="https://2015.pgconf.eu/">PGConf EU 2015&lt;/a>, I gave a presentation on &lt;a href="https://www.slideshare.net/jkatz05/developing-and-deploying-apps-with-the-postgres-fdw">lessons learned deploying the &lt;code>postgres_fdw&lt;/code> in production&lt;/a>; most of those lessons still apply!&lt;/p>
&lt;p>With partitioning and FDWs, we now have the tools to run distributed pgvector queries!&lt;/p>
&lt;h2 id="distributed-queries-for-pgvector">Distributed queries for pgvector&lt;/h2>
&lt;p>We can combine partitioning and the &lt;code>postgres_fdw&lt;/code> to run pgvector queries across multiple instances. For this experiment, I set up 3 instances:&lt;/p>
&lt;ul>
&lt;li>Two r7gd.4xlarge (16 vCPU, 128GB RAM) instances. These are called &lt;code>node1&lt;/code> and &lt;code>node2&lt;/code> in this post.&lt;/li>
&lt;li>One &amp;ldquo;head node&amp;rdquo; that sends the data to the different systems, which I&amp;rsquo;ll refer to as &lt;code>head&lt;/code>. I used a smaller instance here, as most of the work is handled by &lt;code>node1&lt;/code> / &lt;code>node2&lt;/code>.&lt;/li>
&lt;/ul>
&lt;p>Additionally, to reduce latency, I kept all of these instances within the same availability zone, and I used the local disk (NVMe) for storage on &lt;code>node1&lt;/code> and &lt;code>node2&lt;/code>.&lt;/p>
&lt;p>On &lt;code>node1&lt;/code> and &lt;code>node2&lt;/code>, I used the following applicable &lt;code>postgresql.conf&lt;/code> configuration parameters:&lt;/p>
&lt;ul>
&lt;li>&lt;code>effective_cache_size&lt;/code>: 64GB&lt;/li>
&lt;li>&lt;code>maintenance_work_mem&lt;/code>: 32GB&lt;/li>
&lt;li>&lt;code>max_parallel_maintenance_workers&lt;/code>: 15&lt;/li>
&lt;li>&lt;code>max_parallel_workers&lt;/code>: 16&lt;/li>
&lt;li>&lt;code>max_parallel_workers_per_gather&lt;/code>: 16&lt;/li>
&lt;li>&lt;code>max_worker_processes&lt;/code>: 32&lt;/li>
&lt;li>&lt;code>shared_buffers&lt;/code>: 32GB&lt;/li>
&lt;li>&lt;code>work_mem&lt;/code>: 64MB&lt;/li>
&lt;/ul>
&lt;p>For all the tests, I used &lt;a href="https://github.com/pgvector/pgvector/commit/0d35a141">pgvector@0d35a141&lt;/a>, which is a few commits off from the code in the v0.6.2 release.&lt;/p>
&lt;p>I ran two tests:&lt;/p>
&lt;ol>
&lt;li>&amp;ldquo;Can we&amp;rdquo;: determine if this method is even feasible.&lt;/li>
&lt;li>&amp;ldquo;Should we&amp;rdquo;: demonstrate how federation technique &lt;a href="https://jkatz.github.io/post/postgres/pgvector-hnsw-performance/">impacts recall and query performance&lt;/a> using &lt;a href="https://github.com/erikbern/ann-benchmarks">ANN Benchmarks&lt;/a>.&lt;/li>
&lt;/ol>
&lt;h3 id="test-1-can-we---feasibility-of-pgvector-distributed-queries">Test 1: &amp;ldquo;Can we&amp;rdquo; - feasibility of pgvector distributed queries&lt;/h3>
&lt;p>First, we need to set up the schema on each node. For this test, I ran the following commands on &lt;code>node1&lt;/code> and &lt;code>node2&lt;/code> in a database called &lt;code>vectors&lt;/code> with a user called &lt;code>distpgv&lt;/code> that has create permissions in that database:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">EXTENSION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">IF&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">NOT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">EXISTS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">TABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vectors&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">uuid&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">PRIMARY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">KEY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">DEFAULT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">gen_random_uuid&lt;/span>&lt;span class="p">(),&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">node_id&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">NOT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">NULL&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">DEFAULT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="c1">-- change to 2 on node 2
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">768&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">ALTER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">TABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vectors&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">ALTER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">COLUMN&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">SET&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">STORAGE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">PLAIN&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The following function is used to generate synthetic data:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">OR&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">REPLACE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">FUNCTION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">public&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">generate_random_normalized_vector&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">integer&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">RETURNS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">LANGUAGE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">plpgsql&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">AS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="err">$&lt;/span>&lt;span class="k">function&lt;/span>&lt;span class="err">$&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">DECLARE&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">a&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">real&lt;/span>&lt;span class="p">[];&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">mag&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">real&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">real&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">BEGIN&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">array_agg&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="p">()::&lt;/span>&lt;span class="nb">real&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">INTO&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">a&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">FROM&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">generate_series&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">mag&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">public&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">vector_norm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">a&lt;/span>&lt;span class="p">::&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">FOR&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">IN&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">..&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">LOOP&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">a&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">a&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">mag&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">END&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">LOOP&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">RETURN&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">a&lt;/span>&lt;span class="p">::&lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">END&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="err">$&lt;/span>&lt;span class="k">function&lt;/span>&lt;span class="err">$&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For this test, I ran the following on each instance to insert 5MM rows (2.5MM on each instance):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">INSERT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">INTO&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vectors&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">generate_random_normalized_vector&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">768&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">FROM&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">generate_series&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="n">_500_000&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once the inserts were completed, I ran the following to build the index (adjust &lt;code>max_parallel_maintenance_workers&lt;/code> to the available cores on your environment):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">SET&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">max_parallel_maintenance_workers&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">TO&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">16&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">INDEX&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">ON&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vectors&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">USING&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">hnsw&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vector_cosine_ops&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">WITH&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">ef_construction&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">256&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>With this, the data is now set up on each of the nodes. Back on the &lt;code>head&lt;/code> instance, we need to set up the FDW and connections to the instances.&lt;/p>
&lt;p>On &lt;code>head&lt;/code>, first install pgvector and the &lt;code>postgres_fdw&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">EXTENSION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">IF&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">NOT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">EXISTS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">EXTENSION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">IF&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">NOT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">EXISTS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">postgres_fdw&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The next set of SQL is the key bit that will allow us to run distributed pgvector queries. You&amp;rsquo;ll create two &amp;ldquo;servers,&amp;rdquo; which will tell PostgreSQL where your remote nodes are, and what their capabilities are:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">SERVER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vectors1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">FOREIGN&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">DATA&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">WRAPPER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">postgres_fdw&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">OPTIONS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">async_capable&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;true&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">extensions&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;vector&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">dbname&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;vectors&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">host&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;&amp;lt;NODE1&amp;gt;&amp;#39;&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">SERVER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vectors2&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">FOREIGN&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">DATA&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">WRAPPER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">postgres_fdw&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">OPTIONS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">async_capable&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;true&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">extensions&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;vector&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">dbname&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;vectors&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">host&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;&amp;lt;NODE2&amp;gt;&amp;#39;&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Aside from the connection parameters, there are two key options to know:&lt;/p>
&lt;ul>
&lt;li>&lt;code>async_capable&lt;/code>: This means that the remote server is capable of executing queries asynchronously. This lets us simultaneously execute pgvector queries on each node, instead of executing them one-at-a-time (serially).&lt;/li>
&lt;li>&lt;code>extensions&lt;/code>: This indicates what extensions are available on the remote server. This allows us to pushdown the pgvector index lookups to the remote nodes.&lt;/li>
&lt;/ul>
&lt;p>Next, we need to add connection information for how a local PostgreSQL user will connect to the remote instances. Substitute &lt;code>local_user&lt;/code> for the user you create on your local instance:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">USER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">MAPPING&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">FOR&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">local_user&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">SERVER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vectors1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">OPTIONS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">user&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;pgvdist&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">password&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;&amp;lt;SECUREPASSWORD&amp;gt;&amp;#39;&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">USER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">MAPPING&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">FOR&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">local_user&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">SERVER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vectors2&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">OPTIONS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">user&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;pgvdist&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">password&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;&amp;lt;SECUREPASSWORD&amp;gt;&amp;#39;&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Finally, we&amp;rsquo;ll create a partitioned table, where each partition is a &amp;ldquo;&lt;a href="https://www.postgresql.org/docs/current/ddl-foreign-data.html">foreign table&lt;/a>,&amp;rdquo; or a reference to a table located on the remote server.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">TABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vectors&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">uuid&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">node_id&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">768&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="n">PARTITION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">BY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">LIST&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">node_id&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="c1">-- reference to the table on node 1
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">FOREIGN&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">TABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vectors_node1&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">PARTITION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">OF&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vectors&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">FOR&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">VALUES&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">IN&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">SERVER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vectors1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">OPTIONS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">schema_name&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;public&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">table_name&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;vectors&amp;#39;&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="c1">-- reference to the table on node 2
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">FOREIGN&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">TABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vectors_node2&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">PARTITION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">OF&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vectors&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">FOR&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">VALUES&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">IN&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">SERVER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vectors2&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">OPTIONS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">schema_name&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;public&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">table_name&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;vectors&amp;#39;&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Let&amp;rsquo;s test to see that we can query the remote tables. Below is an example of the output from an explain plan that counts all the rows across each node:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">EXPLAIN&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">count&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">FROM&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vectors&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>yields:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl"> Aggregate (cost=2687700.00..2687700.01 rows=1 width=8)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> -&amp;gt; Append (cost=100.00..2675200.00 rows=5000000 width=0)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> -&amp;gt; Async Foreign Scan on vectors_node1 vectors_1 (cost=100.00..1325100.00 rows=2500000 width=0)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> -&amp;gt; Async Foreign Scan on vectors_node2 vectors_2 (cost=100.00..1325100.00 rows=2500000 width=0)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can see that the PostgreSQL query planner will attempt to perform an &amp;ldquo;async foreign scan,&amp;rdquo; which means it will run each query asynchronously on the individual nodes, and then determine the final results on the head node. When I used &lt;code>EXPLAIN ANALYZE&lt;/code>, I received the following output that showed the asynchronous execution:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl"> Aggregate (cost=2687700.00..2687700.01 rows=1 width=8) (actual time=5755.761..5755.762 rows=1 loops=1)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> -&amp;gt; Append (cost=100.00..2675200.00 rows=5000000 width=0) (actual time=0.603..5415.869 rows=5000000 loops=1)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> -&amp;gt; Async Foreign Scan on vectors_node1 vectors_1 (cost=100.00..1325100.00 rows=2500000 width=0) (actual time=0.319..1192.879 rows=2500000 loops=1)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> -&amp;gt; Async Foreign Scan on vectors_node2 vectors_2 (cost=100.00..1325100.00 rows=2500000 width=0) (actual time=0.303..1140.443 rows=2500000 loops=1)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Planning Time: 0.169 ms
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Execution Time: 5756.769 ms
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now for the big test: running a distributed / federated query for pgvector. First, let&amp;rsquo;s create some test data. I ran this command using &lt;code>psql&lt;/code> to store the query vector in a session variable called &lt;code>v&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">generate_random_normalized_vector&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">768&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">AS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">v&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="err">\&lt;/span>&lt;span class="n">gset&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>I ran the following query to find the 10 nearest neighbors across all the nodes:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">node_id&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="s1">&amp;#39;v&amp;#39;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">&amp;lt;=&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">AS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">distance&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">FROM&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vectors&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">ORDER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">BY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">distance&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">LIMIT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>which yielded:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl"> id | node_id | distance
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">--------------------------------------+---------+---------------------
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 9a88e75f-4a03-4964-9435-f6596087db7f | 2 | 0.20965892220421145
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 251fe736-e06b-4a30-ad98-10132fd04db6 | 2 | 0.20981185523973567
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 3d024d54-b01e-468a-9e3b-d0d63a46c599 | 2 | 0.21044588244644724
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> d03cd294-d6dc-4074-a614-a2514a64a035 | 1 | 0.2111870772354053
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 9e5db921-c4f7-4bb8-b840-81d1c5fd4d02 | 2 | 0.21178618704635432
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> c6edadd6-c5d6-4fd3-9f15-8c6b67e01986 | 2 | 0.212410164619098
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> bc1822aa-3cfd-4614-8a87-d93909e00e49 | 2 | 0.2132984165340187
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 1563e694-c84e-4ed3-9285-e0f33e5717c5 | 1 | 0.21351215879655328
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> c5616138-629b-4dac-97be-8da2a031593c | 1 | 0.21449695955189663
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 6056e8f5-c52a-4f4d-8ca5-3c160d6116d3 | 2 | 0.21495852514977798
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>with the following execution plan:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">Limit (cost=200.01..206.45 rows=10 width=28) (actual time=18.171..18.182 rows=10 loops=1)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> -&amp;gt; Merge Append (cost=200.01..3222700.01 rows=5000000 width=28) (actual time=18.169..18.179 rows=10 loops=1)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Sort Key: ((&amp;#39;$1&amp;#39;::vector &amp;lt;=&amp;gt; vectors.embedding))
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> -&amp;gt; Foreign Scan on vectors_node1 vectors_1 (cost=100.00..1586350.00 rows=2500000 width=28) (actual time=8.607..8.609 rows=2 loops=1)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> -&amp;gt; Foreign Scan on vectors_node2 vectors_2 (cost=100.00..1586350.00 rows=2500000 width=28) (actual time=9.559..9.566 rows=9 loops=1)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Planning Time: 0.298 ms
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Execution Time: 19.355 ms
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Success! We can see that we were able to run the distributed query and get the most similarity vectors regardless of node they were on. Based on the timings, we also see that used the HNSW indexes that were available on each instance.&lt;/p>
&lt;p>However, on closer inspection, we can see that we didn&amp;rsquo;t perform an async foreign scan, but executed each statement serially. Currently (based on my read of the PostgreSQL docs and code), PostgreSQL does not support async foreign scans with a &amp;ldquo;Merge Append&amp;rdquo; node (e.g., running multiple operations that require merging sort results, such as a K-NN sort). This may not matter if you don&amp;rsquo;t have many nodes, but could factor into larger, distributed data sets.&lt;/p>
&lt;p>Now that we&amp;rsquo;ve answered &amp;ldquo;could we,&amp;rdquo; let&amp;rsquo;s look at one aspect of &amp;ldquo;should we&amp;rdquo; and measure recall across a distributed data set.&lt;/p>
&lt;h2 id="test-2-should-we---ann-benchmarks-using-distributed-queries-for-pgvector">Test 2: &amp;ldquo;Should we&amp;rdquo; - ANN Benchmarks using distributed queries for pgvector&lt;/h2>
&lt;p>For the next test, we&amp;rsquo;ll see how using this federation technique impacts recall and query performance. We want to test recall to ensure that distributing the data does not cause a regression in the quality of the results. Understanding query performance can help provide guidance on if/when we want to use this technique at all. I specifically did not test other aspects of vector storage (e.g., index size) as this was less of a factor in the differences in the environments tested in this example.&lt;/p>
&lt;p>In this experiment, we&amp;rsquo;ll compare the 2-node federated setup in the previous example to a single instance (1-node, using a r7g.4xlarge) set up. I had to make a few modifications to the pgvector test in &lt;a href="https://github.com/erikbern/ann-benchmarks">ANN Benchmarks&lt;/a> to support distributed queries, specifically:&lt;/p>
&lt;ul>
&lt;li>Supporting creating tables and indexes on individual nodes (in this case, &lt;code>node1&lt;/code> and &lt;code>node2&lt;/code>)&lt;/li>
&lt;li>Push setting &lt;code>hnsw.ef_search&lt;/code> down to individual nodes. As there isn&amp;rsquo;t a way to federate the values of session variables from &lt;code>head&lt;/code> to &lt;code>node1&lt;/code> and &lt;code>node2&lt;/code> (or if there is, I didn&amp;rsquo;t figure it out), I explicitly set the values using &lt;a href="https://www.postgresql.org/docs/current/sql-altersystem.html">&lt;code>ALTER SYSTEM&lt;/code>&lt;/a> and &lt;code>pg_reload_conf()&lt;/code>.&lt;/li>
&lt;li>On the &lt;code>head&lt;/code> node, I created a partitioned table with a hash partitioning method with references to the remote nodes:&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">TABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">items&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">PARTITION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">BY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">HASH&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="c1">-- reference to the table on node 1
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">FOREIGN&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">TABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">items_node1&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">PARTITION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">OF&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">items&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">FOR&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">VALUES&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">WITH&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">MODULUS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">REMAINDER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">SERVER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vectors1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">OPTIONS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">schema_name&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;public&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">table_name&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;items&amp;#39;&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="c1">-- reference to the table on node 2
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">FOREIGN&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">TABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">items_node2&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">PARTITION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">OF&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">items&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">FOR&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">VALUES&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">WITH&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">MODULUS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">REMAINDER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">SERVER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vectors2&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">OPTIONS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">schema_name&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;public&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">table_name&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;items&amp;#39;&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For all tests, I used the following build parameters:&lt;/p>
&lt;ul>
&lt;li>&lt;code>m&lt;/code>: 16&lt;/li>
&lt;li>&lt;code>ef_construction&lt;/code>: 256&lt;/li>
&lt;/ul>
&lt;p>Below are the results from three different datasets available in ANN Benchmarks (for my thoughts on these datasets, &lt;a href="https://jkatz.github.io/post/postgres/pgvector-hnsw-performance/">please see this blog post&lt;/a>), followed by analysis:&lt;/p>
&lt;ul>
&lt;li>&lt;code>sift-128-euclidean&lt;/code> (1MM, 128-dim)&lt;/li>
&lt;li>&lt;code>dbpedia-openai-1000k-angular&lt;/code> (1MM, 1536-dim)&lt;/li>
&lt;li>&lt;code>gist-960-euclidean&lt;/code> (1MM, 960-dim)&lt;/li>
&lt;/ul>
&lt;p>Recall is measured from 0 to 1 (0% to 100% relevant results returned). Queries per second (QPS) is measured in how many queries complete per second :)&lt;/p>
&lt;h4 id="sift-128-euclidean">sift-128-euclidean&lt;/h4>
&lt;p>&lt;img src="https://jkatz.github.io/images/pgvector-distributed-sift128-recall.png" alt="pgvector-distributed-sift128-recall.png">&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;code>ef_search&lt;/code>&lt;/th>
&lt;th>Recall (1-node)&lt;/th>
&lt;th>Recall (2-node)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>0.776&lt;/td>
&lt;td>0.833&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>20&lt;/td>
&lt;td>0.884&lt;/td>
&lt;td>0.922&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>40&lt;/td>
&lt;td>0.953&lt;/td>
&lt;td>0.973&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>80&lt;/td>
&lt;td>0.986&lt;/td>
&lt;td>0.994&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>120&lt;/td>
&lt;td>0.994&lt;/td>
&lt;td>0.998&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>200&lt;/td>
&lt;td>0.998&lt;/td>
&lt;td>0.999&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>400&lt;/td>
&lt;td>1.000&lt;/td>
&lt;td>1.000&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>800&lt;/td>
&lt;td>1.000&lt;/td>
&lt;td>1.000&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;img src="https://jkatz.github.io/images/pgvector-distributed-sift128-qps.png" alt="pgvector-distributed-sift128-qps.png">&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;code>ef_search&lt;/code>&lt;/th>
&lt;th>QPS (1-node)&lt;/th>
&lt;th>QPS (2-node)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>2111&lt;/td>
&lt;td>338&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>20&lt;/td>
&lt;td>1536&lt;/td>
&lt;td>272&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>40&lt;/td>
&lt;td>1031&lt;/td>
&lt;td>206&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>80&lt;/td>
&lt;td>644&lt;/td>
&lt;td>137&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>120&lt;/td>
&lt;td>474&lt;/td>
&lt;td>109&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>200&lt;/td>
&lt;td>276&lt;/td>
&lt;td>84&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>400&lt;/td>
&lt;td>157&lt;/td>
&lt;td>59&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>800&lt;/td>
&lt;td>86&lt;/td>
&lt;td>36&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="dbpedia-openai-1000k-angular">dbpedia-openai-1000k-angular&lt;/h4>
&lt;p>&lt;img src="https://jkatz.github.io/images/pgvector-distributed-dbpedia1000k-recall.png" alt="pgvector-distributed-dbpedia1000k-recall.png">&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;code>ef_search&lt;/code>&lt;/th>
&lt;th>Recall (1-node)&lt;/th>
&lt;th>Recall (2-node)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>0.852&lt;/td>
&lt;td>0.885&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>20&lt;/td>
&lt;td>0.927&lt;/td>
&lt;td>0.945&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>40&lt;/td>
&lt;td>0.968&lt;/td>
&lt;td>0.977&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>80&lt;/td>
&lt;td>0.987&lt;/td>
&lt;td>0.991&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>120&lt;/td>
&lt;td>0.992&lt;/td>
&lt;td>0.995&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>200&lt;/td>
&lt;td>0.996&lt;/td>
&lt;td>0.997&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>400&lt;/td>
&lt;td>0.998&lt;/td>
&lt;td>0.999&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>800&lt;/td>
&lt;td>0.999&lt;/td>
&lt;td>0.999&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;img src="https://jkatz.github.io/images/pgvector-distributed-dbpedia1000k-qps.png" alt="pgvector-distributed-dbpedia1000k-qps.png">&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;code>ef_search&lt;/code>&lt;/th>
&lt;th>QPS (1-node)&lt;/th>
&lt;th>QPS (2-node)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>1131&lt;/td>
&lt;td>89&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>20&lt;/td>
&lt;td>853&lt;/td>
&lt;td>55&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>40&lt;/td>
&lt;td>550&lt;/td>
&lt;td>31&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>80&lt;/td>
&lt;td>342&lt;/td>
&lt;td>17&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>120&lt;/td>
&lt;td>251&lt;/td>
&lt;td>13&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>200&lt;/td>
&lt;td>163&lt;/td>
&lt;td>13&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>400&lt;/td>
&lt;td>93&lt;/td>
&lt;td>11&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>800&lt;/td>
&lt;td>52&lt;/td>
&lt;td>9&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="gist-960-euclidean">gist-960-euclidean&lt;/h4>
&lt;p>&lt;img src="https://jkatz.github.io/images/pgvector-distributed-gist960-recall.png" alt="pgvector-distributed-gist960-recall.png">&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;code>ef_search&lt;/code>&lt;/th>
&lt;th>Recall (1-node)&lt;/th>
&lt;th>Recall (2-node)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>0.509&lt;/td>
&lt;td>0.584&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>20&lt;/td>
&lt;td>0.652&lt;/td>
&lt;td>0.733&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>40&lt;/td>
&lt;td>0.783&lt;/td>
&lt;td>0.852&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>80&lt;/td>
&lt;td>0.883&lt;/td>
&lt;td>0.932&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>120&lt;/td>
&lt;td>0.925&lt;/td>
&lt;td>0.962&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>200&lt;/td>
&lt;td>0.960&lt;/td>
&lt;td>0.981&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>400&lt;/td>
&lt;td>0.987&lt;/td>
&lt;td>0.995&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>800&lt;/td>
&lt;td>0.995&lt;/td>
&lt;td>0.998&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;img src="https://jkatz.github.io/images/pgvector-distributed-gist960-qps.png" alt="pgvector-distributed-gist960-qps.png">&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;code>ef_search&lt;/code>&lt;/th>
&lt;th>QPS (1-node)&lt;/th>
&lt;th>QPS (2-node)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>1107&lt;/td>
&lt;td>134&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>20&lt;/td>
&lt;td>782&lt;/td>
&lt;td>89&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>40&lt;/td>
&lt;td>509&lt;/td>
&lt;td>54&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>80&lt;/td>
&lt;td>302&lt;/td>
&lt;td>30&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>120&lt;/td>
&lt;td>218&lt;/td>
&lt;td>24&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>200&lt;/td>
&lt;td>144&lt;/td>
&lt;td>21&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>400&lt;/td>
&lt;td>81&lt;/td>
&lt;td>17&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>800&lt;/td>
&lt;td>45&lt;/td>
&lt;td>13&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>There are two interesting observations from these runs:&lt;/p>
&lt;ul>
&lt;li>At lower values of &lt;code>hnsw.ef_search&lt;/code>, we observe an improvement in recall on the 2-node system up to 15%, which is not insignificant.&lt;/li>
&lt;li>Overall we observe that QPS on the 1-node system is higher than the 2-node system. This is primarily due to network latency, though the serial execution of each foreign scan may impact (and will impact as the number of nodes in the system increase).&lt;/li>
&lt;/ul>
&lt;p>From this, it&amp;rsquo;d be easy to conclude that the distributed technique should not be used. However, the workload fit entirely into memory on the 1-node instance; if the 1-node was memory constrained and had to continuously fetch data from networked-attached storage, we would see different results.&lt;/p>
&lt;p>Below I repeated the 1-node test, but I used the default PostgreSQL settings, including &lt;code>128MB&lt;/code> for &lt;code>shared_buffers&lt;/code>. This would memory constraint the system. I still used the local NVMe for storage. You can see the results of this test below:&lt;/p>
&lt;h4 id="sift-128-euclidean-1">sift-128-euclidean&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;code>ef_search&lt;/code>&lt;/th>
&lt;th>QPS (1-node; PG defaults)&lt;/th>
&lt;th>QPS (2-node)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>1030&lt;/td>
&lt;td>338&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>20&lt;/td>
&lt;td>710&lt;/td>
&lt;td>272&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>40&lt;/td>
&lt;td>450&lt;/td>
&lt;td>206&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>80&lt;/td>
&lt;td>266&lt;/td>
&lt;td>137&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>120&lt;/td>
&lt;td>193&lt;/td>
&lt;td>109&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>200&lt;/td>
&lt;td>119&lt;/td>
&lt;td>84&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>400&lt;/td>
&lt;td>69&lt;/td>
&lt;td>59&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>800&lt;/td>
&lt;td>39&lt;/td>
&lt;td>36&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="dbpedia-openai-1000k-angular-1">dbpedia-openai-1000k-angular&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;code>ef_search&lt;/code>&lt;/th>
&lt;th>QPS (1-node; PG defaults)&lt;/th>
&lt;th>QPS (2-node)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>731&lt;/td>
&lt;td>89&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>20&lt;/td>
&lt;td>510&lt;/td>
&lt;td>55&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>40&lt;/td>
&lt;td>323&lt;/td>
&lt;td>31&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>80&lt;/td>
&lt;td>192&lt;/td>
&lt;td>17&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>120&lt;/td>
&lt;td>141&lt;/td>
&lt;td>13&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>200&lt;/td>
&lt;td>93&lt;/td>
&lt;td>13&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>400&lt;/td>
&lt;td>53&lt;/td>
&lt;td>11&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>800&lt;/td>
&lt;td>30&lt;/td>
&lt;td>9&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="gist-960-euclidean-1">gist-960-euclidean&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;code>ef_search&lt;/code>&lt;/th>
&lt;th>QPS (1-node; PG defaults)&lt;/th>
&lt;th>QPS (2-node)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>712&lt;/td>
&lt;td>134&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>20&lt;/td>
&lt;td>482&lt;/td>
&lt;td>89&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>40&lt;/td>
&lt;td>306&lt;/td>
&lt;td>54&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>80&lt;/td>
&lt;td>179&lt;/td>
&lt;td>30&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>120&lt;/td>
&lt;td>128&lt;/td>
&lt;td>24&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>200&lt;/td>
&lt;td>81&lt;/td>
&lt;td>21&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>400&lt;/td>
&lt;td>44&lt;/td>
&lt;td>17&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>800&lt;/td>
&lt;td>25&lt;/td>
&lt;td>13&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Overall, while the 1-node test using the PostgreSQL defaults had higher QPS than the 2-node system, we see that the differences weren&amp;rsquo;t as large once the workload was memory constrained &amp;ndash; and in certain cases, the performance was comparable.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>Distributing a workload across multiple databases can be used to further scale a workload once you&amp;rsquo;re unable to scale it past a single instance. pgvector builds on PostgreSQL features that help with scaling vertically, and recently pgvector releases &lt;a href="https://aws.amazon.com/blogs/database/accelerate-hnsw-indexing-and-searching-with-pgvector-on-amazon-aurora-postgresql-compatible-edition-and-amazon-rds-for-postgresql/">have also shown that vector workloads can scale on PostgreSQL&lt;/a>.&lt;/p>
&lt;p>While the experiments in this post show that it&amp;rsquo;s possible to distribute pgvector workloads across multiple instances &amp;ndash; and we can see benefits with recall &amp;ndash; there&amp;rsquo;s additional work that can simplify and help scale pgvector across multiple writable instances (e.g., pushdown session parameters, async execution across &amp;ldquo;merge append&amp;rdquo; nodes). If your workloads is read heavy, you can still use read replicas as a way to distribute traffic if your primary instance is saturated, but note that the entire data set is available on each instance.&lt;/p></description></item><item><title>PGConf.dev: Why, what, and how you can participate</title><link>https://jkatz.github.io/post/postgres/why-pgconf-dev/</link><pubDate>Tue, 09 Jan 2024 00:00:00 +0000</pubDate><guid>https://jkatz.github.io/post/postgres/why-pgconf-dev/</guid><description>&lt;p>When I first began exploring how to get involved in the PostgreSQL community, the first event I heard of was &lt;a href="https://www.pgcon.org/">PGCon&lt;/a>. I was still in college when PGCon had started(!), and I did have &lt;a href="https://en.wikipedia.org/wiki/Fear_of_missing_out">FOMO&lt;/a> about not going (that said, I don&amp;rsquo;t think the phrase &amp;ldquo;FOMO&amp;rdquo; existed yet). Through the years, the timing of PGCon became very important: it served as a checkpoint between the in progress PostgreSQL major release (Beta 1 would have launched 1-2 weeks prior) and upcoming work on the new version of PostgreSQL. Additionally, because of the concentration of PostgreSQL contributors, both hackers and community builders, it was a great place to discuss how we can continue to make the PostgreSQL community better.&lt;/p>
&lt;p>As PostgreSQL grew, both in &lt;a href="https://jkatz.github.io/post/postgres/postgresql-2024/">functionality and popularity&lt;/a>, so did many of the events around PostgreSQL. PostgreSQL events went from being hosted in rented basements and academic spaces to hotels and event spaces. Even through all this, PGCon remained the same: everyone knew that once a year, they&amp;rsquo;d make the trek to the University of Ottawa to discuss all things PostgreSQL. This is not necessarily a bad thing &amp;ndash; after all, PostgreSQL &lt;a href="https://www.postgresql.org/docs/current/history.html">started as an academic project&lt;/a>, the community felt that it was time to evolve PGCon for several reasons:&lt;/p>
&lt;ul>
&lt;li>While we all grew accustomed to Ottawa, travel to and from Ottawa could be challenging. I spoke to community members who sometimes had to take 4(!) flights to get there. (While my travels were never that bad, I&amp;rsquo;m happy to tell the tale of how I got lost in rural Quebec without a GPS).&lt;/li>
&lt;li>Dan Langille had tireless worked on organizing PGCon, mainly by himself, for 15+ years, which was a huge time and energy commitment. We all truly respect Dan&amp;rsquo;s work, and Dan was ready to expand the efforts to get more people involved in event organizing.&lt;/li>
&lt;li>PGCon had billed itself as a conference for all things PostgreSQL, but we wanted to ensure we put the emphasize on development, both software and community building, as the theme of the event.&lt;/li>
&lt;/ul>
&lt;p>PGCon has been an important part of the PostgreSQL community for nearly two decades. Dan had built a great infrastructure in and around the event, and wanted to work with other members of the community to take PGCon to its next stages. And this is where we got to &lt;a href="https://www.pgconf.dev/">PGConf.dev&lt;/a>, the next step in the evolution of PGCon.&lt;/p>
&lt;h2 id="what-is-pgconfdev">What is PGConf.dev?&lt;/h2>
&lt;p>&lt;a href="https://www.pgconf.dev/">PGConf.dev&lt;/a> (&amp;ldquo;Pee-gee-conf-dot-dev&amp;rdquo;), or the &amp;ldquo;PostgreSQL Development Conference,&amp;rdquo; is an event that focuses on contributing to the PostgreSQL community. While &amp;ldquo;development&amp;rdquo; is often thought of &amp;ldquo;software development,&amp;rdquo; it also applies to community building too. Building PostgreSQL involves building all aspects of the project: from the core server, to related software (drivers, extensions, etc.), to community infrastructure that lets people contribute in all different ways (governance, event / user group organization, etc.).&lt;/p>
&lt;p>On the surface, &lt;a href="https://www.pgconf.dev/">PGConf.dev&lt;/a> is similar to PGCon: there will be workshops, breakout sessions, and a day of &lt;a href="https://en.wikipedia.org/wiki/Unconference">unconference&lt;/a>! That said, there are certain areas that PGConf.dev will place heavy emphasis on:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>&amp;ldquo;Development&amp;rdquo; topics&lt;/strong>: &amp;ldquo;Development&amp;rdquo; means building, but it&amp;rsquo;s not limited to just software. Given the timing in the PostgreSQL release cycle, there will be plenty of content and discussion around forward-looking software development topics: recently committed features, new features to build, etc., but also topics on how we can continue to grow all aspects of the community (advocacy, governance, inclusivity).&lt;/li>
&lt;li>&lt;strong>Mentorship&lt;/strong>: or helping to encourage new contributors to join and grow existing contributors in all aspects of the project. To this end, PGConf.dev is planning to have workshops to provide guidance on different aspects of developing as a contributor.&lt;/li>
&lt;li>&lt;strong>Ease of travel&lt;/strong>: We aim to keep PGConf.dev in a travel accessible city. For now, the organizers decided to stay in Canada, but are focused on hosting the event in cities that have more direct international flights. For &lt;a href="https://2024.pgconf.dev/">PGConf.dev 2024&lt;/a>, we&amp;rsquo;ll be in Vancouver, but future years could be in Montreal or Toronto.&lt;/li>
&lt;/ul>
&lt;p>While PGConf.dev builds on the history of PGCon, this is the first year of running the event with a different theme and in a different city. In paying homage to the origins of PGCon, PGConf.dev 2024 is hosted in the &lt;a href="https://2024.pgconf.dev/venue/">downtown Vancouver Simon Fraser University&lt;/a> campus (don&amp;rsquo;t worry &amp;ndash; there are no squeaky chairs!). The venue type may change in future years, but we want to keep the academic / collaborative vibe, given the focus of PGConf.dev is building PostgreSQL!&lt;/p>
&lt;h2 id="how-can-i-participate-in-pgconfdev">How can I participate in PGConf.dev?&lt;/h2>
&lt;p>The best way to participate at PGConf.dev is to attend and support the event! There are several ways you can do so:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://2024.pgconf.dev/cfp/">Submit a talk&lt;/a>! The CFP is open through January 15, 2024 (2024-01-15), and the &lt;a href="https://2024.pgconf.dev/cfp/">CFP page&lt;/a> provides guidance on the types of talks the program committee is looking for. As mentioned above, we&amp;rsquo;re looking for &amp;ldquo;development&amp;rdquo; talks, whether its software (PostgreSQL, drivers, extensions, utilities, etc!) or community building.&lt;/li>
&lt;li>&lt;a href="https://2024.pgconf.dev/sponsors/">Sponsor&lt;/a> - we&amp;rsquo;re only able to host PGConf.dev because of the generous support of our sponsors. Sponsoring PGConf.dev gives us the opportunity to bring PostgreSQL contributors together in one place to help plan the future of PostgreSQL. Given the nature of this event, we&amp;rsquo;ve tailored the sponsorship opportunities towards community support.&lt;/li>
&lt;li>&lt;a href="https://www.pgevents.ca/events/pgconfdev2024/register/">Attend&lt;/a> - registration for PGConf.dev is open, and attending is a great way to collaborate with the PostgreSQL community and help contribute!&lt;/li>
&lt;li>&lt;strong>Spread the word&lt;/strong>: Let people know about &lt;a href="https://www.pgconf.dev/">PGConf.dev&lt;/a> and how it&amp;rsquo;s an opportunity to meet PostgreSQL contributors and help directly impact the future of PostgreSQL!&lt;/li>
&lt;/ul>
&lt;h2 id="the-continued-evolution-of-pgcon">The continued evolution of PGCon&lt;/h2>
&lt;p>As mentioned earlier, while PGConf.dev builds on the history of PGCon, we know there&amp;rsquo;s more to learn and evolve with &lt;a href="https://www.pgconf.dev/">PGConf.dev&lt;/a>. We&amp;rsquo;re excited to have your feedback on the event so we can continue to evolve it and make it the preeminent opportunity for both contributing and learning how to contribute to PostgreSQL. We&amp;rsquo;re looking forward to seeing all of you in Vancouver in May 2024!&lt;/p></description></item><item><title>Thoughts on PostgreSQL in 2024</title><link>https://jkatz.github.io/post/postgres/postgresql-2024/</link><pubDate>Tue, 02 Jan 2024 00:00:00 +0000</pubDate><guid>https://jkatz.github.io/post/postgres/postgresql-2024/</guid><description>&lt;p>A question I often hear, and also ask myself, is &amp;ldquo;where is PostgreSQL going?&amp;rdquo; This is a deep question: it&amp;rsquo;s not limited to the work on the core database engine, but rather everything going on in the community, including related open source projects and event and community development. Even with the popularity of PostgreSQL, which was selected as &lt;a href="https://db-engines.com/en/blog_post/106">DB Engine&amp;rsquo;s &amp;ldquo;DBMS of the Year&amp;rdquo; for the fourth time&lt;/a>, it&amp;rsquo;s a good idea to step back at times and reflect on what PostgreSQL will look like in the future. While it may not necessarily lead to immediate changes, it does help give context to all the work going on in the community.&lt;/p>
&lt;p>The new year is a great opportunity to ask &amp;ldquo;where is PostgreSQL going?&amp;rdquo; and is a question I&amp;rsquo;ve been personally reflecting on. So here are some of my thoughts on where PostgreSQL is going as we enter into 2024. This is not meant to be a roadmap, but rather personal thoughts on where PostgreSQL is going.&lt;/p>
&lt;h2 id="postgresql-feature-development">PostgreSQL feature development&lt;/h2>
&lt;p>At the &lt;a href="https://wiki.postgresql.org/wiki/PgCon_2023_Developer_Meeting">PGCon 2023 Developer Meeting&lt;/a>, I proposed a topic entitled &lt;a href="https://wiki.postgresql.org/wiki/PgCon_2023_Developer_Meeting#What_are_the_big_challenges_for_our_users.3F_What_are_the_big_challenges_for_us_to_solve.3F">&amp;ldquo;What are the big challenges for PostgreSQL users?&amp;rdquo;&lt;/a>. The goal of this was to talk about both common user requests and understand where database workloads were heading to determine if we are building PostgreSQL towards where database workloads are going. Based on many conversation and observations, I proposed three broad feature buckets to look at:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Availability&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Performance&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Developer features&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>These are all ongoing areas of work for 2024 and beyond, but there are definitely steps PostgreSQL can take in the coming year to make improvements in all of these areas. Below I dive into more details about each of these feature groups.&lt;/p>
&lt;h3 id="availability">Availability&lt;/h3>
&lt;p>Continuing to improve the availability of a PostgreSQL cluster is the first, second, and third most requested &amp;ldquo;feature&amp;rdquo; I hear about from both current and prospective PostgreSQL users. I&amp;rsquo;m not exaggerating either: while restarting PostgreSQL can be nearly instantaneous, there are use cases where that can be too much time (though those are at the extreme). Additionally, operations with locks that block writes for prolonged periods of time can be considered as &amp;ldquo;downtime.&amp;rdquo;&lt;/p>
&lt;p>While most PostgreSQL users can currently achieve their uptime requirements, there is a class of workloads with critical uptime requirements that we can better support in PostgreSQL with additional development effort. Most of this section (and blog post) focuses on one feature area where continued improvements will allow PostgreSQL to be deployed in even more environments that have these requirements.&lt;/p>
&lt;h4 id="how-logical-replication-can-help-with-active-active-bluegreen-zero-downtime-upgrading-and-other-workflows">How logical replication can help with active-active, blue/green, zero-downtime upgrading, and other workflows&lt;/h4>
&lt;p>For existing PostgreSQL users and users looking to migrate to PostgreSQL, features around availability are the biggest ask. Typically, this centers around &lt;a href="https://en.wikipedia.org/wiki/High_availability">high availability&lt;/a>, or the ability to continue to have access to the database (especially read/write access) during a planned (update) or unplanned (outage) disruption. PostgreSQL provides many features to support high availability, including streaming replication. However, maximizing HA still requires the use of an additional service or a utility like &lt;a href="https://github.com/zalando/patroni">Patroni&lt;/a> to achieve many uptime goals.&lt;/p>
&lt;p>Many users I talk to are happy with the availability they can get with PostgreSQL: it works for most of their use cases. However, I&amp;rsquo;ve been seeing an emerging trend of workloads on PostgreSQL that need even higher availability, where a 15-30s offline window isn&amp;rsquo;t good enough. This is both for planned outages (e.g. minor version upgrades, major version upgrades) and unplanned outages. I&amp;rsquo;ve even talked to users who have workloads that can only be unavailable for 1s &amp;ndash; and while I was initially skeptical, when I heard what the workloads were for, I did agree that 1s was a reasonable requirement for them!&lt;/p>
&lt;p>A key feature for PostgreSQL that will continue to improve availability is &lt;a href="https://www.postgresql.org/docs/current/logical-replication.html">logical replication&lt;/a>. Logical replication allows the real-time streaming of changes from a database into any system that can understand the PostgreSQL logical replication protocol. Logical replication in PostgreSQL &lt;a href="https://jkatz.github.io/post/postgres/postgres-10-tribute/">has been around for awhile&lt;/a>, but &lt;a href="https://www.postgresql.org/about/news/postgresql-16-released-2715/">recent releases&lt;/a> have added significant enhancements that can better support availability use-cases, including functionality and performance features.&lt;/p>
&lt;p>One advantage this has over physical (or binary) replication is that you can use logical replication to stream changes from a PostgreSQL 15 to a PostgreSQL 16 system as part of a major version upgrade. This can help reduce the amount of downtime it takes to perform a major version upgrade (here is an example of how &lt;a href="https://www.instacart.com/company/how-its-made/zero-downtime-postgresql-cutovers/">Instacart used logical replication to get to zero-downtime on major version upgrades&lt;/a>), but there is still work to be done in PostgreSQL to improve this use case and other high availability use cases. Additional features will help unlock more seamless ways of supporting &lt;a href="https://en.wikipedia.org/wiki/Blue%E2%80%93green_deployment">blue-green deployments&lt;/a> in PostgreSQL.&lt;/p>
&lt;p>Logical replication can also be used as part of the high availability mechanism itself. One technique, &amp;ldquo;active-active replication,&amp;rdquo; allows multiple databases can simultaneously accept writes and replicate the changes amongst themselves. This technique is typically used in systems that have that &amp;ldquo;no more than 1s of unavailability&amp;rdquo; requirement: if a writer database is unavailable, then an application can switch its database traffic to a different writer database without waiting for it to be promoted. While this sounds ideal, building and managing an active-active system is extremely complicated: it impacts application design, requires you to have a write-conflict management and resolution strategy, and requires careful fault tolerance monitoring to help ensure data integrity (e.g. a &amp;ldquo;conflict storm&amp;rdquo;) and replication health (e.g. what happens if an instance can&amp;rsquo;t replicate changes for several hours?).&lt;/p>
&lt;p>However, both the major version upgrade and active-active cases do present a roadmap for how we can continue to improve logical replication in PostgreSQL. &lt;a href="https://amitkapila16.blogspot.com/">Amit Kapila&lt;/a>, who has led many of the logical replication feature efforts, and I developed a talk this year called &lt;a href="https://www.postgresql.eu/events/pgconfeu2023/sessions/session/4783/slides/434/pgconfeu2023_active_active.pdf">The journey towards active-active replication in PostgreSQL&lt;/a> (&lt;a href="https://www.youtube.com/watch?v=jPp4XIY4XRw">video of us co-presening one version&lt;/a>) that talks about why solving for these use cases are important, the current state-of-the-art of PostgreSQL logical replication, and what work we need to do to get PostgreSQL to better support these cases. The good news: as of PostgreSQL 16, we have most of the foundational blocks for supporting active-active, blue-green deployments, and zero downtime major version upgrades &amp;ndash; and even if they are not in core, there are PostgreSQL extensions that can provide this functionality (disclosure: I&amp;rsquo;ve been involved with one such extension, &lt;a href="https://aws.amazon.com/blogs/database/using-pgactive-active-active-replication-extension-for-postgresql-on-amazon-rds-for-postgresql/">&lt;code>pgactive&lt;/code>&lt;/a>).&lt;/p>
&lt;p>There are multiple efforts in 2024 to help close these feature gaps. Targeted for PostgreSQL 17 (usual disclaimer that these may not be included), there has been a focus on ensuring logical replication can work with key workflows, such as &lt;a href="https://www.postgresql.org/docs/current/pgupgrade.html">&lt;code>pg_upgrade&lt;/code>&lt;/a> and in &lt;a href="https://commitfest.postgresql.org/46/4423/">high availability systems&lt;/a>, and working to support replication of additional changes (e.g. &lt;a href="https://commitfest.postgresql.org/46/3823/">sequences&lt;/a>). Beyond that, we must continue to support more commands in logical replication (e.g. &lt;a href="https://commitfest.postgresql.org/46/3595/">DDL&lt;/a>), continue to improve performance (more parallelism support, worker optimizations), and add features that simplify management of logical replication (node synchronization/resynchronization).&lt;/p>
&lt;p>All of these efforts will make it possible to use PostgreSQL in more workloads that have very high uptime requirements, and simplify how users roll out new changes to their production environments. While there&amp;rsquo;s still more work to do with enhancing logical replication in PostgreSQL, it looks like 2024 will give us more features that help users run PostgreSQL in critical environments.&lt;/p>
&lt;h4 id="unblocking-the-locks">Unblocking the locks&lt;/h4>
&lt;p>Another area of availability to consider is around schema maintenance operations (i.e. &lt;a href="https://en.wikipedia.org/wiki/Data_definition_language">DDL&lt;/a> statements), such an &lt;a href="https://www.postgresql.org/docs/current/sql-altertable.html">&lt;code>ALTER TABLE&lt;/code>&lt;/a> that takes an &lt;a href="https://www.postgresql.org/docs/current/explicit-locking.html#LOCKING-TABLES">&lt;code>ACCESS EXCLUSIVE&lt;/code>&lt;/a> lock on the table that blocks all other write operations on that table. For many users, this is the same thing as being unavailable, even if it&amp;rsquo;s only to a subset of their data. Lack of full support for nonblocking/online schema maintenance operations in PostgreSQL has become more noticeable as other relational databases include support for this feature.&lt;/p>
&lt;p>There are various utilities and extensions that let you run nonblocking schema updates, but it would be more convenient, and likely performant, to support more nonblocking schema changes natively in PostgreSQL. Based on the design, we may already have the foundation to build out this feature, but it will take some time. While I&amp;rsquo;m not aware of active implementation efforts, I do think in 2024 we need to make more progress on making it possible for users to run most, if not all, DDL commands without blocking writes, if they so choose.&lt;/p>
&lt;h3 id="performance">Performance&lt;/h3>
&lt;p>Performance is very much a &amp;ldquo;what have you done for me lately&amp;rdquo; feature: we can always go faster! The good news is that PostgreSQL has a reputation of vertically scaling, or being able to scale as you provide more hardware resources to a single instance. While there are use cases where horizontally scaling both reads and writes makes sense, we do need to continue to ensure PostgreSQL can continue to scale as compute and memory resources continue to grow.&lt;/p>
&lt;p>Here&amp;rsquo;s a more &amp;ldquo;practical&amp;rdquo; way of putting it: there is an Amazon EC2 instance that has &lt;a href="https://aws.amazon.com/ec2/instance-types/high-memory/">448 vCPU and 24TB of RAM&lt;/a> &amp;ndash; is PostgreSQL able to fully maximize its use of all of those resources on a single instance? Looking at the current and upcoming hardware that PostgreSQL users will use gives us a measured target for how we can continue to improve PostgreSQL performance.&lt;/p>
&lt;p>As we enter 2024, there are already multiple efforts that will help make it possible to continue to vertically scale PostgreSQL. One of the biggest efforts, and one that&amp;rsquo;s been an ongoing multi-year project, is to support direct IO (DIO) and asynchronous IO (AIO) in PostgreSQL. For details, I&amp;rsquo;ll defer to Andres Freund&amp;rsquo;s &lt;a href="https://www.pgconf.eu/">PGConf.EU&lt;/a> slides on the &lt;a href="https://anarazel.de/talks/2023-12-14-pgconf-eu-path-to-aio/path-to-aio.pdf">status of adding AIO to PostgreSQL&lt;/a>, but it looks like in 2024 that we&amp;rsquo;ll be much closer to full AIO support.&lt;/p>
&lt;p>Another effort I&amp;rsquo;m intrigued by is &lt;a href="https://wiki.postgresql.org/wiki/Parallel_Recovery">parallel recovery&lt;/a>. PostgreSQL users with heavy write workloads tend to postpone &lt;a href="https://www.postgresql.org/docs/current/sql-checkpoint.html">checkpoints&lt;/a> to defer I/O workload. This can be problematic on a busy system if PostgreSQL crashes and a checkpoint has not occurred for awhile. When PostgreSQL restarts, it enters &amp;ldquo;crash recovery&amp;rdquo; where it replays every change since the last checkpoint so it can reach a consistent state. During crash recovery, PostgreSQL cannot accept reads or writes, which means that it&amp;rsquo;s unavailable. This is problematic for busy stems: while PostgreSQL can accept concurrent writes, it can only replay changes with a single process. If a crash on a busy system occurred an hour after the last checkpoint, it could take several more hours to reach a consistent state while the system is offline!&lt;/p>
&lt;p>One way to help overcome this limitation is to support &amp;ldquo;&lt;a href="https://wiki.postgresql.org/wiki/Parallel_Recovery">parallel recovery&lt;/a>,&amp;rdquo; or being able to replay changes in parallel. At &lt;a href="https://www.pgcon.org/">PGCon 2023&lt;/a>, Koichi Suzuki gave a &lt;a href="https://www.pgcon.org/events/pgcon_2023/sessions/session/392/slides/69/Parallel%20Recovery%20in%20PostgreSQL.pdf">detailed presentation on how PostgreSQL can support parallel recovery&lt;/a>. This would apply not only to crash recovery, but how PostgreSQL can replay any WAL changes (e.g. point-in-time-recovery). While this is a very challenging problem to solve, supporting parallel recovery helps PostgreSQL to continue to scale vertically, as users can further optimize for heavy write workloads and mitigate the risk of a crash causing an untenable delay in coming back online.&lt;/p>
&lt;p>This is not an exhaustive list of performance-related features. There are many more efforts around PostgreSQL server performance, including indexing optimizations, locking improvements, leveraging hardware acceleration, and more. This in addition to work on clients, such as drivers and connection poolers, that can bring additional performance gains to how apps interact with PostgreSQL. Looking at what the community is working on in 2024, I do believe we&amp;rsquo;ll continue to see general performance gains across all areas of PostgreSQL.&lt;/p>
&lt;h3 id="developer-features">Developer features&lt;/h3>
&lt;p>I view &amp;ldquo;developer features&amp;rdquo; as a fairly broad category around how users can architect and build their apps around PostgreSQL. This includes SQL syntax, functions, &lt;a href="https://wiki.postgresql.org/wiki/PL_Matrix">procedural language support&lt;/a>, and other features that help users both build apps and transition from other database systems. One example of such an innovation is the &lt;a href="https://www.postgresql.org/docs/current/rangetypes.html">&lt;code>multirange&lt;/code>&lt;/a> data type, added in PostgreSQL 14, which let users group non-contiguous ranges together. This had many practical purposes, such as in scheduling, and personally let me &lt;a href="https://www.crunchydata.com/blog/better-range-types-in-postgres-14-turning-100-lines-of-sql-into-3">reduce hundreds of lines of PL/pgSQL code into roughly three lines&lt;/a>. Developer features is also a way to keep track of how PostgreSQL can support emergent workloads, such as &lt;a href="https://jkatz.github.io/post/postgres/vectors-json-postgresql/">JSON or vectors&lt;/a>.&lt;/p>
&lt;p>Currently, a lot of innovation on PostgreSQL developer features is occurring in extensions, which is an advantage of PostgreSQL&amp;rsquo;s extensible model. In the server itself, there are areas where PostgreSQL is lagging behind its previous pace of releasing developer features. For example, PostgreSQL was the &lt;a href="https://jkatz.github.io/post/postgres/vectors-json-postgresql/">first relational database to support JSON as a queryable data type&lt;/a>, but has been lagging on implementing syntax and features specified in the SQL/JSON standard. PostgreSQL 16 released several of the SQL/JSON syntax features, and there are multiple efforts targeted for 2024 that will include more of the SQL/JSON specification.&lt;/p>
&lt;p>With that said, we should be investing in adding developer features in PostgreSQL that are not possible to add in extensions, such as SQL standard features. I suggest a focus of features that are already available in other databases, such as more of the SQL/JSON standard (e.g. &lt;code>JSON_TABLE&lt;/code>), system versioned tables (useful for auditing and &amp;ldquo;flashback&amp;rdquo; / bitemporal queries to view data at a specific point in time), and module support (useful for &amp;ldquo;packaging&amp;rdquo; stored procedures).&lt;/p>
&lt;p>Additionally, with the previously mentioned focus on availability and performance, we should continue to simplify how users can migrate from other databases to PostgreSQL. As part of my day job, I had the opportunity to read through a lot of content around migration strategies from commercial databases to PostgreSQL, and there&amp;rsquo;s still ample opportunity to simplify the process while enhancing PostgreSQL capabilities. This includes features available in other databases (e.g. global temporary tables, global partitioned indexes, &lt;a href="https://www.postgresql.org/message-id/f7470d5a-3cf1-4919-8404-5c4d91341a9f@tantorlabs.com">autonomous transactions&lt;/a>) and adding more functionality and performance optimizations in PL/pgSQL (bulk data processing functions, &lt;a href="https://commitfest.postgresql.org/46/1608/">schema variables&lt;/a>, &lt;a href="https://commitfest.postgresql.org/46/4684/">caching function metadata&lt;/a>). All these things improve the PostgreSQL developer experience while making it easier for users coming from other relational databases to adopt PostgreSQL.&lt;/p>
&lt;p>Finally, we need to see how we can continue to support the emergent workload coming from AI/ML data, specifically vector storage and search. At &lt;a href="https://www.pgcon.org/">PGCon&lt;/a> 2023, while folks wanted to see native vector support in PostgreSQL itself, there was consensus that implementing functionality in an extension like &lt;a href="https://github.com/pgvector/pgvector">pgvector&lt;/a> would let us support these workloads more quickly (and this &lt;a href="https://jkatz.github.io/post/postgres/pgvector-overview-0.5.0/">strategy seems to have worked&lt;/a> with &lt;a href="https://aws.amazon.com/blogs/database/accelerate-hnsw-indexing-and-searching-with-pgvector-on-amazon-rds-for-postgresql/">great performance results on vector data&lt;/a>). However, &lt;a href="https://www.postgresql.eu/events/pgconfeu2023/sessions/session/4592/slides/435/pgconfeu2023_vectors.pdf">given many of the properties of vector workloads&lt;/a>, there are additions we can make to PostgreSQL to further support them, including planner optimizations for working with &lt;a href="https://www.postgresql.org/message-id/ad8a178f-bbe7-d89d-b407-2f0fede93144@postgresql.org">TOAST&amp;rsquo;d data that&amp;rsquo;s in the active query path&lt;/a>, and exploring how we can better support queries where the bulk filtering step occurs in the &lt;code>ORDER BY&lt;/code> clause.&lt;/p>
&lt;p>I do think we can make a lot of progress on all of these areas in 2024 and continue to add features directly to PostgreSQL that make it easier to build applications, even as we see a boon of functionality in extensions around PostgreSQL.&lt;/p>
&lt;h3 id="but-what-about-security">But what about security?&lt;/h3>
&lt;p>I do want to quickly disucss security features. PostgreSQL does have a strong reputation for enabling workloads in security-focused environments, but there is always more to do. The past several years, adding native support for &lt;a href="https://wiki.postgresql.org/wiki/Transparent_Data_Encryption">transparent data encryption&lt;/a> (TDE) in PostgreSQL has received a lot of attention, but there are other areas we can continue to innovate. This includes adding support for additional authentication methods or mechanisms (OIDC is amongst the biggest asks) and exploring the possibility of a federation authorization model to allow PostgreSQL to inherit permissions from other systems. And while this is challenging today, I&amp;rsquo;d suggest we look at how we can support TDE on a per-database level. I&amp;rsquo;m keeping this discussion short as there are ways to satisfy the requirements that these features would add to PostgreSQL today, but we can certainly continue to build towards full native support.&lt;/p>
&lt;p>And with that, let&amp;rsquo;s look at other areas where PostgreSQL can make progress in 2024.&lt;/p>
&lt;h2 id="extensions">Extensions&lt;/h2>
&lt;p>PostgreSQL was designed to be extensible: you can add functionality to PostgreSQL without having to fork it. This includes new data types, indexing methods, ways to work with other database systems, utilities that make it easier to manage PostgreSQL features, &lt;a href="https://wiki.postgresql.org/wiki/PL_Matrix">additional programming languages&lt;/a>, and even &lt;a href="https://github.com/aws/pg_tle">extensions that let you write your own extensions&lt;/a>. People have built open source communities and companies around specific PostgreSQL extensions (e.g. &lt;a href="https://postgis.net/">PostGIS&lt;/a>), and PostgreSQL extensions have made it possible to support all kinds of workloads (geospatial, timeseries, analytical, AI) from a single database. With &lt;a href="https://gist.github.com/joelonsql/e5aa27f8cc9bd22b8999b7de8aee9d47">thousands of available PostgreSQL extensions&lt;/a>, they truly are a &amp;ldquo;force multiplier&amp;rdquo; for PostgreSQL and help drive significant adoption while letting users quickly build functionality for their databases!&lt;/p>
&lt;p>The side-effect of all this is we&amp;rsquo;re now seeing &amp;ldquo;extension &lt;a href="https://en.wikipedia.org/wiki/Urban_sprawl">sprawl&lt;/a>.&amp;rdquo; How do I know which extension to use? What is the level of support of an extension? How do I know an extension will continue to be actively maintained? How can I help contribute a feature to an extension? Even &amp;ldquo;where can I download an extension&amp;rdquo; has become a big question: while postgresql.org has an &lt;a href="https://www.postgresql.org/download/products/6-postgresql-extensions/">incomplete list of extensions&lt;/a> and the &lt;a href="https://www.postgresql.org/download/">community packages&lt;/a> maintain a set of extensions, there are now multiple PostgreSQL extension repositories available (&lt;a href="https://pgxn.org/">PGXN&lt;/a>, &lt;a href="https://database.dev/">dbdev&lt;/a>, &lt;a href="https://pgt.dev/">Trunk&lt;/a>), and &lt;a href="https://pgxman.com/">pgxman&lt;/a>.&lt;/p>
&lt;p>One of the strengths of the PostgreSQL community is that it is widely distributed, but we can make it easier to help guide users through the sprawl make informed choices about how they manage their data. I see 2024 as an opportunity to put more central resources into how we represent PostgreSQL extensions, and help users understand when to use certain extensions and their development maturity level, and likewise help extension builders with both governance and maintenance resources.&lt;/p>
&lt;h2 id="community-building">Community Building&lt;/h2>
&lt;p>I wanted to round out thoughts for 2024 around community building. The PostgreSQL contributor community has significantly grown since I first started, and the community has done a better job of &lt;a href="https://www.postgresql.org/community/contributors/">recognizing contributors&lt;/a> to all parts of the project, not just the code base (noting that there is still room for improvement here). But we can continue to do better, and there are three areas I&amp;rsquo;d like to specifically highlight: mentorship and &lt;a href="https://en.wikipedia.org/wiki/Diversity,_equity,_and_inclusion">DEI&lt;/a>, and transparency, which will help in all areas of the project.&lt;/p>
&lt;p>During the &lt;a href="https://wiki.postgresql.org/wiki/PgCon_2023_Developer_Meeting#What_are_the_big_challenges_for_our_users.3F_What_are_the_big_challenges_for_us_to_solve.3F">developer meeting @ PGCon 2023&lt;/a>, &lt;a href="https://mastodon.social/@melanieplageman/">Melanie Plageman&lt;/a> gave a very detailed analysis of the experience of being a newer contributor to PostgreSQL and the challenges it takes to ramp up. Melanie identified many problems: ramp up time on learning basic of how to contribute to PostgreSQL (getting started on the codebase, communicating on the mailing list), the effort to get a patch to a committable state and having a committer interested in it, guidance that may be given with the best of intentions (get started by reviewing patches!) which actually may be more challenging than writing code, and how feedback, when delivered, is delivered.&lt;/p>
&lt;p>On the last point, how to give feedback, I want to call out an &lt;a href="https://rhaas.blogspot.com/2023/12/praise-criticism-and-dialogue.html">excellent blog post by Robert Haas&lt;/a> that specifically addresses the power of giving &lt;a href="https://rhaas.blogspot.com/2023/12/praise-criticism-and-dialogue.html">praise while delivering feedback&lt;/a> &amp;ndash; these things do make a difference and it&amp;rsquo;s a good reminder in general that we should be supportive even while we&amp;rsquo;re being critical.&lt;/p>
&lt;p>Back to Melanie&amp;rsquo;s points, mentorship is something we can do better across the community. Personally, I admit I have been bad at this in areas around project advocacy, including helping to get more people to contribute to the &lt;a href="https://www.postgresql.org/developer/related-projects/">web infrastructure&lt;/a> and the &lt;a href="https://www.postgresql.org/about/press/presskit16/">release process&lt;/a>. This doesn&amp;rsquo;t mean PostgreSQL lacks mentorship &amp;ndash; I can count numerous folks in the community as mentors &amp;ndash; but we can be better in terms of how we can help people get started with contributing and finding mentors who can guide them on their journey.&lt;/p>
&lt;p>2024 serves as a gateway to building better mentorship processes, and we&amp;rsquo;re looking to test some of these ideas at &lt;a href="https://2024.pgconf.dev/">PGConf.dev 2024&lt;/a> in May 2024 in Vancouver.&lt;/p>
&lt;p>(Some history: Before &lt;a href="https://www.pgconf.dev/">PGConf.dev&lt;/a>, &lt;a href="https://www.pgcon.org/">PGCon&lt;/a> was the event where PostgreSQL contributors gathered to discuss strategic projects for the upcoming development cycle. PGCon was organized by Dan Langille from 2007 to 2023, and after umpteen years of organizing, he was ready to extend the efforts to a group of folks and helped to establish &lt;a href="https://www.pgconf.dev/">PGConf.dev&lt;/a>).&lt;/p>
&lt;p>&lt;a href="https://www.pgconf.dev/">PGConf.dev&lt;/a> is a conference for folks who want to contribute to PostgreSQL, and covers topics around PostgreSQL development (both core server and all open source projects around PostgreSQL such as extensions and drivers), community building, and open source thought leadership. A big portion of PGConf.dev is dedicated to mentorship, and is planning to include workshops around how to contribute to PostgreSQL. If you&amp;rsquo;re looking for ways to help contribute to PostgreSQL, I strongly suggest attending or &lt;a href="https://2024.pgconf.dev/cfp/">submitting a talk&lt;/a>!&lt;/p>
&lt;p>This leads into how the PostgreSQL community can improve in &lt;a href="https://en.wikipedia.org/wiki/Diversity,_equity,_and_inclusion">DEI&lt;/a>. I strongly suggest reading the slides and watching the video (when it&amp;rsquo;s available) of &lt;a href="https://karenjex.blogspot.com/">Karen Jex&lt;/a> and &lt;a href="https://mydbanotebook.org/">Ltitia AVROT&lt;/a>&amp;rsquo;s PGConf.eu 2023 talk &lt;a href="https://www.postgresql.eu/events/pgconfeu2023/schedule/session/4913-trying-to-be-barbie-in-kens-mojo-dojo-casa-house/">Trying to be Barbie in Ken&amp;rsquo;s Mojo Dojo Casa House&lt;/a>, as it&amp;rsquo;s an insightful presentation on how we can continue to make the PostgreSQL community more inclusive. The community has made progress in this area (and Karen and Ltitia point to initiatives that have helped with this), but we can still be better, and we should actively and proactively work to address feedback to help ensure contributing to PostgreSQL is a welcoming experience. There are actions we can all take, for example, calling out an inappropriate (e.g. sexist) behavior as it happens and providing guidance on why it&amp;rsquo;s not appropriate.&lt;/p>
&lt;p>Finally, there&amp;rsquo;s transparency. This might seem odd in open source, given, well, it&amp;rsquo;s open. But there are quite a few governance issues that are discussed not in the open, and it helps to understand how decisions are made. The &lt;a href="https://www.postgresql.org/about/policies/coc_committee/">PostgreSQL Code of Conduct Committee&lt;/a> provides an excellent example of how the community can be transparent about issues that require sensitivity. Each year, the Code of Conduct committee publishes a report (&lt;a href="https://www.postgresql.org/about/policies/coc/reports/2022/">here is the one from 2022&lt;/a>) of its work, including high level descriptions of cases and overall statistics. This is a practice we can reproduce across many of the PostgreSQL teams that are involved in tasks that may require privacy due to their sensitivity.&lt;/p>
&lt;h2 id="conclusion-this-was-originally-supposed-to-be-a-shorter-post">Conclusion: This was originally supposed to be a shorter post&lt;/h2>
&lt;p>When I originally started writing this, I thought it&amp;rsquo;d be a pithy post that I&amp;rsquo;d finish in a few hours. A few days later&amp;hellip;&lt;/p>
&lt;p>In all seriousness, PostgreSQL is in a good place. It remains popular, and its reputation for reliability, robustness, and performance remain sound. But we can still do better, and the good news is that the community is actively working towards improving in every which way&lt;/p>
&lt;p>While these are thoughts for what PostgreSQL can do in 2024 and beyond, there&amp;rsquo;s so much PostgreSQL already does today. In fact, asking questions like &amp;ldquo;where is PostgreSQL going&amp;rdquo; does give us an opportunity to step back and reflect on all the progress PostgreSQL has made over the past several years while looking ahead on what is to come!&lt;/p></description></item><item><title>pgvector 0.5.0 Feature Highlights and HOWTOs</title><link>https://jkatz.github.io/post/postgres/pgvector-overview-0.5.0/</link><pubDate>Mon, 28 Aug 2023 00:00:00 +0000</pubDate><guid>https://jkatz.github.io/post/postgres/pgvector-overview-0.5.0/</guid><description>&lt;p>It&amp;rsquo;s here! &lt;a href="https://github.com/pgvector/pgvector/releases/tag/v0.5.0">pgvector 0.5.0&lt;/a> is released and has some incredible new features. &lt;a href="https://github.com/pgvector/pgvector">pgvector&lt;/a> is an open-source project that brings &lt;a href="https://jkatz.github.io/post/postgres/vectors-json-postgresql/">vector database capabilities to PostgreSQL&lt;/a>. The pgvector community is moving very rapidly on adding new features, so I thought it prudent to put together some highlights of the 0.5.0 release.&lt;/p>
&lt;p>Here&amp;rsquo;s a quick list of highlights, though I encourage you read the rest in depth and explore on your own!&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#new-index-type-hierarchical-navigable-small-worlds-hnsw">HNSW indexing support&lt;/a>&lt;/li>
&lt;li>&lt;a href="#improved-performance-of-distance-functions">Faster distance calculations&lt;/a>&lt;/li>
&lt;li>&lt;a href="#parallelization-of-ivfflat-index-builds">Parallel builds for &lt;code>ivfflat&lt;/code>&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>This is a big release, so let&amp;rsquo;s dive right in.&lt;/p>
&lt;h2 id="new-index-type-hierarchical-navigable-small-worlds-hnsw">New index type: Hierarchical Navigable Small Worlds (&lt;code>hnsw&lt;/code>)&lt;/h2>
&lt;p>The biggest highlight of the pgvector 0.5.0 release is the introduction of the &lt;code>hnsw&lt;/code> index type. &lt;code>hnsw&lt;/code> is based on the &lt;a href="https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf">hierarchical navigable small worlds&lt;/a> paper, which describes an indexing technique that creates layers of increasingly dense &amp;ldquo;neighborhoods&amp;rdquo; of vectors. The main idea of HNSW is that you can achieve a better performance/&lt;a href="https://en.wikipedia.org/wiki/Precision_and_recall">recall&lt;/a> ratio by connecting vectors that are close to each other, so that when you perform similarity search, you have a higher likelihood of finding the exact nearest neighbors you&amp;rsquo;re looking for.&lt;/p>
&lt;p>Beyond performance, pgvector&amp;rsquo;s HNSW implementation has several notable features:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>&amp;ldquo;Build as you go&amp;rdquo;&lt;/strong>: With HNSW, you can create an index on an empty table and add vectors as you go without impacting recall! This is different from &lt;code>ivfflat&lt;/code>, where you first need to load your vectors before building the index to find optimal centers for better recall. As you add more data to your &lt;code>ivfflat&lt;/code> index, you may also need to re-index to find updated centers.&lt;/li>
&lt;li>&lt;strong>Update and delete&lt;/strong>: pgvector&amp;rsquo;s HNSW implementation lets you update and delete vectors from the index, as part of standard &lt;code>UPDATE&lt;/code> and &lt;code>DELETE&lt;/code> queries. Many HNSW implementations do not support this feature.&lt;/li>
&lt;li>&lt;strong>Concurrent inserts&lt;/strong>: Additionally, pgvector&amp;rsquo;s HNSW implementations lets you concurrently insert values into the index, making it easier to simultaneously load data from multiple sources.&lt;/li>
&lt;/ul>
&lt;p>In a previous post, I went into depth on the &lt;a href="https://jkatz.github.io/post/postgres/pgvector-hnsw-performance/">HNSW performance for pgvector&lt;/a> with benchmarks that compared it to &lt;code>ivfflat&lt;/code> and &lt;code>pg_embedding&lt;/code>&amp;rsquo;s HNSW implementation. The chart below shows the performance/recall tradeoffs on OpenAI-style embedding data using cosine distance (for &lt;a href="https://jkatz.github.io/post/postgres/pgvector-hnsw-performance/">full testing methodology&lt;/a> and the &lt;a href="https://github.com/erikbern/ann-benchmarks">ANN Benchmark framework&lt;/a>. Please read the &lt;a href="https://jkatz.github.io/post/postgres/pgvector-hnsw-performance/">previous post&lt;/a>) for more details on the test methodology and other considerations (index build time, index size on disk):&lt;/p>
&lt;p>&lt;img src="https://jkatz.github.io/images/dbpedia-openai-1000k-angular.png" alt="dbpedia-openai-1000k-angular.png">&lt;/p>
&lt;p>Instead of focusing on benchmarking in this post, I want to provide guidance on how to use the key parameters in &lt;code>hnsw&lt;/code> so you understand the ramifications on your performance/recall ratio and index build times.&lt;/p>
&lt;p>In the &lt;a href="https://jkatz.github.io/post/postgres/pgvector-hnsw-performance/">previous post&lt;/a>, we reviewed the 3 parameters that are part of the HNSW algorithm. We&amp;rsquo;ll break them down to where they are applicable in pgvector&amp;rsquo;s &lt;code>hnsw&lt;/code> implementation:&lt;/p>
&lt;h3 id="index-building-options">Index building options&lt;/h3>
&lt;p>These options are available in the &lt;code>WITH&lt;/code> clause of &lt;code>CREATE INDEX&lt;/code>, e.g.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">TABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vecs&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">PRIMARY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">KEY&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">768&lt;/span>&lt;span class="p">));&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">INDEX&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">ON&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vecs&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">USING&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">hnsw&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vector_l2_ops&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">WITH&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">m&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">16&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">ef_construction&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">64&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>&lt;code>m&lt;/code>: (default: &lt;code>16&lt;/code>; range: &lt;code>2&lt;/code> to &lt;code>100&lt;/code>) Indicates how many bidirectional links (or paths) exist between each indexed element. Setting this to a higher number can increase recall, but can also significantly increase index build time and may impact query performance.&lt;/li>
&lt;li>&lt;code>ef_construction&lt;/code>: (default: &lt;code>64&lt;/code>; range: &lt;code>4&lt;/code> to &lt;code>1000&lt;/code>) Indicates how many nearest neighbors to check while adding an element to the index. Increasing this value can increase recall, but will also increase index build time. This value must also be at least double &lt;code>m&lt;/code>, e.g. if &lt;code>m&lt;/code> is &lt;code>24&lt;/code> then &lt;code>ef_construction&lt;/code> must be at least &lt;code>48&lt;/code>.&lt;/li>
&lt;/ul>
&lt;p>Note that as of this writing, you must specify the operator class to use with the &lt;code>hnsw&lt;/code> index. For example, to use cosine distance with an HNSW index, you would use a command similar to the below:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">INDEX&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">ON&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vecs&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">USING&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">hnsw&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vector_cosine_ops&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="index-search-options">Index search options&lt;/h3>
&lt;ul>
&lt;li>&lt;code>hnsw.ef_search&lt;/code>: (default: &lt;code>40&lt;/code>; range &lt;code>1&lt;/code> to &lt;code>1000&lt;/code>) Indicates the number of nearest neighbors to keep in a &amp;ldquo;dynamic list&amp;rdquo; while keeping the search. A large value can improve recall, usually with a tradeoff in performance. You need &lt;code>hnsw.ef_search&lt;/code> be at least as big as your &lt;code>LIMIT&lt;/code> value.&lt;/li>
&lt;/ul>
&lt;h3 id="how-to-use-hnsw-in-pgvector">How to use &lt;code>hnsw&lt;/code> in pgvector&lt;/h3>
&lt;p>The default &lt;a href="https://github.com/pgvector/pgvector/pull/230">index build settings&lt;/a> are chosen to optimize build time relative to the recall you can achieve during search. If you&amp;rsquo;re not getting the recall that you expect for your data set, first try increasing the value of &lt;code>ef_construction&lt;/code> before adjusting &lt;code>m&lt;/code>, as adjusting &lt;code>ef_construction&lt;/code> is often a faster operation. There are some studies that show that increasing &lt;code>m&lt;/code> can help with recall for higher dimensionality data sets, though in the &lt;a href="https://jkatz.github.io/post/postgres/pgvector-hnsw-performance/">previous post&lt;/a> we saw that &lt;a href="https://jkatz.github.io/post/postgres/pgvector-hnsw-performance/">pgvector could process OpenAI-style embeddings effectively with high recall&lt;/a>.&lt;/p>
&lt;p>You can increase performance of your search queries by lowing the value &lt;code>hnsw.ef_search&lt;/code>, e.g. set &lt;code>hnsw.ef_search&lt;/code> to &lt;code>20&lt;/code>, though note that this could impact your recall:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">SET&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">hnsw&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">ef_search&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">TO&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">20&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">FROM&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vecs&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">ORDER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">BY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">q&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">&amp;lt;-&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">LIMIT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="improved-performance-of-distance-functions">Improved performance of distance functions&lt;/h2>
&lt;p>Speed is paramount when computing distance between two vectors. Any way you can shave off computation time means you can build indexes and search for vectors more quickly.&lt;/p>
&lt;p>pgvector 0.5.0 does exactly this, improving &lt;a href="https://github.com/pgvector/pgvector/pull/180">distance calculations across the board&lt;/a> with &lt;a href="https://github.com/pgvector/pgvector/pull/180#issuecomment-1640936140">noticeable gains for ARM64 architecture&lt;/a>. By default, pgvector can use CPU acceleration for vector processing through compile flags, and writing the implementation code in certain ways can help unlock performance gains once its compiled.&lt;/p>
&lt;p>The gains in pgvector 0.5.0 are noticeable. To demonstrate this, I ran an experiment on my Mac M1 Pro (2021 edition, 8 CPI, 16 GB RAM) to show the speedup in building an &lt;code>ivfflat&lt;/code> index with both Euclidean (&lt;code>vector_l2_ops&lt;/code>, or the default operator class) and cosine distnace (&lt;code>vector_cosine_ops&lt;/code>) on a table with 1,000,000 768-dimensional vectors using &lt;code>PLAIN&lt;/code> storage:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">TABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vecs&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">PRIMARY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">KEY&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">768&lt;/span>&lt;span class="p">));&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">ALTER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">TABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vecs&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">ALTER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">COLUMN&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">SET&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">STORAGE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">PLAIN&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Below are some of the relevant local settings used to test the index builds:&lt;/p>
&lt;ul>
&lt;li>&lt;code>shared_buffers&lt;/code>: &lt;code>4GB&lt;/code>&lt;/li>
&lt;li>&lt;code>max_wal_size&lt;/code>: &lt;code>10GB&lt;/code>&lt;/li>
&lt;li>&lt;code>work_mem&lt;/code>: &lt;code>8MB&lt;/code>&lt;/li>
&lt;li>&lt;code>max_parallel_mainetance_workers&lt;/code>: &lt;code>0&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Before each run, I ensured that the &lt;code>vecs&lt;/code> table was loaded into memory using the &lt;a href="https://www.postgresql.org/docs/current/pgprewarm.html">&lt;code>pg_prewarm&lt;/code>&lt;/a> extension:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">pg_prewarm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;vecs&amp;#39;&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Finally, I created the &lt;code>ivfflat&lt;/code> index with &lt;code>lists&lt;/code> set to &lt;code>100&lt;/code>. Note that this was to run a series of tests quickly; the recommended value of &lt;code>lists&lt;/code> for 1,000,000 rows is &lt;code>1000&lt;/code>. The effect of the distance calculations may be more pronounced with a larger value of &lt;code>lists&lt;/code>.&lt;/p>
&lt;p>Below is a table summarizing the results. Please note that these results are directional, particularly due to the value of &lt;code>lists&lt;/code>:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Version&lt;/th>
&lt;th>Euclidean (s)&lt;/th>
&lt;th>cosine (s)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>0.4.4&lt;/td>
&lt;td>71.66&lt;/td>
&lt;td>69.92&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>0.5.0&lt;/td>
&lt;td>45.65&lt;/td>
&lt;td>64.02&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The above test showed a noticeable improvement with Euclidean distance and a marginal improvement with cosine distance. Andrew Kane&amp;rsquo;s tests showed a &lt;a href="https://gist.github.com/ankane/96e9750405f0b89974edd7db01168f5d">greater speedup across all distance functions&lt;/a> on ARM64 systems. That said, you should likely see some performance gains in your pgvector workloads, with these being most pronounced on tasks with many distance computations (e.g. index builds, searches over large sets of vectors).&lt;/p>
&lt;h2 id="parallelization-of-ivfflat-index-builds">Parallelization of &lt;code>ivfflat&lt;/code> index builds&lt;/h2>
&lt;p>&lt;code>ivfflat&lt;/code> is comparatively fast when it comes to building indexes, though there are always ways to improve performance. One area is adding parallelization to simultaneously perform operations. To understand how parallelism can benefit &lt;code>ivfflat&lt;/code>, first let&amp;rsquo;s explore the different phases of the index build. Recall that we can build an &lt;code>ivfflat&lt;/code> index on a table using a query like the one below:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">INDEX&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">ON&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vecs&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">USING&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">ivfflat&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">WITH&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">lists&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">100&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now, let&amp;rsquo;s look at the different build phases:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>k-means&lt;/strong>: pgvector samples a subset of all of the vectors in the table to determine &lt;code>k&lt;/code> centers, where &lt;code>k&lt;/code> is the value of &lt;code>lists&lt;/code>. Using the above query, the value of &lt;code>k&lt;/code> is &lt;code>100&lt;/code>.&lt;/li>
&lt;li>&lt;strong>assignment&lt;/strong>: pgvector then goes through every record in the table and assigns it to its closest lists, where the distance is calculated using the selected distance operator (e.g. Euclidean).&lt;/li>
&lt;li>&lt;strong>sort&lt;/strong>: pgvector then sorts the records in each list&lt;/li>
&lt;li>&lt;strong>write-to-disk&lt;/strong>: Finally, pgvector writes the index to disk.&lt;/li>
&lt;/ul>
&lt;p>There are two areas here that can benefit from parallelization:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>k-means&lt;/strong>: K-means is fairly computational heavy, but it&amp;rsquo;s also easily parallelizable.&lt;/li>
&lt;li>&lt;strong>assignment&lt;/strong>: During the assignment phase, pgvector must load every record from the table, which can take a long time if the table is very large.&lt;/li>
&lt;/ul>
&lt;p>Analysis (to be shown further down) around where pgvector was spending the most time during the index building process showed that pgvector was spending the most time in the &lt;strong>assignment&lt;/strong> phase, particularly as the size of the indexable dataset grew. While the time spent in &lt;strong>k-means&lt;/strong> grew quadratically with the number of lists, it was still only a fraction of the time spent compared to &lt;strong>assignment&lt;/strong>. Interestingly, &lt;strong>write-to-disk&lt;/strong> stayed relative constant as a percentage of time across the tests.&lt;/p>
&lt;p>pgvector 0.5.0 added parallelization around the &lt;strong>assignment&lt;/strong> phase, specifically, spawning multiple parallel maintenance workers to scan the table and assign records to the closest list. There are a few parameters you need to be aware of when using this feature:&lt;/p>
&lt;ul>
&lt;li>&lt;code>max_parallel_maintenance_workers&lt;/code>: This is the max number of parallel workers PostgreSQL will spawn for a mainetnance operation, such as an index build. This defaults to &lt;code>2&lt;/code>, so you may need to set it higher to get the full benefit of this feature.&lt;/li>
&lt;li>&lt;code>max_parallel_workers&lt;/code>: This is the max number of parallel workers PostgreSQL spawns for parallel operations. This defaults to &lt;code>8&lt;/code> in PostgreSQL, so you may need to increase it along with &lt;code>max_parallel_maintenance_workers&lt;/code> based upon how much parallelism you need.&lt;/li>
&lt;li>&lt;code>min_parallel_table_scan_size&lt;/code>: This parameter determines how much data must be scanned to determine when and how many parallel workers to spawn. If you&amp;rsquo;re using &lt;code>EXTENDED&lt;/code> / TOAST storage (the default) for your vectors, you may not be getting a parallel scan because &lt;a href="https://www.postgresql.org/message-id/flat/ad8a178f-bbe7-d89d-b407-2f0fede93144%40postgresql.org">PostgreSQL will only consider the amount of space used in the main table, not the TOAST table&lt;/a>. Setting this to a lower value (e.g. &lt;code>SET min_parallel_table_scan_size TO 1&lt;/code>) may help you spawn more parallel workers.&lt;/li>
&lt;/ul>
&lt;p>Below is an example of the impact of parallelization on a 1,000,000 set of 768-dimensional vectors (note, to see parallel workers spawned in your session, you&amp;rsquo;ll have to run &lt;code>SET client_min_messages TO 'DEBUG1';&lt;/code>):&lt;/p>
&lt;p>Serial build:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">INDEX&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">ON&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vecs&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">USING&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">ivfflat&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">WITH&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">lists&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">500&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="n">DEBUG&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">building&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">index&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;vecs_embedding_idx&amp;#34;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">on&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">table&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;vecs&amp;#34;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">serially&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">INDEX&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="n">Time&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">112836&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="mi">829&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">ms&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">01&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mi">52&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="mi">837&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Parallel build with &lt;code>2&lt;/code> workers (including the leader, so really &amp;ldquo;3&amp;rdquo; workers :):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">SET&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">max_parallel_maintenance_workers&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">TO&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">INDEX&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">ON&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vecs&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">USING&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">ivfflat&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">WITH&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">lists&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">500&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="n">DEBUG&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">using&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">parallel&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">workers&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="n">DEBUG&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">worker&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">processed&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">331820&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">tuples&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="n">DEBUG&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">worker&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">processed&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">331764&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">tuples&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="n">DEBUG&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">leader&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">processed&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">336416&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">tuples&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">INDEX&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="n">Time&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">61849&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="mi">137&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">ms&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">01&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mi">01&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="mi">849&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can see that there is almost a 2x speedup with parallel builds. But how much is too much? Let&amp;rsquo;s see what happens when we allow for PostgreSQL to use more parallel workers on this data set:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">SET&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">max_parallel_maintenance_workers&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">TO&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">8&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">INDEX&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">ON&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vecs&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">USING&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">ivfflat&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">WITH&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">lists&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">500&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="n">DEBUG&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">using&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">6&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">parallel&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">workers&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="n">DEBUG&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">worker&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">processed&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">142740&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">tuples&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="n">DEBUG&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">worker&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">processed&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">142394&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">tuples&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="n">DEBUG&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">worker&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">processed&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">142192&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">tuples&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="n">DEBUG&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">worker&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">processed&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">142316&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">tuples&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="n">DEBUG&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">worker&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">processed&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">142674&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">tuples&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="n">DEBUG&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">leader&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">processed&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">142284&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">tuples&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="n">DEBUG&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">worker&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">processed&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">145400&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">tuples&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">INDEX&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="n">Time&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">67140&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="mi">314&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">ms&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">01&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="mi">07&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="mi">140&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can see that for this data set, we were able to achieve max performance with only &lt;code>2&lt;/code> workers, and the leader.&lt;/p>
&lt;p>Where does this feature shave off time? Let&amp;rsquo;s look at a comparison between a few different runs and times between the different phases:&lt;/p>
&lt;h3 id="1000000-768-dim-vectors-lists1000">1,000,000 768-dim vectors, lists=1000&lt;/h3>
&lt;p>Time measured in &lt;strong>seconds&lt;/strong>.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Build&lt;/th>
&lt;th>Total&lt;/th>
&lt;th>k-means&lt;/th>
&lt;th>assignment&lt;/th>
&lt;th>sort&lt;/th>
&lt;th>write-to-disk&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Serial&lt;/td>
&lt;td>185&lt;/td>
&lt;td>32&lt;/td>
&lt;td>130&lt;/td>
&lt;td>0.04&lt;/td>
&lt;td>23&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Parallel&lt;/td>
&lt;td>80&lt;/td>
&lt;td>34&lt;/td>
&lt;td>21&lt;/td>
&lt;td>1.3&lt;/td>
&lt;td>24&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Speedup&lt;/td>
&lt;td>2.3x&lt;/td>
&lt;td>&lt;/td>
&lt;td>6.1x&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>In the above experiment, we see that we get a 2x+ overall speedup, with a 6x improvement in the assignment phase, though note that the sort has a significant slowdown (albeit it&amp;rsquo;s not noticeable). Let&amp;rsquo;s add more vectors into the mix.&lt;/p>
&lt;h3 id="5000000-768-dim-vectors-lists2000">5,000,000 768-dim vectors, lists=2000&lt;/h3>
&lt;p>Time measured in &lt;strong>seconds&lt;/strong>.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Build&lt;/th>
&lt;th>Total&lt;/th>
&lt;th>k-means&lt;/th>
&lt;th>assignment&lt;/th>
&lt;th>sort&lt;/th>
&lt;th>write-to-disk&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Serial&lt;/td>
&lt;td>1724&lt;/td>
&lt;td>168&lt;/td>
&lt;td>1430&lt;/td>
&lt;td>0.04&lt;/td>
&lt;td>126&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Parallel&lt;/td>
&lt;td>551&lt;/td>
&lt;td>173&lt;/td>
&lt;td>253&lt;/td>
&lt;td>4&lt;/td>
&lt;td>120&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Speedup&lt;/td>
&lt;td>3.1x&lt;/td>
&lt;td>&lt;/td>
&lt;td>5.64x&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Again, in this experiment we see that we save the most time in the assignment phase, though note that the sort time increases roughly linearly as we add more vectors into the build.&lt;/p>
&lt;p>Finally, let&amp;rsquo;s look at one very large example!&lt;/p>
&lt;h3 id="100000000-384-dim-vectors-lists1000">100,000,000 384-dim vectors, lists=1000&lt;/h3>
&lt;p>A few things to note about this test:&lt;/p>
&lt;ul>
&lt;li>First, you&amp;rsquo;d normally use a larger value of lists to drive a better performance/recall ratio across your data set.&lt;/li>
&lt;li>I mainly experimented with different values of &lt;code>max_parallel_maintenance_workers&lt;/code>, and for this data set, the optimal value was around &lt;code>32&lt;/code>.&lt;/li>
&lt;li>Finally, I stopped the tests before the &lt;strong>write-to-disk&lt;/strong> phase completed, but everything was trending much faster than the 23 hours it took to build this serially!&lt;/li>
&lt;/ul>
&lt;p>Time is in &lt;strong>hours&lt;/strong>. All builds were in parallel:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Workers&lt;/th>
&lt;th>Total&lt;/th>
&lt;th>k-means&lt;/th>
&lt;th>assignment&lt;/th>
&lt;th>sort&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>17.9&lt;/td>
&lt;td>0.36&lt;/td>
&lt;td>17.5&lt;/td>
&lt;td>0.08&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>16&lt;/td>
&lt;td>11.3&lt;/td>
&lt;td>0.38&lt;/td>
&lt;td>10.6&lt;/td>
&lt;td>0.34&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>32&lt;/td>
&lt;td>10.6&lt;/td>
&lt;td>0.37&lt;/td>
&lt;td>9.3&lt;/td>
&lt;td>1.01&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>48&lt;/td>
&lt;td>11.7&lt;/td>
&lt;td>0.36&lt;/td>
&lt;td>10.1&lt;/td>
&lt;td>1.26&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Overall, if you have larger data sets, you should see improvements in &lt;code>ivfflat&lt;/code> build times, particularly for larger data sets.&lt;/p>
&lt;h2 id="other-features">Other features&lt;/h2>
&lt;p>pgvector 0.5.0 is a pretty big release, so don&amp;rsquo;t miss out on these other items:&lt;/p>
&lt;ul>
&lt;li>&lt;code>SUM&lt;/code> aggregates: You can now sum up all your vectors, e.g. &lt;code>SELECT sum(embedding) FROM vecs;&lt;/code>.&lt;/li>
&lt;li>Manhattan / Taxicab / L1 distance: This release adds the &lt;code>l1_distance&lt;/code> function so you can find the &lt;a href="https://en.wikipedia.org/wiki/Taxicab_geometry">Manhattan distance&lt;/a> between two vectors (which I &lt;a href="https://jkatz.github.io/about/">personally find very helpful&lt;/a>).&lt;/li>
&lt;li>Element-wise multiplication: You can now multiple two vectors by each other, e.g. &lt;code>SELECT '[1,2,3]'::vector(3) * '[4,5,6]'::vector(3)&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h2 id="whats-next">What&amp;rsquo;s next?&lt;/h2>
&lt;p>Ready to try out or upgrade to pgvector 0.5.0? If you&amp;rsquo;re already using pgvector, once you&amp;rsquo;ve installed the new version into your database, you can upgrade to 0.5.0 using the following command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">ALTER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">EXTENSION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">UPDATE&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>or explicitly via:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">ALTER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">EXTENSION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">vector&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">UPDATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">TO&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;0.5.0&amp;#39;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>There&amp;rsquo;s still &lt;a href="https://github.com/pgvector/pgvector/issues/27">more to come&lt;/a> in pgvector. The 0.5.0 release will help support more vector-driven workloads on PostgreSQL, and is a great setup for future releases that will further expand the capabilities of PostgreSQL as a vector database.&lt;/p>
&lt;p>And last but not least, but a huge THANK YOU to everyone who&amp;rsquo;s worked on pgvector to date, whether its coding, testing, providing feedback, and contributing ideas. A very special thanks to &lt;a href="https://github.com/ankane">Andrew Kane&lt;/a> for his tireless work on the 0.5.0 release, and also many thanks to &lt;a href="https://github.com/pashkinelfe">Pavel Borisov&lt;/a> who did the work unlocked the performance gains ont he distance operations.&lt;/p>
&lt;p>As awesome as the 0.5.0 release of pgvector is, the best has yet to come. &lt;a href="https://github.com/pgvector/pgvector">Stay tuned&lt;/a>!&lt;/p></description></item><item><title>An early look at HNSW performance with pgvector</title><link>https://jkatz.github.io/post/postgres/pgvector-hnsw-performance/</link><pubDate>Thu, 10 Aug 2023 00:00:00 +0000</pubDate><guid>https://jkatz.github.io/post/postgres/pgvector-hnsw-performance/</guid><description>&lt;p>(Disclosure: I have been contributing to pgvector, though I did not work on the HNSW implementation outside of testing).&lt;/p>
&lt;p>The surge of usage of AI/ML systems, particularly around generative AI, has also lead to a surge in requirements to store its output, &lt;a href="https://jkatz.github.io/post/postgres/vectors-json-postgresql/">vector data&lt;/a>, in databases. For more details on this, please read my previous post on how &lt;a href="https://jkatz.github.io/post/postgres/vectors-json-postgresql/">vectors are the new JSON&lt;/a>, as they are the &lt;em>lingua franca&lt;/em> in AI/ML systems.&lt;/p>
&lt;p>&lt;a href="https://github.com/pgvector/pgvector">pgvector&lt;/a> is an open source extension that lets you store vector data in &lt;a href="https://www.postgresql.org/">PostgreSQL&lt;/a> and perform &lt;a href="https://en.wikipedia.org/wiki/Nearest_neighbor_search">nearest neighbor&lt;/a> (K-NN) searches to find similar results. This includes support for exact nearest neighbor searches via scanning an entire table, and &amp;ldquo;approximate nearest neighbor&amp;rdquo; (ANN) via indexing. ANN indexes let you perform faster searches over your vector data, but with a tradeoff of &lt;a href="https://en.wikipedia.org/wiki/Precision_and_recall">recall&lt;/a>&amp;quot;, where &amp;ldquo;recall&amp;rdquo; is the percentage of relevant results returned. A typically K-NN query looks something like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">FROM&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">table&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">ORDER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">BY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">embedding&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">&amp;lt;-&amp;gt;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">query_vector&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">LIMIT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Prior to the v0.5.0 release, pgvector supported one indexing method: &lt;code>ivfflat&lt;/code>. To build an index, the &lt;code>ivfflat&lt;/code> algorithm samples (looks over a subset) your vector data, uses a k-means algorithms to define a number of &lt;code>lists&lt;/code> (or &amp;ldquo;centers&amp;rdquo;), and indexes the vectors in the table by assigning each of them to its closest &amp;ldquo;list&amp;rdquo; by distance (e.g. &lt;a href="https://en.wikipedia.org/wiki/Euclidean_distance">L2 distance&lt;/a>, &lt;a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine distance&lt;/a>). When querying your vector data, you can select how many lists to check using the &lt;code>ivfflat.probes&lt;/code> config parameter. Setting a higher value of &lt;code>ivfflat.probes&lt;/code> increases recall but can decrease query performance.&lt;/p>
&lt;p>The &lt;code>ivfflat&lt;/code> method has many advantages, including its simplicity to use, speed in building the index, and ability to use it as a &amp;ldquo;&lt;a href="https://github.com/facebookresearch/faiss/wiki/Indexing-1G-vectors">coarse quantizer&lt;/a>&amp;rdquo; when indexing over large datasets. However, other algorithms, including &amp;ldquo;&lt;a href="https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf">hierarchical navigable small worlds&lt;/a>&amp;rdquo; &lt;a href="https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf">HNSW&lt;/a>, provide other methodologies for improving the performance / recall ratio of similarity searches. Many HNSW implementations have demonstrated &lt;a href="https://github.com/erikbern/ann-benchmarks">favorable performance/recall tradeoffs&lt;/a>, but usually this comes at a cost of index building time.&lt;/p>
&lt;p>Starting with v0.5.0, pgvector has support for &lt;code>hnsw&lt;/code> thanks to &lt;a href="https://github.com/ankane">Andrew Kane&lt;/a>. Much like its &lt;code>ivfflat&lt;/code> implementation, pgvector users can perform all the expected data modification operations with an &lt;code>hnsw&lt;/code> including insert/update/delete (yes &amp;ndash; &lt;code>hnsw&lt;/code> in pgvector supports update and delete!). The HNSW algorithm is designed for adding data iteratively and does not require to index an existing data set to achieve better recall. For more information on how HNSW works, please read the &lt;a href="https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf">original research paper&lt;/a>.&lt;/p>
&lt;p>While v0.5.0 is not out at the time of this blog post, I wanted to provide an early look at HNSW in pgvector given the excitement for this feature. The rest of this blog post covers how I ran the experiment and some analysis.&lt;/p>
&lt;h2 id="testing-performance--recall-of-the-pgvector-hnsw-implementation">Testing performance / recall of the pgvector HNSW implementation&lt;/h2>
&lt;p>When testing ANN indexing algorithms, you cannot look only at query performance. You may have an indexing algorithm that returns results quickly, but if the recall of those results is low, you may not be delivering the best possible results to your users. This is why it&amp;rsquo;s important to measure recall alongside performance to understand how you can optimize your index for both speed and relevance of your search results.&lt;/p>
&lt;p>It&amp;rsquo;s also important to see how long it takes to build an index. This is a practical consideration for running vector storage systems in production, as different implementations may need to take locks on the primary table that prevent changes during a build. PostgreSQL offers the ability to both &lt;a href="https://www.postgresql.org/docs/current/sql-createindex.html">create indexes concurrently&lt;/a> and to &lt;a href="https://www.postgresql.org/docs/current/sql-reindex.html">reindex concurrently&lt;/a>, which lets you continue to make changes to data in a table without blocking writes.&lt;/p>
&lt;p>Here&amp;rsquo;s a summary of different performance attributes that I analyzed in this experiment:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Performance / Recall ratio&lt;/strong>: As mentioned, in ANN indexes, you must look at both performance AND recall to make an informed judgement on how well the index worked.&lt;/li>
&lt;li>&lt;strong>Index build time&lt;/strong>: How long does it take to build the index? This affects both maintaining the index and how quickly you can add additional data.&lt;/li>
&lt;li>&lt;strong>Index size&lt;/strong>: How much space does the index take up on disk? (This lead to a &lt;a href="https://github.com/erikbern/ann-benchmarks/pull/456">small patch in the ANN Benchmark suite&lt;/a>, and a major kudos to their quick review!)&lt;/li>
&lt;/ul>
&lt;p>For this experiment, I focused entirely on optimizing my environment for performance, particularly around keeping the workload in memory. I used an &lt;a href="https://aws.amazon.com/ec2/instance-types/r7g/">r7gd.16xlarge&lt;/a> (64 vCPU, 512GM RAM) instance with my PostgreSQL data directory installed directly on the local NVMe. I used &lt;a href="https://www.postgresql.org/about/news/postgresql-15-released-2526/">PostgreSQL 15.3&lt;/a> and ran all of the tests locally, i.e. the benchmarking suite connected directly to the PostgreSQL instance over a local socket. I used the &lt;a href="https://github.com/erikbern/ann-benchmarks">ANN Benchmark&lt;/a> test framework (more on this below), ran one test at a time, and note that the framework runs a single query at a time.&lt;/p>
&lt;p>For test data, I used the following known data sets from the &lt;a href="https://github.com/erikbern/ann-benchmarks">ANN Benchmark&lt;/a> suite. In the parenthesis, I use (data set size, vector dimensions):&lt;/p>
&lt;ul>
&lt;li>mnist-784-euclidean (60K, 784-dim)&lt;/li>
&lt;li>sift-128-euclidean (1M, 128-dim)&lt;/li>
&lt;li>gist-960-euclidean (1M, 960-dim)&lt;/li>
&lt;li>dbpedia-openai-1000k-angular (1M, 1536-dim)&lt;/li>
&lt;/ul>
&lt;p>Running these with the ANN Benchmark testing suite required &lt;a href="https://gist.github.com/jkatz/28a2f174effa987713926ce37bb5d304">creating new modules&lt;/a> for the HNSW implementation for pgvector where I did the following:&lt;/p>
&lt;ul>
&lt;li>Store the vectors in the table using &lt;code>PLAIN&lt;/code> storage, i.e. the data was not &lt;a href="https://www.postgresql.org/docs/current/storage-toast.html">TOASTed&lt;/a> but stored inline.&lt;/li>
&lt;li>Built the index (and measured total build time + index size)&lt;/li>
&lt;li>&lt;a href="https://www.postgresql.org/docs/current/pgprewarm.html">Prewarmed the shared buffer cache&lt;/a> to start with the table and index data in memory.&lt;/li>
&lt;/ul>
&lt;p>For PostgreSQL configuration, I had the following settings, with a focus on keeping the workload in memory:&lt;/p>
&lt;ul>
&lt;li>&lt;code>shared_buffers&lt;/code>: 128GB&lt;/li>
&lt;li>&lt;code>effective_cache_size&lt;/code>: 256GB&lt;/li>
&lt;li>&lt;code>work_mem&lt;/code>: 4GB&lt;/li>
&lt;li>&lt;code>maintenance_work_mem&lt;/code>: 64GB&lt;/li>
&lt;li>&lt;code>max_wal_size&lt;/code>: 20GB&lt;/li>
&lt;li>&lt;code>wal_compression&lt;/code>: zstd&lt;/li>
&lt;li>&lt;code>checkpoint_timeout&lt;/code>: 2h&lt;/li>
&lt;li>&lt;code>jit&lt;/code>: off&lt;/li>
&lt;/ul>
&lt;p>Additionally, as parallelism in the index building process for &lt;code>ivfflat&lt;/code> is also a pgvector v0.5.0, I&amp;rsquo;ll note the following parallelism settings:&lt;/p>
&lt;ul>
&lt;li>&lt;code>max_worker_processes&lt;/code>: 128&lt;/li>
&lt;li>&lt;code>max_parallel_workers&lt;/code>: 64&lt;/li>
&lt;li>&lt;code>max_parallel_maintenance_workers&lt;/code>: 64&lt;/li>
&lt;li>&lt;code>max_parallel_workers_per_gather&lt;/code>: 64&lt;/li>
&lt;li>&lt;code>min_parallel_table_scan_size&lt;/code>: 1&lt;/li>
&lt;/ul>
&lt;p>Next, we need to consider what parameters we need to test and vary for testing the HNSW implementation. hnswlib as an explanation for &lt;a href="https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md">how the different parameters&lt;/a> work, with my attempt to summarize them below:&lt;/p>
&lt;ul>
&lt;li>&lt;code>M&lt;/code>: An index building parameter that indicates how many bidirectional links (or paths) exist between each indexed element. This is a key piece in how we search over an HNSW index.&lt;/li>
&lt;li>&lt;code>ef_construction&lt;/code>: An index building parameter that indicates how many nearest neighbors to check while adding an element to the index.&lt;/li>
&lt;li>&lt;code>ef_search&lt;/code>: This is a search parameter that indicates the number of nearest neighbors to keep in a &amp;ldquo;dynamic list&amp;rdquo; while keeping the search. A large value can improve recall, usually with a tradeoff in performance. You need &lt;code>ef_search&lt;/code> be at least as big as your &lt;code>LIMIT&lt;/code> value.&lt;/li>
&lt;/ul>
&lt;p>To set up the actual code to test, given this is a moving target, I picked pgvector at commit &lt;a href="https://github.com/pgvector/pgvector/commit/600ca5a7">600ca5a7&lt;/a> to test both its ivfflat and HNSW implementation. To have an additional implementation to compare against, I also selected pg_embedding, another recent HNSW implementation targeted for PostgreSQL. In summary, I tested each implementation at the following commits, which we the latest when I ran these tests:&lt;/p>
&lt;ul>
&lt;li>&lt;code>pgvector&lt;/code> (ivfflat): &lt;a href="https://github.com/pgvector/pgvector/commit/600ca5a7">600ca5a7&lt;/a>&lt;/li>
&lt;li>&lt;code>pgvector_hnsw&lt;/code> (hnsw) &lt;a href="https://github.com/pgvector/pgvector/commit/600ca5a7">600ca5a7&lt;/a>&lt;/li>
&lt;li>&lt;code>pg_embedding&lt;/code> (hnsw) &lt;a href="https://github.com/neondatabase/pg_embedding/commit/e1db3a5a">e1db3a5a&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Finally, we need test parameters. For &lt;code>ivfflat&lt;/code>, I used the &lt;a href="https://github.com/erikbern/ann-benchmarks/blob/main/ann_benchmarks/algorithms/pgvector/config.yml">existing configuration&lt;/a> in ANN Benchmark that varies lists and probes. For the HNSW algorithms, I took the test parameters from the mirrored test parameters from the other HNSW implementations, but only test a subset of construction parameters.&lt;/p>
&lt;ul>
&lt;li>Construction
&lt;ul>
&lt;li>M=12, ef_construction=60&lt;/li>
&lt;li>M=16, ef_construction=40&lt;/li>
&lt;li>M=24, ef_construction=40&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Search: ef_search=10, ef_search=20, ef_search=40, ef_search=80, ef_search=120, ef_search=200, ef_search=400, ef_search=600, ef_search=800,&lt;/li>
&lt;/ul>
&lt;p>(I do want to personally understand the impact of the construction parameters more, and will be running tests in the future that dive further into the variations. However, running the test suite does take time!)&lt;/p>
&lt;p>Running these with the ANN Benchmark suite required creating &lt;a href="https://gist.github.com/jkatz/28a2f174effa987713926ce37bb5d304">new modules&lt;/a>. Credit to &lt;a href="https://github.com/pashkinelfe">Pavel Borisov&lt;/a> for most of the work on the pg_embedding HNSW test. I anticipate portions of this work to be proposed for the upstream ANN Benchmark project once HNSW is released for pgvector.&lt;/p>
&lt;p>So, how did the tests go?&lt;/p>
&lt;h2 id="results-and-analysis">Results and Analysis&lt;/h2>
&lt;p>The first thing to emphasize is that these tests were conducted at a specific point in time in all the projects involved. The results today may not match the results tomorrow, but they provide a directional snapshot of where things stand today. I also wanted to use this as an exercise to highlight the tradeoffs on where you may want to use &lt;code>ivfflat&lt;/code> vs. &lt;code>hnsw&lt;/code>, as both indexes have utility.&lt;/p>
&lt;p>For the rest of this section, we&amp;rsquo;ll look at the tests in each data set, and explore:&lt;/p>
&lt;ul>
&lt;li>The query performance / recall ratio&lt;/li>
&lt;li>Index build time&lt;/li>
&lt;li>Index size&lt;/li>
&lt;/ul>
&lt;h3 id="mnist-784-euclidean-60k-784-dim">mnist-784-euclidean (60K, 784-dim)&lt;/h3>
&lt;p>This is my &amp;ldquo;sanity check&amp;rdquo; data set: it&amp;rsquo;s not too large (you can run tests quickly), but it also has higher dimensionality vector (you can see some of the oddities that come with higher dimensions). I have used it to successfully hunt down obvious performance regressions.&lt;/p>
&lt;h4 id="pgvector-ivfflat">pgvector (ivfflat)&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Parameters&lt;/th>
&lt;th>Build time (min)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>lists=100&lt;/td>
&lt;td>0.19&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>lists=200&lt;/td>
&lt;td>0.20&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>lists=400&lt;/td>
&lt;td>0.24&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>lists=1000&lt;/td>
&lt;td>0.54&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>lists=2000&lt;/td>
&lt;td>0.91&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>lists=4000&lt;/td>
&lt;td>1.56&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>NOTE:&lt;/strong> I did not get the index size as I did not run the test with &lt;a href="https://github.com/erikbern/ann-benchmarks/pull/456">this patch&lt;/a> in place. However, the index size is roughly the same size as the vector column indexed in the table, plus overhead with the lists&lt;/p>
&lt;h4 id="pgvector_hnsw-hnsw">pgvector_hnsw (hnsw)&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Parameters&lt;/th>
&lt;th>Build time (min)&lt;/th>
&lt;th>Index size (MB)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>M=12,ef_construction=60&lt;/td>
&lt;td>0.87&lt;/td>
&lt;td>234&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>M=16,ef_construction=40&lt;/td>
&lt;td>1.02&lt;/td>
&lt;td>234&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>M=24,ef_construction=40&lt;/td>
&lt;td>1.45&lt;/td>
&lt;td>234&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="pg_embedding_hnsw-hnsw">pg_embedding_hnsw (hnsw)&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Parameters&lt;/th>
&lt;th>Build time (min)&lt;/th>
&lt;th>Index size (MB)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>M=12,ef_construction=60&lt;/td>
&lt;td>1.28&lt;/td>
&lt;td>234&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>M=16,ef_construction=40&lt;/td>
&lt;td>1.16&lt;/td>
&lt;td>234&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>M=24,ef_construction=40&lt;/td>
&lt;td>1.16&lt;/td>
&lt;td>234&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;img src="https://jkatz.github.io/images/mnist-784-euclidean.png" alt="mnist-784-euclidean.png">&lt;/p>
&lt;p>Analysis:&lt;/p>
&lt;ul>
&lt;li>This is the case-in-point why you must measure both performance and recall. From a pure performance standpoint, pgvector&amp;rsquo;s &lt;code>ivfflat&lt;/code> index does have higher QPS (though not by much compared to pgvector&amp;rsquo;s &lt;code>hnsw&lt;/code>), but at a great expense with recall.&lt;/li>
&lt;li>This is a small dataset, so we can&amp;rsquo;t draw too many conclusions. However, the results show that pgvector&amp;rsquo;s &lt;code>hnsw&lt;/code> implementation does have very strong performance / recall characteristics on this smaller data set.&lt;/li>
&lt;/ul>
&lt;h3 id="sift-128-euclidean-1m-128-dim">sift-128-euclidean (1M, 128-dim)&lt;/h3>
&lt;p>This is another data set I like to run against earlier on in my testing, as it&amp;rsquo;s much larger (1 million vectors) but has lower dimensionality, so testing is generally faster. You&amp;rsquo;ll also start to see the effect of indexing these vectors at scale. Additionally, because this is lower dimensionality (I still find it amusing that 128 dimensions is &amp;ldquo;low&amp;rdquo;), you will see less of the higher dimensionality quirks in your searches.&lt;/p>
&lt;h4 id="pgvector-ivfflat-1">pgvector (ivfflat)&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Parameters&lt;/th>
&lt;th>Build time (min)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>lists=100&lt;/td>
&lt;td>0.61&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>lists=200&lt;/td>
&lt;td>0.61&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>lists=400&lt;/td>
&lt;td>0.63&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>lists=1000&lt;/td>
&lt;td>0.75&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>lists=2000&lt;/td>
&lt;td>1.35&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>lists=4000&lt;/td>
&lt;td>3.58&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>NOTE:&lt;/strong> I did not get the index size as I did not run the test with &lt;a href="https://github.com/erikbern/ann-benchmarks/pull/456">this patch&lt;/a> in place. However, the index size is roughly the same size as the vector column indexed in the table, plus overhead with the lists&lt;/p>
&lt;h4 id="pgvector_hnsw-hnsw-1">pgvector_hnsw (hnsw)&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Parameters&lt;/th>
&lt;th>Build time (min)&lt;/th>
&lt;th>Index size (MB)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>M=12,ef_construction=60&lt;/td>
&lt;td>12.20&lt;/td>
&lt;td>769&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>M=16,ef_construction=40&lt;/td>
&lt;td>15.91&lt;/td>
&lt;td>782&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>M=24,ef_construction=40&lt;/td>
&lt;td>25.23&lt;/td>
&lt;td>902&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="pg_embedding_hnsw-hnsw-1">pg_embedding_hnsw (hnsw)&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Parameters&lt;/th>
&lt;th>Build time (min)&lt;/th>
&lt;th>Index size (MB)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>M=12,ef_construction=60&lt;/td>
&lt;td>14.08&lt;/td>
&lt;td>651&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>M=16,ef_construction=40&lt;/td>
&lt;td>11.27&lt;/td>
&lt;td>651&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>M=24,ef_construction=40&lt;/td>
&lt;td>11.88&lt;/td>
&lt;td>710&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;img src="https://jkatz.github.io/images/sift-128-euclidean.png" alt="sift-128-euclidean.png">&lt;/p>
&lt;p>Analysis:&lt;/p>
&lt;ul>
&lt;li>Again, it&amp;rsquo;s crucial to look at both performance and recall. &lt;code>ivfflat&lt;/code> has high throughput, but on this test, it underperforms both &lt;code>hnsw&lt;/code> implementations when looking at the same relative recall.&lt;/li>
&lt;li>pgvector&amp;rsquo;s &lt;code>hnsw&lt;/code> implementation achieves better performance and recall compared to the other two algorithms on this data set, though they all trend towards similar performance when increasing the search area.&lt;/li>
&lt;li>&lt;code>ivfflat&lt;/code> is incredibly quick for building indexes (and note this leverages the parallel build feature in v0.5.0).&lt;/li>
&lt;li>We can start to see some differences in index build time and size between the two HNSW implementations. On the surface, it appears increasing &lt;code>ef_construction&lt;/code> impacts pg_embedding&amp;rsquo;s HNSW implementation, whereas for pgvector it&amp;rsquo;s in increasing &lt;code>M&lt;/code>, but this would require a deeper investigation on varying the parameters to draw additional conclusions.&lt;/li>
&lt;/ul>
&lt;h3 id="gist-960-euclidean-1m-960-dim">gist-960-euclidean (1M, 960-dim)&lt;/h3>
&lt;p>I like this dataset because this is where you start to see different implementations sweat. It has a nice size (1M vectors) and a higher dimensionality (960), so your index building and lookups are going to need to do more work.&lt;/p>
&lt;p>I did not include the &lt;code>ivfflat&lt;/code> implementation in this test due to time so I could prioritize the final test.&lt;/p>
&lt;h4 id="pgvector_hnsw-hnsw-2">pgvector_hnsw (hnsw)&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Parameters&lt;/th>
&lt;th>Build time (min)&lt;/th>
&lt;th>Index size (MB)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>M=12,ef_construction=60&lt;/td>
&lt;td>41.37&lt;/td>
&lt;td>4,316&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>M=16,ef_construction=40&lt;/td>
&lt;td>60.20&lt;/td>
&lt;td>7,688&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>M=24,ef_construction=40&lt;/td>
&lt;td>112.04&lt;/td>
&lt;td>7,812&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="pg_embedding_hnsw-hnsw-2">pg_embedding_hnsw (hnsw)&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Parameters&lt;/th>
&lt;th>Build time (min)&lt;/th>
&lt;th>Index size (MB)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>M=12,ef_construction=60&lt;/td>
&lt;td>50.48&lt;/td>
&lt;td>3,906&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>M=16,ef_construction=40&lt;/td>
&lt;td>45.77&lt;/td>
&lt;td>3,906&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>M=24,ef_construction=40&lt;/td>
&lt;td>49.01&lt;/td>
&lt;td>3,906&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;img src="https://jkatz.github.io/images/gist-960-euclidean.png" alt="gist-960-euclidean.png">&lt;/p>
&lt;p>Analysis:&lt;/p>
&lt;ul>
&lt;li>From both a performance/recall standpoint, pgvector&amp;rsquo;s HNSW implementation outperforms pg_embedding on this data set, though the gap closes for higher values of &lt;code>ef_search&lt;/code>.&lt;/li>
&lt;li>pg_embedding has smaller indexes on this build, and pgvector&amp;rsquo;s index build time increased with larger values of &lt;code>M&lt;/code> whereas pg_embedding stayed flat. There&amp;rsquo;s likely wor&lt;/li>
&lt;/ul>
&lt;h3 id="dbpedia-openai-1000k-angular-1m-1536-dim">dbpedia-openai-1000k-angular (1M, 1536-dim)&lt;/h3>
&lt;p>This is the dataset-du-jour that provides vector data that is similar to what you would find in an LLM, in this case, 1536 dimensional vectors. Unlike the other examples above, this data set uses a cosine distance operation.&lt;/p>
&lt;h4 id="pgvector-ivfflat-2">pgvector (ivfflat)&lt;/h4>
&lt;p>I only completed two runs, and would like to re-run it again during the next point-in-time evaluation of the algorithms.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Parameters&lt;/th>
&lt;th>Build time (min)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>lists=200&lt;/td>
&lt;td>16.55&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>lists=400&lt;/td>
&lt;td>16.68&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>NOTE:&lt;/strong> I did not get the index size as I did not run the test with &lt;a href="https://github.com/erikbern/ann-benchmarks/pull/456">this patch&lt;/a> in place. However, the index size is roughly the same size as the vector column indexed in the table, plus overhead with the lists&lt;/p>
&lt;h4 id="pgvector_hnsw-hnsw-3">pgvector_hnsw (hnsw)&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Parameters&lt;/th>
&lt;th>Build time (min)&lt;/th>
&lt;th>Index size (MB)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>M=12,ef_construction=60&lt;/td>
&lt;td>49.40&lt;/td>
&lt;td>7,734&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>M=16,ef_construction=40&lt;/td>
&lt;td>60.10&lt;/td>
&lt;td>7,734&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>M=24,ef_construction=40&lt;/td>
&lt;td>82.35&lt;/td>
&lt;td>7,734&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="pg_embedding_hnsw-hnsw-3">pg_embedding_hnsw (hnsw)&lt;/h4>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Parameters&lt;/th>
&lt;th>Build time (min)&lt;/th>
&lt;th>Index size (MB)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>M=12,ef_construction=60&lt;/td>
&lt;td>72.22&lt;/td>
&lt;td>7,734&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>M=16,ef_construction=40&lt;/td>
&lt;td>67.06&lt;/td>
&lt;td>7,734&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>M=24,ef_construction=40&lt;/td>
&lt;td>68.56&lt;/td>
&lt;td>7,734&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;img src="https://jkatz.github.io/images/dbpedia-openai-1000k-angular.png" alt="dbpedia-openai-1000k-angular.png">&lt;/p>
&lt;p>Analysis:&lt;/p>
&lt;ul>
&lt;li>pgvector&amp;rsquo;s HNSW implementation has better performance/recall ratio than the other algorithms by a fairly wide margin.&lt;/li>
&lt;li>You can&amp;rsquo;t beat ivfflat in index building times: you can understand why it&amp;rsquo;s popular as a coarse quantizer. The index building times between pgvector&amp;rsquo;s HNSW and pg_embedding&amp;rsquo;s HNSW are more comparable here, which may be due to the implementation of the cosine distance functions.&lt;/li>
&lt;li>I&amp;rsquo;m surprised the indexes are the same size; I need to drill into that more.&lt;/li>
&lt;/ul>
&lt;h2 id="whats-next">What&amp;rsquo;s next?&lt;/h2>
&lt;p>As mentioned, this is just a first look at pgvector&amp;rsquo;s HNSW implementation at a specific commit (&lt;a href="https://github.com/pgvector/pgvector/commit/600ca5a7">600ca5a7&lt;/a>). Even since I ran the tests, there have been additional commits that iterate on its HNSW implementation. I&amp;rsquo;m personally excited to see where things land for the v0.5.0.&lt;/p>
&lt;p>This is also to serve as a lesson that this is a fast moving space, not just for storing vector data in PostgreSQL, but in general. As I mentioned in &lt;a href="https://jkatz.github.io/post/postgres/vectors-json-postgresql/">Vectors are the new JSON in PostgreSQL&lt;/a>, while the vector itself is very old, modern processing research is still relatively new (~20 years) and we&amp;rsquo;re still finding new innovations in this space. There are still some proven research techniques that pgvector can adopt that will help in all areas: performance/recall, index sizes, and build times.&lt;/p>
&lt;p>I think PostgreSQL will continue to be in a place where folks want to use it as a vector database, not just because it&amp;rsquo;s convenient, but because it can handle the demands of high performance workloads. The addition of the HNSW algorithm to the PostgreSQL extension ecosystem is one step, though a big step, in continuing to help people successfully store and search over vector data in PostgreSQL.&lt;/p></description></item><item><title>Vectors are the new JSON in PostgreSQL</title><link>https://jkatz.github.io/post/postgres/vectors-json-postgresql/</link><pubDate>Mon, 26 Jun 2023 00:00:00 +0000</pubDate><guid>https://jkatz.github.io/post/postgres/vectors-json-postgresql/</guid><description>&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Row_and_column_vectors">Vectors&lt;/a> are the new &lt;a href="https://en.wikipedia.org/wiki/JSON">JSON&lt;/a>.&lt;/p>
&lt;p>That in itself is an interesting statement, given vectors are a well-studied mathematical structure, and JSON is a data interchange format. And yet in the world of data storage and retrieval, both of these data representations have become the &lt;a href="https://en.wikipedia.org/wiki/Lingua_franca">lingua franca&lt;/a> of their domains and are either essential, or soon-to-be-essential, ingredients in modern application development. And if current trends continue (I think they will), vectors will be as crucial as JSON is for building applications.&lt;/p>
&lt;p>Generative AI and all the buzz around it has caused developers to look for convenient ways to store and run queries against the outputs of these systems, with &lt;a href="https://www.postgresql.org/">PostgreSQL&lt;/a> being a natural choice for a lot of reasons. But even with the hype around generative AI, this is not a new data pattern. Vectors, as a mathematical concept, have been around for hundreds of years. Machine learning has over a half-century worth of research. The &lt;a href="https://www.postgresql.org/docs/current/arrays.html">array&lt;/a> &amp;ndash; the fundamental data structure for a vector &amp;ndash; is taught in most introductory computer science classes. Even PostgreSQL has had support for vector operations for over 20 years (more on that later)!&lt;/p>
&lt;p>So, what is new? It&amp;rsquo;s the &lt;em>accessibility&lt;/em> of these AI/ML algorithms and how easy it is to represent some &amp;ldquo;real world&amp;rdquo; structure (text, images, video) as a vector and store it for some future use by an application. And again, while folks may point to the fact it&amp;rsquo;s not new to store the output of these systems (&amp;ldquo;embeddings&amp;rdquo;) in data storage systems, the emergent pattern is the &lt;em>accessibility&lt;/em> of being able to query and return this data in near real-time in almost any application.&lt;/p>
&lt;p>What does this have to do with &lt;a href="https://www.postgresql.org/">PostgreSQL&lt;/a>? Everything! Efficient storage and retrieval of a data type used in a common pattern greatly simplifies app development, lets people to keep their related data in the same place, and can work with existing tooling. We saw this with JSON over 10 years ago, and now we&amp;rsquo;re seeing this with vector data.&lt;/p>
&lt;p>To understand why vectors are the new JSON, let&amp;rsquo;s rewind and look back at what happened as JSON emerged as the de facto data type for web communications.&lt;/p>
&lt;h2 id="a-very-brief-history-of-json-in-postgresql">A very brief history of JSON in PostgreSQL&lt;/h2>
&lt;p>Back during the &amp;ldquo;rise of JSON&amp;rdquo; I was very much still an app developer. The systems I was building either emitted JSON data to the frontend so that it could complete some sort of action (e.g. rendering an updatable widget) or working with a &amp;ldquo;modern&amp;rdquo; API that returned its data in JSON format. What was nice about JSON was its simplicity (very easy to both read and manipulate), yet its relative expressiveness for a data interchange format. There are some things I would have loved to see in JSON &amp;ndash; skewing towards the database-side, I&amp;rsquo;m definitely a fan of having schemas &amp;ndash; but JSON did simplify communicating between systems efficiently, both from a development and operations standpoint.&lt;/p>
&lt;p>While JSON was meant first and foremost to be an interchange format, people did ask &amp;ldquo;well, why can&amp;rsquo;t I just store and query this natively?&amp;rdquo; This lead to the emergence of specialized data storage systems that let you store and query JSON documents. While I had tried out a few different ad hoc JSON storage systems for a very specific problem, I wasn&amp;rsquo;t sure if I wanted to bring them into my application stack due to performance and maintainability reasons (I won&amp;rsquo;t name names, as I did this analysis well over a decade ago and a lot has changed). This lead to the question &amp;ndash; is it possible store &lt;a href="https://www.postgresql.org/docs/current/datatype-json.html">JSON data in PostgreSQL&lt;/a>.&lt;/p>
&lt;p>I remember going to PostgreSQL events eagerly awaiting what updates there were on PostgreSQL-native support for storage and retrieval of JSON documents. I remember the excitement of &lt;a href="https://www.postgresql.org/about/news/postgresql-92-released-1415/">PostgreSQL 9.2 adding the text-based JSON type&lt;/a>. The initial support for JSON in PostgreSQL validated that the content you were storing was valid JSON, and came with a few functions and operators that helped you extract data contained within the document. There was no native indexing support, but you could build &lt;a href="https://www.postgresql.org/docs/current/indexes-expressional.html">expression indexes&lt;/a> if you planned to frequently query a key within the document.&lt;/p>
&lt;p>This initial JSON support in PostgreSQL helped solve several problems for me, specifically, snapshotting the state of several tables in my database, and logging output from APIs I interfaced with. The initial text-based JSON data type did not have much in the way of search capabilities: it was possible to build &lt;a href="https://www.postgresql.org/docs/current/indexes-expressional.html">expression indexes&lt;/a> to query on a particular key in a JSON document, but pragmatically I would store that key in a column next to the JSON document.&lt;/p>
&lt;p>There&amp;rsquo;s a key element in here: the initial support for JSON had limited utility as a &amp;ldquo;JSON database.&amp;rdquo; Yes, we could now store JSON, and we had some limited querying abilities for it, and it was clear it needed more work to compete with the functionality of ad hoc JSON databases. However, PostgreSQL was still &lt;em>good enough&lt;/em> for many of these use-cases, and in part developers were okay with these limitations so long as they could use the text-based JSON data type with their existing application infrastructure. PostgreSQL was also the first relational database to add support for JSON, setting a trend that ultimately lead to the adoption of JSON in the SQL standard.&lt;/p>
&lt;p>However, the viability of PostgreSQL as a &amp;ldquo;JSON database&amp;rdquo; changed with the &lt;a href="https://www.postgresql.org/about/news/postgresql-94-increases-flexibility-scalability-and-performance-1557/">release of PostgreSQL 9.4&lt;/a>. This release added &lt;code>JSONB&lt;/code>, a binary representation of the JSON data type, but came with the ability to use &lt;a href="https://www.postgresql.org/docs/current/datatype-json.html#JSON-INDEXING">GIN indexing&lt;/a> to search on arbitrary data with JSON documents. From a performance standpoint, this put PostgreSQL on par with JSON databases, while still providing all the other benefits of keeping data in a relational database. While it took a few years, PostgreSQL was able to adapt and evolve to support the workloads of applications.&lt;/p>
&lt;p>Support for JSON in PostgreSQL has continued to evolve and improve through the years, and no doubt will continue to do so as PostgreSQL continues to implement and adopt SQL/JSON. I&amp;rsquo;ve talked to PostgreSQL users who use its JSON support to store tens of &lt;em>terabytes&lt;/em> of JSON documents within a PostgreSQL database &amp;ndash; and they have positive feedback on the experience!&lt;/p>
&lt;p>The key part of this story is that developers were willing to bet on PostgreSQL to have a competitive JSON storage system, and worked with its initial limitations until there was more robust support. Which brings us to vectors.&lt;/p>
&lt;h2 id="the-rise-of-the-vector-a-new-kind-of-json">The rise of the vector: &amp;ldquo;a new kind of JSON&amp;rdquo;&lt;/h2>
&lt;p>Vectors are not new, but they&amp;rsquo;re having a surge in popularity these days. As mentioned earlier, this is due to the newfound accessibility of AI/ML systems, and that the output of these systems are vectors. A common use-case is to build a model on stored data (text, sound, video), convert it to vector format, and then use it for &amp;ldquo;&lt;a href="https://en.wikipedia.org/wiki/Semantic_search">semantic search&lt;/a>.&amp;rdquo; In this case, semantic search is performed when you take a new input, convert it to its corresponding vector, and find the most similar results in the database. Similarity is found using a &lt;a href="https://en.wikipedia.org/wiki/Distance">distance function&lt;/a>, such as &lt;a href="https://en.wikipedia.org/wiki/Euclidean_distance">Euclidean&lt;/a> or &lt;a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine distance&lt;/a>, and the results are often capped at the &amp;ldquo;&lt;a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">&lt;strong>k&lt;/strong> nearest neighbors&lt;/a>&amp;rdquo; (K-NN), or &lt;strong>k&lt;/strong> most similar objects. It can take a lot of time to encode the &amp;ldquo;training set&amp;rdquo; of vectors, so it makes sense to &amp;ldquo;cache&amp;rdquo; them in a permanent data storage system, such as a database, and perform K-NN queries there. Having a set of vectors that are ready to be queried for semantic searches makes a generally better experience for users, which has given rise to the notion of needing a &amp;ldquo;vector database.&amp;rdquo;&lt;/p>
&lt;p>Storing a vector is not new in PostgreSQL. The PostgreSQL &lt;a href="https://www.postgresql.org/docs/current/arrays.html">array type&lt;/a> was around when PostgreSQL was first open-sourced in 1996(!), though there were many improvements made to it through the years. In fact, PostgreSQL &amp;ldquo;arrays&amp;rdquo; is a bit of a misnomer, as they can store multiple dimensions of data (e.g. a matrix). Natively in PostgreSQL arrays contain limited functionality around operations that are common amongst vectors, e.g. calculating the &amp;ldquo;distance&amp;rdquo; between two arrays. It&amp;rsquo;s possible to write stored procedures to handle this, but that puts extra work on the developer.&lt;/p>
&lt;p>Fortunately, the &lt;a href="https://www.postgresql.org/docs/current/cube.html">&lt;code>cube&lt;/code>&lt;/a> data type overcomes these limitations. &lt;code>cube&lt;/code> has also been around for 20+ years in the PostgreSQL code base, and is designed for performing operations on higher dimensional vectors* (more on that in a second). &lt;code>cube&lt;/code> contains most of the common distance functions used in vector similarity searches, including Euclidean distance, and can use GiST indexes to perform efficient K-NN queries! However, &lt;code>cube&lt;/code> is capped at storing vectors with 100 dimensions, and many modern AI/ML systems have dimensionality that far exceeds that.&lt;/p>
&lt;p>So, if arrays can handle vector dimensionality, but not operations, and &lt;code>cube&lt;/code>s can handle operations, but not dimensionality, what can we do?&lt;/p>
&lt;h2 id="pgvectorhttpsgithubcompgvectorpgvector-an-open-source-extension-for-storing-and-searching-vectors-in-postgresql">&lt;a href="https://github.com/pgvector/pgvector">&lt;code>pgvector&lt;/code>&lt;/a>: an open source extension for storing and searching vectors in PostgreSQL&lt;/h2>
&lt;p>One of the foundations of PostgreSQL is its extensibility: PostgreSQL has interfaces to create new data types and new indexing methods. This gives us &lt;a href="https://github.com/pgvector/pgvector">pgvector&lt;/a>, an open source &lt;a href="https://www.postgresql.org/docs/current/extend-extensions.html">PostgreSQL extension&lt;/a> that provides an indexable &lt;code>vector&lt;/code> data type. In a nutshell, pgvector lets you store vectors in PostgreSQL and perform K-NN queries with an assortment of distance metrics: Euclidean, cosine, and inner product. As of today, pgvector comes with one index, &lt;code>ivfflat&lt;/code>, which implements the &lt;a href="https://github.com/facebookresearch/faiss/wiki/Faiss-indexes">IVF FLAT&lt;/a> method of vector indexing.&lt;/p>
&lt;p>What happens when you query indexed vector data may be a bit different than how you&amp;rsquo;re used to querying data in PostgreSQL. Due to the computational expense of performing nearest-neighbor searches over high-dimensionality vectors, many vector indexing methods look for &amp;ldquo;approximate&amp;rdquo; answers that are &amp;ldquo;close enough&amp;rdquo; to the correct answer. This has lead to the field of &amp;ldquo;Approximate Nearest Neighbor&amp;rdquo; (ANN) searches. The two dimensions that people look at for ANN queries are the tradeoff between performance and &amp;ldquo;&lt;a href="https://en.wikipedia.org/wiki/Precision_and_recall">recall&lt;/a>&amp;rdquo;, where &amp;ldquo;recall&amp;rdquo; is the percentage of relevant results returned.&lt;/p>
&lt;p>Let&amp;rsquo;s look at the &lt;code>ivfflat&lt;/code> method as an example. When building an &lt;code>ivfflat&lt;/code> index, you decide how many &lt;code>lists&lt;/code> you want to have in it. Each &lt;code>list&lt;/code> represents a &amp;ldquo;center&amp;rdquo;; these centers are calculated using a &lt;a href="https://en.wikipedia.org/wiki/K-means_clustering">k-means&lt;/a> algorithm. Once you determine all of your centers, &lt;code>ivfflat&lt;/code> determines what center each vector is closest to and adds it to the index. When it&amp;rsquo;s time to query your vector data, you then decide how many centers to check, which is determined by the &lt;code>ivfflat.probes&lt;/code> parameter. This is where you see the ANN performance/recall tradeoff: the more centers you visit, the more precise your results, but at the expense of performance.&lt;/p>
&lt;p>Because of the popularity of storing the output of AI/ML in &amp;ldquo;vector databases&amp;rdquo; and of &lt;code>pgvector&lt;/code>, there are plenty of examples for how to use &lt;code>pgvector&lt;/code>. So instead, I&amp;rsquo;ll focus on where things are heading.&lt;/p>
&lt;h1 id="the-next-steps-for-better-support-for-vectors-in-postgresql">The next steps for better support for vectors in PostgreSQL&lt;/h1>
&lt;p>Similar to the PostgreSQL 9.2 days of JSON, we&amp;rsquo;re in the earlier stages of how we store vector data in PostgreSQL &amp;ndash; and while a lot of what we see in both PostgreSQL and pgvector is very good, it&amp;rsquo;s about to get a whole lot better.&lt;/p>
&lt;p>&lt;code>pgvector&lt;/code> can already handle many common use cases for AI/ML data &amp;ndash; I&amp;rsquo;ve already seen many users successfully deploy apps with it! &amp;ndash; so the next step is to help it scale. This is not too different from what happened with JSON and JSONB in PostgreSQL, but having &lt;code>pgvector&lt;/code> as an extension will help things to iterate more rapidly.&lt;/p>
&lt;p>At &lt;a href="https://www.pgcon.org/2023/">PGCon 2023&lt;/a>, which is a PostgreSQL conference where many internals developers gather, I presented a lightning talk called &lt;a href="https://www.slideshare.net/jkatz05/vectors-are-the-new-json-in-postgresql">Vectors are the new JSON&lt;/a> where I shared use-cases and some upcoming challenges with improving PostgreSQL and &lt;code>pgvector&lt;/code> performance for querying vector data. Some problems to tackle (many of which are in progress!) involve adding more parallelism to pgvector, adding support for indexing for vectors with more than 2,000 dimensions, and leverage hardware acceleration where possible to speed up calculations. The good news is that some of these things are not too hard to add, they just require open source contributions!&lt;/p>
&lt;p>There is a lot of excitement around using PostgreSQL as a vector database (emphasis on &lt;strong>database&lt;/strong> ;-), and I expect that, as history has shown with JSON, the PostgreSQL community will find a way to support this emergent workload in a way that&amp;rsquo;s scalable and safe. I do encourage you to provide feedback &amp;ndash; both on PostgreSQL itself and pgvector &amp;ndash; on how you&amp;rsquo;re working with vector data in PostgreSQL, or how you want to work with data in PostgreSQL, as that will help guide the community on providing optimal support for vector queries.&lt;/p></description></item><item><title>A tribute to PostgreSQL 10</title><link>https://jkatz.github.io/post/postgres/postgres-10-tribute/</link><pubDate>Thu, 10 Nov 2022 00:00:00 +0000</pubDate><guid>https://jkatz.github.io/post/postgres/postgres-10-tribute/</guid><description>&lt;p>The &lt;a href="https://www.postgresql.org/about/news/postgresql-151-146-139-1213-1118-and-1023-released-2543/">last release of PostgreSQL 10&lt;/a> took place on Nov 10, 2022. While there are many articles on the new and exciting features in PostgreSQL, I thought this would be a good time to reflect on the impact of PostgreSQL 10.&lt;/p>
&lt;p>PostgreSQL 10 had a lot of firsts. It was the first PostgreSQL release to have the two digit numbering scheme (PostgreSQL 10 vs. PostgreSQL 9.6). It was the first release with logical replication, declarative partitioning, and SCRAM.&lt;/p>
&lt;p>The PostgreSQL 10 release also had personal significance. The &lt;a href="https://www.postgresql.org/about/news/postgresql-10-released-1786/">release announcement&lt;/a> was the first major version release where I helped to assemble the different pieces. This involved reading through the release notes, asking developers what exactly they built, iterating through the content of the release with the community, and working with our international community to localize and translate the content (and if you are interested in helping with any of this process, reach out to me!).&lt;/p>
&lt;p>As we are now at the final release of PostgreSQL 10, I wanted to take a moment to memorialize this release and review the lasting impact it has had on the project and how people run their database workloads on PostgreSQL.&lt;/p>
&lt;h2 id="whats-in-a-number-moving-from-xyz--xy">What&amp;rsquo;s in a number? Moving from X.Y.Z =&amp;gt; X.Y&lt;/h2>
&lt;p>One of the hardest problems in computer science is naming things. There is a corollary that &lt;a href="https://www.postgresql.org/message-id/flat/CA%2BTgmoa2HVxpTUOjTvEMDrZfmdoFuJ3BMaBDE1Y5xAnA%3DXiRLw%40mail.gmail.com">versioning is a close second&lt;/a>. There are a variety of reasons why PostgreSQL moved from a &lt;code>X.Y.Z&lt;/code> versioning scheme to a &lt;code>X.Y&lt;/code>, but one that stood out to me was around the clarity of upgrading. It is easier to explain the upgrade jump between a &amp;ldquo;10&amp;rdquo; and &amp;ldquo;11&amp;rdquo; than a &amp;ldquo;9.5&amp;rdquo; and &amp;ldquo;9.6&amp;rdquo;. It also help remove the terms like &amp;ldquo;PostgreSQL 9&amp;rdquo; from the lexicon, especially given the huge feature difference between 9.0 and 9.6.&lt;/p>
&lt;p>It did take some time to propagate information for how this worked &amp;ndash; for years the release announcements included guidance on the new numbering scheme. Of everything in the PostgreSQL 10 release, this is arguably the most impactful change simply based on how it made people talk about PostgreSQL.&lt;/p>
&lt;h2 id="parallelize-all-the-things">Parallelize &amp;ldquo;all the things!&amp;rdquo;&lt;/h2>
&lt;p>PostgreSQL 9.6 was the first release to introduce parallel query, or the ability to have multiple workers simultaneously retrieve data. PostgreSQL 10 &lt;a href="https://www.postgresql.org/docs/10/release-10.html#id-1.11.6.28.5.3.2">greatly built on this functionality&lt;/a> and added many more parallel operations.&lt;/p>
&lt;p>What I found amazing about this functionality was its immediate impact upon upgrading. Where I was working at the time, I had our production OLTP workload running PostgreSQL 9.4. I began profiling our workload against a PostgreSQL 10 instance, and just with upgrading, I saw a 2.5x speedup over our &lt;strong>entire workload&lt;/strong>. When I drilled into it further, I observed that the parallel query work was a big driver of this.&lt;/p>
&lt;p>There have been a lot of enhancements since PostgreSQL 10 that have made similar &amp;ldquo;just upgrade to get big performance improvements&amp;rdquo;, whether it is around indexing, concurrency management, or query performance optimizations. But I distinctly remember how impressed I was that I could update to PostgreSQL 10 and observe a site-wide performance improvement with minimal tuning.&lt;/p>
&lt;h2 id="an-easier-and-better-way-to-partition">An easier (and better) way to partition&lt;/h2>
&lt;p>Longtime PostgreSQL users note that PostgreSQL had partitioning before PostgreSQL 10. You could create partitioned data using PostgreSQL &amp;ldquo;rules&amp;rdquo; system to help place it into different tables and query it. For a college internship, I actually used this system as part of segmenting and storing data for US TV channel schedules, though it did take a bit of work to set up and maintain the rules system.&lt;/p>
&lt;p>PostgreSQL 10 lowered the complexity of partition management in PostgreSQL by letting users create partitions with simple SQL commands. While subsequent releases expanded the partitioning functionality and improved its performance, PostgreSQL 10 laid the groundwork for this essential feature. You can see the impact of PostgreSQL&amp;rsquo;s partitioning support in the extension ecosystem too, as many extensions either provided more management functions or query mechanisms over data with lots of partitions.&lt;/p>
&lt;h2 id="publishers-subscribers-logical-replication-comes-to-postgresql">Publishers, subscribers: logical replication comes to PostgreSQL&lt;/h2>
&lt;p>When physical replication, or copying the binary data stored on disk between PostgreSQL servers, was introduced in PostgreSQL 9.0, one of the next asks was for logical replication. Logical replication has many uses, including moving data between two different writable PostgreSQL systems, or different major versions, or even into other databases! PostgreSQL 9.4 introduced the fundamental layer for logical replication in PostgreSQL: the ability to &amp;ldquo;decode&amp;rdquo; statements into a different format. This spawned various logical decoding output plugins, including &lt;a href="https://github.com/eulerto/wal2json">wal2json&lt;/a> that turned PostgreSQL writes into JSON that could be used in applications (e.g. &lt;a href="https://en.wikipedia.org/wiki/Change_data_capture">change data capture&lt;/a>).&lt;/p>
&lt;p>PostgreSQL 10 added direct support for &lt;a href="https://www.postgresql.org/docs/current/logical-replication.html">logical replication&lt;/a>, introducing the ability to create publications and subscribers between databases. Instead of relying on extensions or custom-built solutions, PostgreSQL users could run &lt;code>CREATE PUBLICATION&lt;/code> and &lt;code>CREATE SUBSCRIPTION&lt;/code> to set up logical replication.&lt;/p>
&lt;p>Fast forward to today (PostgreSQL 15) and you see a whole array of products using PostgreSQL logical replication as part of their change data capture functionality, major version upgrades, or part of their replication strategy.
While at Crunchy Data, I wrote up a blog post on how you can combine two features originally introduced in PostgreSQL 10 &amp;ndash; declarative partitioning and logical replication &amp;ndash; to create an &lt;a href="https://www.crunchydata.com/blog/active-active-postgres-federation-on-kubernetes">active-active federated PostgreSQL clusters&lt;/a>.&lt;/p>
&lt;p>As of this writing, there is still a bunch of ongoing work on logical replication in PostgreSQL, including making it possible to replicate more objects like sequences and DDL commands. But I remember the release of PostgreSQL 10 being a release that significantly helped users in how they could move data between systems.&lt;/p>
&lt;h2 id="reaching-a-quorum">Reaching a quorum&lt;/h2>
&lt;p>During the PostgreSQL 10 release, I was less familiar with how high availability systems work. While I knew adding &amp;ldquo;quorum commit&amp;rdquo; for synchronous replication was a big feature, I did not fully understand why.&lt;/p>
&lt;p>Here&amp;rsquo;s how to think of quorum commit. Let&amp;rsquo;s say you have 5 PostgreSQL instances: 1 primary and 4 standbys. You can set a rule that you consider a transaction to be committed if it is written to at least 3 instances. Prior to PostgreSQL 10, if you were using synchronous replication, a transaction would only be considered committed if it was written to &lt;strong>all&lt;/strong> instances, not a subset of them.&lt;/p>
&lt;p>Adding quorum commit advanced PostgreSQL&amp;rsquo;s ability to be a part of high availability systems. It also made it easier to use synchronous replicas with PostgreSQL, reducing performance and availability burdens. If you&amp;rsquo;re interested in how many high availability systems work, read up on the &lt;a href="https://raft.github.io/">Raft algorithm&lt;/a>. The paper itself is not too difficult a read (though I did read through sections a few times to understand them), but it does show the significance of a quorum commit.&lt;/p>
&lt;h2 id="making-password-more-secure">Making password more secure&lt;/h2>
&lt;p>I&amp;rsquo;m an unabashed proponent of using the &lt;a href="https://www.rfc-editor.org/rfc/rfc5802">SCRAM password authentication method&lt;/a>. Seriously, &lt;a href="https://www.slideshare.net/jkatz05/get-your-insecure-postgresql-passwords-to-scram">I will talk to you for an hour (or longer if you let me) on why you should use SCRAM&lt;/a>. But why?&lt;/p>
&lt;p>Before PostgreSQL 10, the primary way to use PostgreSQL authentication was with its own &lt;code>md5&lt;/code> authentication method. The &lt;code>md5&lt;/code> method has &lt;a href="https://www.postgresql.org/docs/current/auth-password.html">some known flaws&lt;/a>, including that someone who has the stored &lt;code>md5&lt;/code> and its corresponding username can log in as that username.&lt;/p>
&lt;p>SCRAM changes that. The SCRAM method, in a nutshell, lets two parties verify that the other knows a password without exchanging the password. Even if an attacker is able to capture the &amp;ldquo;SCRAM secret&amp;rdquo; stored on a server, their options for learning the credential are either through a 1/ MITM attack (which can be mitigated through good TLS practices, i.e. &lt;code>sslmode=verify-full&lt;/code>) or 2/ brute-forcing the password.&lt;/p>
&lt;p>Using SCRAM required changes to &lt;a href="https://wiki.postgresql.org/wiki/List_of_drivers">PostgreSQL drivers&lt;/a>. The community responded: you can see from that list that all the libpq and non-libpq based drivers added support for SCRAM. This allowed PostgreSQL to make SCRAM the default authentication method in PostgreSQL 14, and hopefully in the future we can remove &lt;code>md5&lt;/code> altogether.&lt;/p>
&lt;h2 id="lost-at-the-time-found-later-on">Lost at the time, found later on&lt;/h2>
&lt;p>The above features were major highlights in the &lt;a href="https://www.postgresql.org/about/news/postgresql-10-released-1786/">PostgreSQL 10 release announcement&lt;/a>. They withstood the &amp;ldquo;test of time&amp;rdquo; and continue to have impact on users of newer PostgreSQL releases.&lt;/p>
&lt;p>But there were other PostgreSQL 10 features that also had significant impact though lacked fanfare. I&amp;rsquo;m sure I am going to miss some, but here are the ones that I noticed while re-reading the PostgreSQL 10 release notes.&lt;/p>
&lt;p>PostgreSQL 10 introduced the ability to &amp;ldquo;push&amp;rdquo; joins and aggregates to PostgreSQL servers being accessed through the &lt;a href="https://www.postgresql.org/docs/current/postgres-fdw.html">&lt;code>postgres_fdw&lt;/code>&lt;/a>. Instead of having to pull all the data using joins and aggregates from remote PostgreSQL servers into a single-PostgreSQL server for processing, which is a potentially exhaustive operation, that work could be handled on remote servers and reduce the amount of data that needed to be transferred. This has had significant ramifications for federated or sharded workloads, as it decreased the burden on the primary instance.&lt;/p>
&lt;p>PostgreSQL 10 also introduced &amp;ldquo;identity columns&amp;rdquo;, a SQL standard way for specifying &lt;code>serial&lt;/code> or an autoincrement. This means that:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">TABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">xyz&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">serial&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">PRIMARY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">KEY&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>can be written as:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">TABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">xyz&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">GENERATED&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">BY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">DEFAULT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">AS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">IDENTITY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">PRIMARY&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">KEY&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>and be SQL compliant. My SQL code and examples now use the identity column syntax!&lt;/p>
&lt;p>PostgreSQL 10 also added the &lt;a href="https://www.postgresql.org/docs/current/amcheck.html">&lt;code>amcheck&lt;/code>&lt;/a> extension, which has become an essential DBA tool for finding data corruption. PostgreSQL 14 later introduced &lt;a href="https://www.postgresql.org/docs/15/app-pgamcheck.html">&lt;code>pg_amcheck&lt;/code>&lt;/a> to provide a CLI for detecting data corruption.&lt;/p>
&lt;p>Finally, PostgreSQL 10 added the ability to change TLS configuration using a &amp;ldquo;reload&amp;rdquo; instead of a &amp;ldquo;restart&amp;rdquo;. I became a huge fan of this feature when helping to build a system that had automatic rotation of TLS certificates in a PostgreSQL server without forcing any user downtime.&lt;/p>
&lt;h2 id="conclusion-postgresql-10-lives-on">Conclusion: PostgreSQL 10 lives on&lt;/h2>
&lt;p>PostgreSQL has come a long way since the PostgreSQL 10 release five years ago. Given how transformative this release was, and how impactful it has remained over these past five years, I wanted to ensure we could give it an appropriate tribute for its final update.&lt;/p>
&lt;p>It&amp;rsquo;s also important to reflect how far PostgreSQL has come, even in the past five years, from both a functionality and adoption standpoint. I&amp;rsquo;m still amazed at all the conversations I have with people over what kinds of workloads they&amp;rsquo;re running with PostgreSQL, what features they are using and enjoying, and all the different ways they are deploying PostgreSQL!&lt;/p>
&lt;p>While there is always more work to do (e.g. see my comments on ongoing logical replication work), it&amp;rsquo;s also good to step back at how far PostgreSQL has come, and celebrate the releases that helped move the project significantly forward.&lt;/p>
&lt;p>&amp;hellip;and while PostgreSQL 10 was an impactful release, if you are still using it in production, you should seriously consider upgrading &lt;/p></description></item><item><title>BEGIN ATOMIC: a better way to create functions in PostgreSQL 14</title><link>https://jkatz.github.io/post/postgres/postgres-begin-atomic/</link><pubDate>Tue, 26 Jul 2022 00:00:00 +0000</pubDate><guid>https://jkatz.github.io/post/postgres/postgres-begin-atomic/</guid><description>&lt;p>Around this time of year, I am reading through the upcoming PostgreSQL release notes (hello &lt;a href="https://www.postgresql.org/docs/15/release-15.html">PostgreSQL 15&lt;/a>), reading &lt;a href="https://www.postgresql.org/list/">mailing lists&lt;/a>, and talking to &lt;a href="https://www.postgresql.org">PostgreSQL&lt;/a> users to understand what are the impactful features. Many of these features will be highlighted in the &lt;a href="https://www.postgresql.org/about/featurematrix/">feature matrix&lt;/a> and release announcement (hello &lt;a href="https://www.postgresql.org/about/press/presskit14/">PostgreSQL 14&lt;/a>!).&lt;/p>
&lt;p>However, sometimes I miss an impactful feature. It could be that we need to see how the feature is actually used post-release, or it could be that it was missed. I believe one such change is the introduction of the SQL-standard &lt;code>BEGIN ATOMIC&lt;/code> syntax in PostgreSQL 14 that is used to create function and stored procedure bodies.&lt;/p>
&lt;p>Let&amp;rsquo;s see how functions we could create functions before PostgreSQL 14, the drawbacks to this method, and how going forward, &lt;code>BEGIN ATOMIC&lt;/code> makes it easier and safer to manage functions!&lt;/p>
&lt;h2 id="before-postgresql-14-creating-functions-as-strings">Before PostgreSQL 14: Creating Functions as Strings&lt;/h2>
&lt;p>PostgreSQL has supported the ability to create stored functions (or &amp;ldquo;user-defined functions&amp;rdquo;) since POSTGRES 4.2 in 1994 (thanks [Bruce Momjian[(https://momjian.us/)] for the answer on this). When declaring the function body, you would write the code as a string (which is why you see the &lt;code>$$&lt;/code> marks in functions). For example, here is a simple function to add two numbers:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">FUNCTION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">RETURNS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">LANGUAGE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">SQL&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">IMMUTABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">PARALLEL&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">SAFE&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">AS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="err">$$&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="err">$&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="err">$&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="err">$$&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If I want to have functions that calls another user-defined function, I can do so similarly to the above. For example, here is a function that uses the &lt;code>add&lt;/code> function to add up a whole bunch of numbers:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">FUNCTION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">test1_add_stuff&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">RETURNS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">LANGUAGE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">SQL&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">IMMUTABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">PARALLEL&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">SAFE&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">AS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="err">$$&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="err">$&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="err">$&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="err">$&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="err">$&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="p">));&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="err">$$&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>After I create the two functions, if I try to run &lt;code>test1_add_stuff&lt;/code>, it works:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">SELECT test1_add_stuff(1,2,3,4);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> test1_add_stuff
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-----------------
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 10
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>What happens if I drop the &lt;code>add&lt;/code> function?&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">DROP&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">FUNCTION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>It drops successfully:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">DROP FUNCTION add(int, int);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">DROP FUNCTION
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And if I try calling &lt;code>test1_add_stuff&lt;/code> again?&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">SELECT test1_add_stuff(1,2,3,4);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ERROR: function add(integer, integer) does not exist
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">LINE 2: SELECT add(add($1, $2), add($3, $4));
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ^
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">HINT: No function matches the given name and argument types. You might need to add explicit type casts.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">QUERY:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> SELECT add(add($1, $2), add($3, $4));
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">CONTEXT: SQL function &amp;#34;test1_add_stuff&amp;#34; during startup
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Well, this stinks. Using the pre-v14 style of creating custom functions in PostgreSQL lacks dependency tracking. So dropping one function could end up breaking other functions.&lt;/p>
&lt;p>PostgreSQL does support &lt;a href="https://www.postgresql.org/docs/current/ddl-depend.html">dependency tracking&lt;/a>. Prior to PostgreSQL 14, PostgreSQL could track dependencies in functions that involve attributes such as arguments or returns types, but it could not track dependencies within a function body.&lt;/p>
&lt;p>This is where &lt;code>BEGIN ATOMIC&lt;/code> function bodies changes things.&lt;/p>
&lt;h2 id="begin-atomic-a-better-way-to-create-and-manage-postgresql-user-defined-functions">BEGIN ATOMIC: a better way to create and manage PostgreSQL user-defined functions&lt;/h2>
&lt;p>I started looking at the &lt;code>BEGIN ATOMIC&lt;/code> method of creating functions &lt;a href="https://www.postgresql.org/message-id/CAKqncciYKMZSNM0LZuCYoKsGgDBxE%3D%3DLEAAH0svqmajYhO1oxw%40mail.gmail.com">thanks to a note from Morris de Oryx&lt;/a> on the usefulness of this feature (and the fact it was &lt;a href="https://www.postgresql.org/about/featurematrix/detail/393/">previously missing from the feature matrix&lt;/a>). Morris succinctly pointed out two of the most impactful attributes of creating a PostgreSQL function with &lt;code>BEGIN ATOMIC&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>Because PostgreSQL parses the functions during creation, it can catch additional errors.&lt;/li>
&lt;li>PostgreSQL has improved dependency tracking of functions.&lt;/li>
&lt;/ul>
&lt;p>Let&amp;rsquo;s use the above example again to see how &lt;code>BEGIN ATOMIC&lt;/code> changes things.&lt;/p>
&lt;h3 id="dependency-tracking">Dependency Tracking&lt;/h3>
&lt;p>Let&amp;rsquo;s create the &lt;code>add&lt;/code> function using the &lt;code>BEGIN ATOMIC&lt;/code> syntax:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">FUNCTION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">RETURNS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">LANGUAGE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">SQL&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">IMMUTABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">PARALLEL&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">SAFE&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">BEGIN&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">ATOMIC&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="err">$&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="err">$&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">END&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Notice the difference: the function body is no longer represented as a string, but actual code statements. Let&amp;rsquo;s now create a new function called &lt;code>test2_add_stuff&lt;/code> that will use the &lt;code>add&lt;/code> function:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">FUNCTION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">test2_add_stuff&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">RETURNS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">LANGUAGE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">SQL&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">IMMUTABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">PARALLEL&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">SAFE&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">BEGIN&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">ATOMIC&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="err">$&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="err">$&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">),&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">add&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="err">$&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="err">$&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="p">));&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">END&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>As expected, &lt;code>test2_add_stuff&lt;/code> should work correctly:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">SELECT test2_add_stuff(1,2,3,4);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> test2_add_stuff
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">-----------------
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 10
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>What happens if we try to drop the &lt;code>add&lt;/code> function now?&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">DROP FUNCTION add(int,int);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ERROR: cannot drop function add(integer,integer) because other objects depend on it
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">DETAIL: function test2_add_stuff(integer,integer,integer,integer) depends on function add(integer,integer)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">HINT: Use DROP ... CASCADE to drop the dependent objects too.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This is awesome: using &lt;code>BEGIN ATOMIC&lt;/code> when for functions guards against dropping a function that is called by one or more other functions!&lt;/p>
&lt;h3 id="creation-time-parsing-checks">Creation Time Parsing Checks&lt;/h3>
&lt;p>Let&amp;rsquo;s look at one more example at how &lt;code>BEGIN ATOMIC&lt;/code> helps us avoid errors.&lt;/p>
&lt;p>Prior to PostgreSQL 14, there are some checks that do occur at function creation time. For example, let&amp;rsquo;s create a function that calls a function that calls a function.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">FUNCTION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">a&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">RETURNS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">LANGUAGE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">SQL&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">IMMUTABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">PARALLEL&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">SAFE&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">AS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="err">$$&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="err">$&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="err">$$&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">FUNCTION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">b&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">RETURNS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">LANGUAGE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">SQL&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">IMMUTABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">PARALLEL&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">SAFE&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">AS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="err">$$&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">a&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="err">$&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="err">$$&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">FUNCTION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">RETURNS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">LANGUAGE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">SQL&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">IMMUTABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">PARALLEL&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">SAFE&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">AS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="err">$$&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">b&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="err">$&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="err">$$&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Calling &lt;code>c&lt;/code> should work as expected:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">SELECT c(1);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> c
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">---
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> 6
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Let&amp;rsquo;s drop both &lt;code>a&lt;/code> and &lt;code>c&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">DROP&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">FUNCTION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">a&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">DROP&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">FUNCTION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>What happens when we recreate &lt;code>c&lt;/code>?&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">CREATE FUNCTION c(int)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">RETURNS int
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">LANGUAGE SQL
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">IMMUTABLE PARALLEL SAFE
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">AS $$
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> SELECT b($1);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$$;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">CREATE FUNCTION
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The function creation works, even though &lt;code>a&lt;/code> is still missing. While functions created without &lt;code>BEGIN ATOMIC&lt;/code> can check for dependencies within the body of the function itself, they do not check throughout the entire parse tree. We can see our call to &lt;code>c&lt;/code> fail due to &lt;code>a&lt;/code> still missing:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">SELECT c(3);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ERROR: function a(integer) does not exist
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">LINE 2: SELECT a($1) * 2;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ^
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">HINT: No function matches the given name and argument types. You might need to add explicit type casts.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">QUERY:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> SELECT a($1) * 2;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">CONTEXT: SQL function &amp;#34;b&amp;#34; during inlining
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">SQL function &amp;#34;c&amp;#34; during startup
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Using &lt;code>BEGIN ATOMIC&lt;/code> for creating all of these functions will prevent us from getting into this situation, as PostgreSQL will scan the parse tree to ensure all of these functions exist. Let&amp;rsquo;s now recreate the functions to use &lt;code>BEGIN ATOMIC&lt;/code> for their function bodies:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">OR&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">REPLACE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">FUNCTION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">a&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">RETURNS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">LANGUAGE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">SQL&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">IMMUTABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">PARALLEL&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">SAFE&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">BEGIN&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">ATOMIC&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="err">$&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">END&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">OR&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">REPLACE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">FUNCTION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">b&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">RETURNS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">LANGUAGE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">SQL&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">IMMUTABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">PARALLEL&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">SAFE&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">BEGIN&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">ATOMIC&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">a&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="err">$&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">END&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">OR&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">REPLACE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">FUNCTION&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">RETURNS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">LANGUAGE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">SQL&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">IMMUTABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">PARALLEL&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">SAFE&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">BEGIN&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">ATOMIC&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">b&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="err">$&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">END&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now, try dropping &lt;code>a&lt;/code> - PostgreSQL will prevent this from happening.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">DROP FUNCTION a(int);
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">ERROR: cannot drop function a(integer) because other objects depend on it
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">DETAIL: function b(integer) depends on function a(integer)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">function c(integer) depends on function b(integer)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">HINT: Use DROP ... CASCADE to drop the dependent objects too.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Great. But what happens if I do a &lt;code>DROP FUNCTION ... CASCADE&lt;/code> on function &lt;code>a&lt;/code>?&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">DROP FUNCTION a(int) CASCADE;
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">NOTICE: drop cascades to 2 other objects
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">DETAIL: drop cascades to function b(integer)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">drop cascades to function c(integer)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">DROP FUNCTION
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>PostgreSQL drops &lt;strong>all&lt;/strong> functions that depend on &lt;code>a&lt;/code>, including both &lt;code>b&lt;/code> and &lt;code>c&lt;/code>. While this is handy for cleaning up a test example, be careful when cascading drops on our production systems so you do not accidentally remove an important object!&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>The &lt;code>BEGIN ATOMIC&lt;/code> syntax for creating PostgreSQL functions should make managing user-defined functions less error prone, particularly the &amp;ldquo;accidental dropped function&amp;rdquo; case. Going forward I will definitely use this feature to manage my PostgreSQL stored functions.&lt;/p>
&lt;p>PostgreSQL releases are packed with new features. In fact, PostgreSQL 14 has over 190 features listed in the release notes! This makes it possible to miss a feature that may make it much easier for you to build applications with PostgreSQL. I encourage you to read the &lt;a href="https://www.postgresql.org/docs/release/">release notes&lt;/a> to see if there is a feature in them that will help your PostgreSQL experience!&lt;/p></description></item><item><title>Notes on updating to PostgreSQL 14.3, 13.7, 12.11, 11.16, and 10.21</title><link>https://jkatz.github.io/post/postgres/may-2022-release-should-i-update/</link><pubDate>Tue, 07 Jun 2022 00:00:00 +0000</pubDate><guid>https://jkatz.github.io/post/postgres/may-2022-release-should-i-update/</guid><description>&lt;p>On May 12, 2022, the PostgreSQL Global Development Group
&lt;a href="https://www.postgresql.org/about/news/postgresql-143-137-1211-1116-and-1021-released-2449/">released its regular quarterly update&lt;/a> for all of its supported versions (10-14) containing
bug fixes and a security fix for &lt;a href="https://www.postgresql.org/support/security/CVE-2022-1552/">CVE-2022-1552&lt;/a>. Per its &lt;a href="https://www.postgresql.org/support/versioning/">versioning policy&lt;/a>,
the PostgreSQL community advises that users run the
&amp;ldquo;&lt;a href="https://www.postgresql.org/support/versioning/">latest available minor release available for a major version&lt;/a>.&amp;rdquo;
This is generally the correct approach: update releases make each major release
more stable, and the community makes a concerted effort to avoid introducing
breaking changes. You should &lt;em>always&lt;/em> test each update release before releasing
it into your production environment.&lt;/p>
&lt;p>However, there are a few issues that you should be aware when deciding to
upgrade. One issue affects all versions of PostgreSQL 14 through versions 14.3,
and one issue is specific to the May 12, 2022 release You do need to weigh the
decision to upgrade against incorporating the fix for CVE-2022-1552 and the
&lt;a href="https://www.postgresql.org/about/news/postgresql-143-137-1211-1116-and-1021-released-2449/">other bug fixes available in this release&lt;/a>.&lt;/p>
&lt;p>The below explains what each issue is, what versions of PostgreSQL it effects,
the tradeoffs around upgrading and any remediations.&lt;/p>
&lt;h2 id="create-index-concurrently--reindex-concurrently-on-postgresql-14">&lt;code>CREATE INDEX CONCURRENTLY&lt;/code> / &lt;code>REINDEX CONCURRENTLY&lt;/code> on PostgreSQL 14&lt;/h2>
&lt;p>Vacuuming is an
&lt;a href="https://www.postgresql.org/docs/current/routine-vacuuming.html">essential part of PostgreSQL maintenance&lt;/a>
that performs actions such as reclaiming disk space from updated and deleted
rows. The &lt;a href="https://www.postgresql.org/about/news/postgresql-14-released-2318/">GA release of PostgreSQL 14&lt;/a>
(14.0) introduced an
&lt;a href="https://git.postgresql.org/gitweb/?p=postgresql.git;a=commit;h=d9d076222f5b">optimization for &lt;code>VACUUM&lt;/code> when &lt;code>CREATE INDEX CONCURRENTLY&lt;/code> and &lt;code>REINDEX CONCURRENTLY&lt;/code>&lt;/a> were
running at the same time.&lt;/p>
&lt;p>There were a few
&lt;a href="https://www.postgresql.org/message-id/17485-396609c6925b982d%40postgresql.org">bug reports of index corruption in PostgreSQL 14&lt;/a> and shortly after the PostgreSQL 14.3
release, several members of the PostgreSQL community were able to consistently
reproduce the issue. The optimization described in the above paragraph could
lead to cases of silent index corruption when indexes are built with
&lt;a href="https://www.postgresql.org/docs/current/sql-createindex.html">&lt;code>CREATE INDEX CONCURRENTLY&lt;/code>&lt;/a>
or &lt;a href="https://www.postgresql.org/docs/current/sql-reindex.html">&lt;code>REINDEX CONCURRENTLY&lt;/code>&lt;/a>.
The issue was present since PostgreSQL 14.0: it does not affect any of the other
supported versions of PostgreSQL (i.e.. PostgreSQL 10 - 13).&lt;/p>
&lt;p>After some discussion, the PostgreSQL community decided to
&lt;a href="https://git.postgresql.org/gitweb/?p=postgresql.git;a=commitdiff;h=e28bb885196916b0a3d898ae4f2be0e38108d81b">revert the &lt;code>VACUUM&lt;/code> optimization&lt;/a> for
the time being until a solution that does not contain the risk of silent index
corruption can be implemented. The community has discussed how to best detect
this corruption issue using
&lt;a href="https://www.postgresql.org/docs/current/app-pgamcheck.html">&lt;code>pg_amcheck&lt;/code>&lt;/a>,
specifically with the &lt;code>--heapallindexed&lt;/code> flag. This will take an
&lt;a href="https://www.postgresql.org/docs/current/explicit-locking.html">&lt;code>ACCESS SHARE&lt;/code>&lt;/a>
lock on each table, but it will not block &lt;code>VACUUM&lt;/code> and can be run on a standby.
Note that &lt;code>pg_amcheck&lt;/code> can only detect the corruption issue on B-tree indexes,
and the community is unsure if it can detect all cases of corruption.&lt;/p>
&lt;p>If you have run &lt;code>CREATE INDEX CONCURRENTLY&lt;/code> or &lt;code>REINDEX CONCURRENTLY&lt;/code> using
PostgreSQL 14 and need an immediate fix, you can fix your indexes by running
either running &lt;code>REINDEX&lt;/code> or dropping and recreating the index &lt;strong>without&lt;/strong> the
&lt;code>CONCURRENTLY&lt;/code> option. Once PostgreSQL 14.4 is available, you can use
&lt;code>CONCURRENTLY&lt;/code>. The
&lt;a href="https://www.postgresql.org/docs/current/app-reindexdb.html">&lt;code>reindexdb&lt;/code>&lt;/a>
command-line utility can help with the process as the &lt;code>--jobs&lt;/code> flag lets you
execute multiple &lt;code>REINDEX&lt;/code> operations at the same time across the entire
database.&lt;/p>
&lt;h2 id="a-brief-explanation-of-cve-2022-1552">A brief explanation of CVE-2022-1552&lt;/h2>
&lt;p>To understand the other issue, its first necessary to understand the impact of
CVE-2022-1552. As described,
&lt;a href="https://www.postgresql.org/support/security/CVE-2022-1552/">CVE-2022-1552&lt;/a>
closes a vulnerability where an unprivileged user can craft malicious SQL and
use certain commands (Autovacuum, &lt;code>REINDEX&lt;/code>, &lt;code>CREATE INDEX&lt;/code>,
&lt;code>REFRESH MATERIALIZED VIEW&lt;/code>, &lt;code>CLUSTER&lt;/code>, and &lt;code>pg_amcheck&lt;/code>) to escalate to become
a PostgreSQL superuser. A malicious user still needs to have an account with the
PostgreSQL system to perform this exploit.&lt;/p>
&lt;p>Systems that have unprivileged PostgreSQL users that have risk of SQL injection
(e.g. web applications) or multi-tenant systems may be particularly affected by
this CVE. While upgrading to 14.3 et al. fixes the issue, the community provides
guidance that if you cannot take this upgrade, you can still remediate the issue
by disabling autovacuum (with a warning on performance tradeoffs), not running
the above commands, and to not perform restores using the output from
&lt;code>pg_dump&lt;/code>.&lt;/p>
&lt;p>This issue affects all supported versions of PostgreSQL (10-14) but, as the CVE
notes, the issue is quite old and is not patched in unsupported versions (e.g.
9.6 and older).&lt;/p>
&lt;h2 id="creating-an-expression-index-using-an-operator-class-from-a-different-schema">Creating an expression index using an operator class from a different schema&lt;/h2>
&lt;p>Shortly after the May 12, 2022 update release, there was a report on the
PostgreSQL bugs mailing list where a user could not create an
&lt;a href="https://www.postgresql.org/docs/current/indexes-expressional.html">expression index&lt;/a>
as an unprivileged user when
&lt;a href="https://www.postgresql.org/message-id/flat/20220531152855.GA2236210%40nathanxps13#cf62ad5183084f9da0f458c446bb995d">using an operator class from a different schema that was created by a different user&lt;/a>.
Specifically, the case used the the
&lt;a href="https://www.postgresql.org/docs/current/pgtrgm.html#id-1.11.7.42.8">&lt;code>gist_trgm_ops&lt;/code>&lt;/a>
operator class from the &lt;code>pg_trgm&lt;/code> index to allow text similarity operators to be
indexable. While the issue was first reported based on the output of
&lt;a href="https://www.postgresql.org/docs/current/app-pgdump.html">&lt;code>pg_dump&lt;/code>&lt;/a>, this can
be reproduced in a straightforward way using a
&lt;a href="https://www.postgresql.org/message-id/20220526055047.GA3153526%40rfd.leadboat.com">few commands&lt;/a>.
There may be a few other cases where this issue may occur with other expression
indexes, but the above situation has been consistently reproduced.&lt;/p>
&lt;p>The fix for &lt;a href="https://www.postgresql.org/support/security/CVE-2022-1552/">CVE-2022-1552&lt;/a>
introduced this issue and only affects PostgreSQL 14.3, 13.7, 12.11, 11.16, and
10.21. As of the writing of this blog post, there is no fix available. For a
remediation, you can add the operator classes to the same schema where you are
creating the index. Ensure that any changes comply with the security posture
you are enforcing for your database.&lt;/p>
&lt;h2 id="should-i-upgrade-to-143-137-1211-1116-1021">Should I upgrade to 14.3, 13.7, 12.11, 11.16, 10.21?&lt;/h2>
&lt;p>If your database has a single-user and is the PostgreSQL superuser, you should
be able to upgrade without issues. Note that if you are on PostgreSQL 14, you
are still affected by the &lt;code>CREATE INDEX CONCURRENTLY&lt;/code> / &lt;code>REINDEX CONCURRENTLY&lt;/code>
issue and you should not use those commands until the fix is in place.&lt;/p>
&lt;p>For all other cases, you will need to weigh the tradeoffs of the above issues.
If you are on PostgreSQL 14, you will be affected by the
&lt;code>CREATE INDEX CONCURRENTLY&lt;/code> / &lt;code>REINDEX CONCURRENTLY&lt;/code> issue regardless if you
take this update. You should be aware of this issue and not run those commands.
If you have, you may need to reindex. The index corruption issue should not
prevent you from updating from PostgreSQL 14.3.&lt;/p>
&lt;p>If you are running a system that contains an unprivileged PostgreSQL user, you
will need to weigh the tradeoff of incorporating the fix for CVE-2022-1552
versus potential breakage with your application. The bug most likely shows
itself when performing &amp;ldquo;schema migrations&amp;rdquo; or restoring from a &lt;code>pg_dump&lt;/code>, but is
limited to if you are using any operator classes (e.g. for indexing) and how you
have structured your schemas. There may be some other unreported cases
that are affected by this issue, so be sure you test restoring your schema from
a &lt;code>pg_dump&lt;/code> (e.g. &lt;code>pg_dump --schema-only&lt;/code>).&lt;/p>
&lt;p>As the CVE mentions, you can still remediate the vulnerability without
upgrading, but there are performance and potentially stability risks with these
steps. Vacuuming is
&lt;a href="https://www.postgresql.org/docs/current/routine-vacuuming.html">an essential part of PostgreSQL maintenance&lt;/a>
and if you do not use it, your system can end up slowing down. In more extreme
cases, a system can hit
&lt;a href="https://www.postgresql.org/docs/current/routine-vacuuming.html#VACUUM-FOR-WRAPAROUND">transaction ID wraparound&lt;/a>,
which will put a PostgreSQL database into an unusable state.&lt;/p>
&lt;p>If you do not believe your application is affected by the issue with creating
indexes, you should consider upgrading. The fix for CVE-2022-1552 is much easier
to apply than the remediation steps. The remediation carries a risk of
performance degradation and instability for your system, so if you believe it is
safe to take the upgrade, you should do so.&lt;/p>
&lt;p>The PostgreSQL community guidance to
&lt;a href="https://www.postgresql.org/support/versioning/">run the latest release of a major version&lt;/a>
is a good best practice to follow. You should read through the
&lt;a href="https://www.postgresql.org/about/news/postgresql-143-137-1211-1116-and-1021-released-2449/">release announcement&lt;/a> and &lt;a href="https://www.postgresql.org/docs/release/">release notes&lt;/a>
to understand what fixes are available, and test your applications against the
update releases before deploying them to production.&lt;/p></description></item><item><title>"Read-Only" Mode for PostgreSQL</title><link>https://jkatz.github.io/post/postgres/postgres-read-only/</link><pubDate>Sun, 23 Jan 2022 00:00:00 +0000</pubDate><guid>https://jkatz.github.io/post/postgres/postgres-read-only/</guid><description>&lt;p>Typically when discussing having &amp;ldquo;read-only&amp;rdquo; connections to a &lt;a href="https://www.postgresql.org">PostgreSQL&lt;/a> database, it is in the context of connecting to a &lt;a href="https://www.postgresql.org/docs/current/high-availability.html">replica&lt;/a>.&lt;/p>
&lt;p>There are a variety of methods available to route connections with known read-only queries (i.e. queries with &lt;code>SELECT&lt;/code> statements&amp;hellip;that are not calling &lt;a href="https://www.postgresql.org/docs/current/xfunc-volatility.html">&lt;code>VOLATILE&lt;/code> functions&lt;/a> that modify data). This includes connection proxy software like &lt;a href="https://www.pgpool.net/">Pgpool-II&lt;/a> or framework mechanisms such as &lt;a href="https://docs.djangoproject.com/en/4.0/topics/db/multi-db/#automatic-database-routing">Django&amp;rsquo;s database router&lt;/a>.&lt;/p>
&lt;p>However, there are situations where you might need to force read-only connections to your primary (read-write) Postgres instance. Some examples include putting your application into a degraded state to perform a database move or upgrade, or allowing an administrator to inspect a system that may be accumulating &lt;a href="https://www.postgresql.org/docs/current/wal-intro.html">write-ahead logs&lt;/a> that track all changes to the system.&lt;/p>
&lt;p>PostgreSQL has a configuration parameter call &lt;a href="https://www.postgresql.org/docs/current/runtime-config-client.html#GUC-DEFAULT-TRANSACTION-READ-ONLY">&lt;code>default_transaction_read_only&lt;/code>&lt;/a>. Setting &lt;code>default_transaction_read_only&lt;/code> globally to &lt;code>on&lt;/code> forces all connections to disallow writes to the database. &lt;code>default_transaction_read_only&lt;/code> is a reloadable parameter, so you do not need to restart your Postgres instance to use it.&lt;/p>
&lt;p>Here is a quick example of how &lt;code>default_transaction_read_only&lt;/code> works. First, ensure your system does not have &lt;code>default_transaction_read_only&lt;/code> set:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="n">postgres&lt;/span>&lt;span class="o">=#&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">SHOW&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">default_transaction_read_only&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">default_transaction_read_only&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="c1">-------------------------------
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">off&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">row&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="n">postgres&lt;/span>&lt;span class="o">=#&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">TABLE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">IF&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">NOT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">EXISTS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">abc&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="nb">int&lt;/span>&lt;span class="p">);&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">INSERT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">INTO&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">abc&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">VALUES&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">RETURNING&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">CREATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">TABLE&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="c1">----
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">row&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">INSERT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This works as expected: we&amp;rsquo;re able to create a table and insert data. Now let&amp;rsquo;s put the system into &lt;code>default_transaction_read_only&lt;/code> mode (note that I am running this on &lt;a href="https://www.postgresql.org/about/news/postgresql-14-released-2318/">PostgreSQL 14&lt;/a>)&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">ALTER&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">SYSTEM&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">SET&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">default_transaction_read_only&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">TO&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">on&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">pg_reload_conf&lt;/span>&lt;span class="p">();&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">SHOW&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">default_transaction_read_only&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Ensure that &lt;code>default_transaction_read_only&lt;/code> is enabled:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="n">postgres&lt;/span>&lt;span class="o">=#&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">SHOW&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">default_transaction_read_only&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">default_transaction_read_only&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="c1">-------------------------------
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">on&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">row&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now verify that writes are disallowed:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="n">postgres&lt;/span>&lt;span class="o">=#&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">INSERT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">INTO&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">abc&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">VALUES&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">RETURNING&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="n">ERROR&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">cannot&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">execute&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">INSERT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">in&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">a&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">read&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="k">only&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">transaction&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Excellent!&lt;/p>
&lt;p>Note that &lt;code>default_transaction_read_only&lt;/code> is not a panacea: there are some &lt;a href="https://en.wikipedia.org/wiki/Caveat_emptor">caveats&lt;/a> that you should be aware of.&lt;/p>
&lt;p>First, &lt;code>default_transaction_read_only&lt;/code> can be overriden in a session, even if the value is set database-wide. For example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="n">postgres&lt;/span>&lt;span class="o">=#&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">SHOW&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">default_transaction_read_only&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">default_transaction_read_only&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="c1">-------------------------------
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">on&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">row&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="n">postgres&lt;/span>&lt;span class="o">=#&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">SET&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">default_transaction_read_only&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">TO&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">off&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">SET&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="n">postgres&lt;/span>&lt;span class="o">=#&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">INSERT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">INTO&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">abc&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">VALUES&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">RETURNING&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="c1">----
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">row&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">INSERT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Second, when utilizing &lt;code>default_transaction_read_only&lt;/code> with an application, you must also ensure your app can be configured to send only read queries to the database, ensuring a smooth user experience.&lt;/p>
&lt;p>That said, if you have a situation where you need to put a PostgreSQL primary instance into a &amp;ldquo;read-only&amp;rdquo; mode temporarily, you can use &lt;code>default_transaction_read_only&lt;/code> to prevent write transactions.&lt;/p></description></item></channel></rss>