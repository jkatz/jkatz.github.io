<!doctype html><html dir=ltr lang=en data-theme><head><title>pgvector 0.5.0 Feature Highlights and HOWTOs |
Jonathan Katz
</title><meta charset=utf-8><meta name=generator content="Hugo 0.124.1"><meta name=viewport content="width=device-width,initial-scale=1,viewport-fit=cover"><meta name=description content="
      An overview of new features in pgvector 0.5.0, including HNSW, parallel index builds, and more!


    "><link rel=stylesheet href=/css/main.min.861c630407a306e1a79890bffbab9b381c5a0b03cd0c3b79755cc5eeea04b6b0.css integrity="sha256-hhxjBAejBuGnmJC/+6ubOBxaCwPNDDt5dVzF7uoEtrA=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/css/markupHighlight.min.058b31f17db60602cc415fd63b0427e7932fbf35c70d8e341a4c39385f5f6f3e.css integrity="sha256-BYsx8X22BgLMQV/WOwQn55MvvzXHDY40Gkw5OF9fbz4=" crossorigin=anonymous type=text/css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css integrity="sha512-+4zCK9k+qNFUR5X+cKL9EIR+ZOhtIloNl9GIKS57V1MyNsYpYcUrUeQc9vNfzsWfV28IaLL3i96P9sdNyeRssA==" crossorigin=anonymous><link rel="shortcut icon" href=/favicons/favicon.ico type=image/x-icon><link rel=apple-touch-icon sizes=180x180 href=/favicons/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicons/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicons/favicon-16x16.png><link rel=canonical href=https://jkatz.github.io/post/postgres/pgvector-overview-0.5.0/><script type=text/javascript src=/js/anatole-header.min.d0408165d31a17f17bba83038bf54e86121f85021bdf936382e636f0f77a952f.js integrity="sha256-0ECBZdMaF/F7uoMDi/VOhhIfhQIb35NjguY28Pd6lS8=" crossorigin=anonymous></script><script type=text/javascript src=/js/anatole-theme-switcher.min.ea8ebe268922ef9849261a1312cd65b640595e65251ce4c00534a176afd1ac0c.js integrity="sha256-6o6+Joki75hJJhoTEs1ltkBZXmUlHOTABTShdq/RrAw=" crossorigin=anonymous></script><meta name=twitter:card content="summary"><meta name=twitter:title content="pgvector 0.5.0 Feature Highlights and HOWTOs"><meta name=twitter:description content="An overview of new features in pgvector 0.5.0, including HNSW, parallel index builds, and more!"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"post","name":"pgvector 0.5.0 Feature Highlights and HOWTOs","headline":"pgvector 0.5.0 Feature Highlights and HOWTOs","alternativeHeadline":"","description":"
      An overview of new features in pgvector 0.5.0, including HNSW, parallel index builds, and more!


    ","inLanguage":"en","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/jkatz.github.io\/post\/postgres\/pgvector-overview-0.5.0\/"},"author":{"@type":"Person","name":"Jonathan Katz"},"creator":{"@type":"Person","name":"Jonathan Katz"},"accountablePerson":{"@type":"Person","name":"Jonathan Katz"},"copyrightHolder":{"@type":"Person","name":"Jonathan Katz"},"copyrightYear":"2023","dateCreated":"2023-08-28T00:00:00.00Z","datePublished":"2023-08-28T00:00:00.00Z","dateModified":"2023-08-28T00:00:00.00Z","publisher":{"@type":"Organization","name":"Jonathan Katz","url":"https://jkatz.github.io/","logo":{"@type":"ImageObject","url":"https:\/\/jkatz.github.io\/favicons\/favicon-32x32.png","width":"32","height":"32"}},"image":[],"url":"https:\/\/jkatz.github.io\/post\/postgres\/pgvector-overview-0.5.0\/","wordCount":"2382","genre":[],"keywords":["postgres","postgresql","pgvector"]}</script></head><body><header><div class="page-top
animated fadeInDown"><a role=button class=navbar-burger data-target=navMenu aria-label=menu aria-expanded=false><span aria-hidden=true></span>
<span aria-hidden=true></span>
<span aria-hidden=true></span></a><nav><ul class=nav__list id=navMenu><div class=nav__links><li><a href=/ title>Home</a></li><li><a href=/post/ title>Posts</a></li><li><a href=/talks/ title>Talks</a></li><li><a href=/about/ title>About</a></li></div><ul><li><a class=theme-switch title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></li></ul></ul></nav></div></header><div class=wrapper><aside><div class="sidebar
animated fadeInDown"><div class=sidebar__content><div class=logo-title><div class=title><img src=/images/profile.png alt="profile picture"><h3 title><a href=/>Jonathan Katz</a></h3><div class=description><p></p></div></div></div><ul class=social-links><li><a href=https://www.twitter.com/jkatz05 rel=me aria-label=Twitter title=Twitter><i class="fab fa-twitter fa-2x" aria-hidden=true></i></a></li><li><a href=https://www.linkedin.com/in/jonathan-katz-6495532/ rel=me aria-label=Linkedin title=Linkedin><i class="fab fa-linkedin fa-2x" aria-hidden=true></i></a></li><li><a href=https://github.com/jkatz/ rel=me aria-label=GitHub title=GitHub><i class="fab fa-github fa-2x" aria-hidden=true></i></a></li><li><a href=mailto:jkatz05@jkatz05.com rel=me aria-label=e-mail title=e-mail><i class="fas fa-envelope fa-2x" aria-hidden=true></i></a></li></ul></div><footer class="footer footer--sidebar"><div class=by_farbox><ul class=footer__list><li class=footer__item>&copy;
2024 Jonathan Katz</li><li class=footer__item><a href=https://github.com/lxndrblz/anatole target=_blank rel="noopener noreferrer" title>Powered by the Anatole Hugo Theme</a></li><li class=footer__item><a href title>Opinions are my own.</a></li></ul></div></footer><script type=text/javascript src=/js/medium-zoom.min.44288fd315b6cda68c1f4743caad56535c0f81a5b5a672f385e82b3896575c1d.js integrity="sha256-RCiP0xW2zaaMH0dDyq1WU1wPgaW1pnLzhegrOJZXXB0=" crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NNLWC1035Y"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NNLWC1035Y")</script></div></aside><main><div class=autopagerize_page_element><div class=content><div class="post
animated fadeInDown"><div class=post-content><div class=post-title><h1>pgvector 0.5.0 Feature Highlights and HOWTOs</h1><div class=info><em class="fas fa-calendar-day"></em>
<span class=date>Mon, Aug 28, 2023
</span><em class="fas fa-stopwatch"></em>
<span class=reading-time>12-minute read</span></div></div><p>It&rsquo;s here! <a href=https://github.com/pgvector/pgvector/releases/tag/v0.5.0>pgvector 0.5.0</a> is released and has some incredible new features. <a href=https://github.com/pgvector/pgvector>pgvector</a> is an open-source project that brings <a href=/post/postgres/vectors-json-postgresql/>vector database capabilities to PostgreSQL</a>. The pgvector community is moving very rapidly on adding new features, so I thought it prudent to put together some highlights of the 0.5.0 release.</p><p>Here&rsquo;s a quick list of highlights, though I encourage you read the rest in depth and explore on your own!</p><ul><li><a href=/post/postgres/pgvector-overview-0.5.0/#new-index-type-hierarchical-navigable-small-worlds-hnsw>HNSW indexing support</a></li><li><a href=/post/postgres/pgvector-overview-0.5.0/#improved-performance-of-distance-functions>Faster distance calculations</a></li><li><a href=/post/postgres/pgvector-overview-0.5.0/#parallelization-of-ivfflat-index-builds>Parallel builds for <code>ivfflat</code></a></li></ul><p>This is a big release, so let&rsquo;s dive right in.</p><h2 id=new-index-type-hierarchical-navigable-small-worlds-hnsw>New index type: Hierarchical Navigable Small Worlds (<code>hnsw</code>)</h2><p>The biggest highlight of the pgvector 0.5.0 release is the introduction of the <code>hnsw</code> index type. <code>hnsw</code> is based on the <a href=https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf>hierarchical navigable small worlds</a> paper, which describes an indexing technique that creates layers of increasingly dense &ldquo;neighborhoods&rdquo; of vectors. The main idea of HNSW is that you can achieve a better performance/<a href=https://en.wikipedia.org/wiki/Precision_and_recall>recall</a> ratio by connecting vectors that are close to each other, so that when you perform similarity search, you have a higher likelihood of finding the exact nearest neighbors you&rsquo;re looking for.</p><p>Beyond performance, pgvector&rsquo;s HNSW implementation has several notable features:</p><ul><li><strong>&ldquo;Build as you go&rdquo;</strong>: With HNSW, you can create an index on an empty table and add vectors as you go without impacting recall! This is different from <code>ivfflat</code>, where you first need to load your vectors before building the index to find optimal centers for better recall. As you add more data to your <code>ivfflat</code> index, you may also need to re-index to find updated centers.</li><li><strong>Update and delete</strong>: pgvector&rsquo;s HNSW implementation lets you update and delete vectors from the index, as part of standard <code>UPDATE</code> and <code>DELETE</code> queries. Many HNSW implementations do not support this feature.</li><li><strong>Concurrent inserts</strong>: Additionally, pgvector&rsquo;s HNSW implementations lets you concurrently insert values into the index, making it easier to simultaneously load data from multiple sources.</li></ul><p>In a previous post, I went into depth on the <a href=/post/postgres/pgvector-hnsw-performance/>HNSW performance for pgvector</a> with benchmarks that compared it to <code>ivfflat</code> and <code>pg_embedding</code>&rsquo;s HNSW implementation. The chart below shows the performance/recall tradeoffs on OpenAI-style embedding data using cosine distance (for <a href=/post/postgres/pgvector-hnsw-performance/>full testing methodology</a> and the <a href=https://github.com/erikbern/ann-benchmarks>ANN Benchmark framework</a>. Please read the <a href=/post/postgres/pgvector-hnsw-performance/>previous post</a>) for more details on the test methodology and other considerations (index build time, index size on disk):</p><p><img alt=dbpedia-openai-1000k-angular.png src=/images/dbpedia-openai-1000k-angular.png></p><p>Instead of focusing on benchmarking in this post, I want to provide guidance on how to use the key parameters in <code>hnsw</code> so you understand the ramifications on your performance/recall ratio and index build times.</p><p>In the <a href=/post/postgres/pgvector-hnsw-performance/>previous post</a>, we reviewed the 3 parameters that are part of the HNSW algorithm. We&rsquo;ll break them down to where they are applicable in pgvector&rsquo;s <code>hnsw</code> implementation:</p><h3 id=index-building-options>Index building options</h3><p>These options are available in the <code>WITH</code> clause of <code>CREATE INDEX</code>, e.g.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=cl><span class=k>CREATE</span><span class=w> </span><span class=k>TABLE</span><span class=w> </span><span class=n>vecs</span><span class=w> </span><span class=p>(</span><span class=n>id</span><span class=w> </span><span class=nb>int</span><span class=w> </span><span class=k>PRIMARY</span><span class=w> </span><span class=k>KEY</span><span class=p>,</span><span class=w> </span><span class=n>embedding</span><span class=w> </span><span class=n>vector</span><span class=p>(</span><span class=mi>768</span><span class=p>));</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>CREATE</span><span class=w> </span><span class=k>INDEX</span><span class=w> </span><span class=k>ON</span><span class=w> </span><span class=n>vecs</span><span class=w> </span><span class=k>USING</span><span class=w> </span><span class=n>hnsw</span><span class=p>(</span><span class=n>embedding</span><span class=w> </span><span class=n>vector_l2_ops</span><span class=p>)</span><span class=w> </span><span class=k>WITH</span><span class=w> </span><span class=p>(</span><span class=n>m</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span><span class=w> </span><span class=n>ef_construction</span><span class=o>=</span><span class=mi>64</span><span class=p>);</span><span class=w>
</span></span></span></code></pre></div><ul><li><code>m</code>: (default: <code>16</code>; range: <code>2</code> to <code>100</code>) Indicates how many bidirectional links (or paths) exist between each indexed element. Setting this to a higher number can increase recall, but can also significantly increase index build time and may impact query performance.</li><li><code>ef_construction</code>: (default: <code>64</code>; range: <code>4</code> to <code>1000</code>) Indicates how many nearest neighbors to check while adding an element to the index. Increasing this value can increase recall, but will also increase index build time. This value must also be at least double <code>m</code>, e.g. if <code>m</code> is <code>24</code> then <code>ef_construction</code> must be at least <code>48</code>.</li></ul><p>Note that as of this writing, you must specify the operator class to use with the <code>hnsw</code> index. For example, to use cosine distance with an HNSW index, you would use a command similar to the below:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=cl><span class=k>CREATE</span><span class=w> </span><span class=k>INDEX</span><span class=w> </span><span class=k>ON</span><span class=w> </span><span class=n>vecs</span><span class=w> </span><span class=k>USING</span><span class=w> </span><span class=n>hnsw</span><span class=p>(</span><span class=n>embedding</span><span class=w> </span><span class=n>vector_cosine_ops</span><span class=p>);</span><span class=w>
</span></span></span></code></pre></div><h3 id=index-search-options>Index search options</h3><ul><li><code>hnsw.ef_search</code>: (default: <code>40</code>; range <code>1</code> to <code>1000</code>) Indicates the number of nearest neighbors to keep in a &ldquo;dynamic list&rdquo; while keeping the search. A large value can improve recall, usually with a tradeoff in performance. You need <code>hnsw.ef_search</code> be at least as big as your <code>LIMIT</code> value.</li></ul><h3 id=how-to-use-hnsw-in-pgvector>How to use <code>hnsw</code> in pgvector</h3><p>The default <a href=https://github.com/pgvector/pgvector/pull/230>index build settings</a> are chosen to optimize build time relative to the recall you can achieve during search. If you&rsquo;re not getting the recall that you expect for your data set, first try increasing the value of <code>ef_construction</code> before adjusting <code>m</code>, as adjusting <code>ef_construction</code> is often a faster operation. There are some studies that show that increasing <code>m</code> can help with recall for higher dimensionality data sets, though in the <a href=/post/postgres/pgvector-hnsw-performance/>previous post</a> we saw that <a href=/post/postgres/pgvector-hnsw-performance/>pgvector could process OpenAI-style embeddings effectively with high recall</a>.</p><p>You can increase performance of your search queries by lowing the value <code>hnsw.ef_search</code>, e.g. set <code>hnsw.ef_search</code> to <code>20</code>, though note that this could impact your recall:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=cl><span class=k>SET</span><span class=w> </span><span class=n>hnsw</span><span class=p>.</span><span class=n>ef_search</span><span class=w> </span><span class=k>TO</span><span class=w> </span><span class=mi>20</span><span class=p>;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>SELECT</span><span class=w> </span><span class=o>*</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>FROM</span><span class=w> </span><span class=n>vecs</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>ORDER</span><span class=w> </span><span class=k>BY</span><span class=w> </span><span class=n>q</span><span class=w> </span><span class=o>&lt;-&gt;</span><span class=w> </span><span class=n>embedding</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>LIMIT</span><span class=w> </span><span class=mi>10</span><span class=p>;</span><span class=w>
</span></span></span></code></pre></div><h2 id=improved-performance-of-distance-functions>Improved performance of distance functions</h2><p>Speed is paramount when computing distance between two vectors. Any way you can shave off computation time means you can build indexes and search for vectors more quickly.</p><p>pgvector 0.5.0 does exactly this, improving <a href=https://github.com/pgvector/pgvector/pull/180>distance calculations across the board</a> with <a href=https://github.com/pgvector/pgvector/pull/180#issuecomment-1640936140>noticeable gains for ARM64 architecture</a>. By default, pgvector can use CPU acceleration for vector processing through compile flags, and writing the implementation code in certain ways can help unlock performance gains once its compiled.</p><p>The gains in pgvector 0.5.0 are noticeable. To demonstrate this, I ran an experiment on my Mac M1 Pro (2021 edition, 8 CPI, 16 GB RAM) to show the speedup in building an <code>ivfflat</code> index with both Euclidean (<code>vector_l2_ops</code>, or the default operator class) and cosine distnace (<code>vector_cosine_ops</code>) on a table with 1,000,000 768-dimensional vectors using <code>PLAIN</code> storage:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=cl><span class=k>CREATE</span><span class=w> </span><span class=k>TABLE</span><span class=w> </span><span class=n>vecs</span><span class=w> </span><span class=p>(</span><span class=n>id</span><span class=w> </span><span class=nb>int</span><span class=w> </span><span class=k>PRIMARY</span><span class=w> </span><span class=k>KEY</span><span class=p>,</span><span class=w> </span><span class=n>embedding</span><span class=w> </span><span class=n>vector</span><span class=p>(</span><span class=mi>768</span><span class=p>));</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>ALTER</span><span class=w> </span><span class=k>TABLE</span><span class=w> </span><span class=n>vecs</span><span class=w> </span><span class=k>ALTER</span><span class=w> </span><span class=k>COLUMN</span><span class=w> </span><span class=n>embedding</span><span class=w> </span><span class=k>SET</span><span class=w> </span><span class=k>STORAGE</span><span class=w> </span><span class=n>PLAIN</span><span class=p>;</span><span class=w>
</span></span></span></code></pre></div><p>Below are some of the relevant local settings used to test the index builds:</p><ul><li><code>shared_buffers</code>: <code>4GB</code></li><li><code>max_wal_size</code>: <code>10GB</code></li><li><code>work_mem</code>: <code>8MB</code></li><li><code>max_parallel_mainetance_workers</code>: <code>0</code></li></ul><p>Before each run, I ensured that the <code>vecs</code> table was loaded into memory using the <a href=https://www.postgresql.org/docs/current/pgprewarm.html><code>pg_prewarm</code></a> extension:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=cl><span class=k>SELECT</span><span class=w> </span><span class=n>pg_prewarm</span><span class=p>(</span><span class=s1>&#39;vecs&#39;</span><span class=p>);</span><span class=w>
</span></span></span></code></pre></div><p>Finally, I created the <code>ivfflat</code> index with <code>lists</code> set to <code>100</code>. Note that this was to run a series of tests quickly; the recommended value of <code>lists</code> for 1,000,000 rows is <code>1000</code>. The effect of the distance calculations may be more pronounced with a larger value of <code>lists</code>.</p><p>Below is a table summarizing the results. Please note that these results are directional, particularly due to the value of <code>lists</code>:</p><table><thead><tr><th>Version</th><th>Euclidean (s)</th><th>cosine (s)</th></tr></thead><tbody><tr><td>0.4.4</td><td>71.66</td><td>69.92</td></tr><tr><td>0.5.0</td><td>45.65</td><td>64.02</td></tr></tbody></table><p>The above test showed a noticeable improvement with Euclidean distance and a marginal improvement with cosine distance. Andrew Kane&rsquo;s tests showed a <a href=https://gist.github.com/ankane/96e9750405f0b89974edd7db01168f5d>greater speedup across all distance functions</a> on ARM64 systems. That said, you should likely see some performance gains in your pgvector workloads, with these being most pronounced on tasks with many distance computations (e.g. index builds, searches over large sets of vectors).</p><h2 id=parallelization-of-ivfflat-index-builds>Parallelization of <code>ivfflat</code> index builds</h2><p><code>ivfflat</code> is comparatively fast when it comes to building indexes, though there are always ways to improve performance. One area is adding parallelization to simultaneously perform operations. To understand how parallelism can benefit <code>ivfflat</code>, first let&rsquo;s explore the different phases of the index build. Recall that we can build an <code>ivfflat</code> index on a table using a query like the one below:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=cl><span class=k>CREATE</span><span class=w> </span><span class=k>INDEX</span><span class=w> </span><span class=k>ON</span><span class=w> </span><span class=n>vecs</span><span class=w> </span><span class=k>USING</span><span class=w> </span><span class=n>ivfflat</span><span class=p>(</span><span class=n>embedding</span><span class=p>)</span><span class=w> </span><span class=k>WITH</span><span class=w> </span><span class=p>(</span><span class=n>lists</span><span class=o>=</span><span class=mi>100</span><span class=p>);</span><span class=w>
</span></span></span></code></pre></div><p>Now, let&rsquo;s look at the different build phases:</p><ul><li><strong>k-means</strong>: pgvector samples a subset of all of the vectors in the table to determine <code>k</code> centers, where <code>k</code> is the value of <code>lists</code>. Using the above query, the value of <code>k</code> is <code>100</code>.</li><li><strong>assignment</strong>: pgvector then goes through every record in the table and assigns it to its closest lists, where the distance is calculated using the selected distance operator (e.g. Euclidean).</li><li><strong>sort</strong>: pgvector then sorts the records in each list</li><li><strong>write-to-disk</strong>: Finally, pgvector writes the index to disk.</li></ul><p>There are two areas here that can benefit from parallelization:</p><ul><li><strong>k-means</strong>: K-means is fairly computational heavy, but it&rsquo;s also easily parallelizable.</li><li><strong>assignment</strong>: During the assignment phase, pgvector must load every record from the table, which can take a long time if the table is very large.</li></ul><p>Analysis (to be shown further down) around where pgvector was spending the most time during the index building process showed that pgvector was spending the most time in the <strong>assignment</strong> phase, particularly as the size of the indexable dataset grew. While the time spent in <strong>k-means</strong> grew quadratically with the number of lists, it was still only a fraction of the time spent compared to <strong>assignment</strong>. Interestingly, <strong>write-to-disk</strong> stayed relative constant as a percentage of time across the tests.</p><p>pgvector 0.5.0 added parallelization around the <strong>assignment</strong> phase, specifically, spawning multiple parallel maintenance workers to scan the table and assign records to the closest list. There are a few parameters you need to be aware of when using this feature:</p><ul><li><code>max_parallel_maintenance_workers</code>: This is the max number of parallel workers PostgreSQL will spawn for a mainetnance operation, such as an index build. This defaults to <code>2</code>, so you may need to set it higher to get the full benefit of this feature.</li><li><code>max_parallel_workers</code>: This is the max number of parallel workers PostgreSQL spawns for parallel operations. This defaults to <code>8</code> in PostgreSQL, so you may need to increase it along with <code>max_parallel_maintenance_workers</code> based upon how much parallelism you need.</li><li><code>min_parallel_table_scan_size</code>: This parameter determines how much data must be scanned to determine when and how many parallel workers to spawn. If you&rsquo;re using <code>EXTENDED</code> / TOAST storage (the default) for your vectors, you may not be getting a parallel scan because <a href=https://www.postgresql.org/message-id/flat/ad8a178f-bbe7-d89d-b407-2f0fede93144%40postgresql.org>PostgreSQL will only consider the amount of space used in the main table, not the TOAST table</a>. Setting this to a lower value (e.g. <code>SET min_parallel_table_scan_size TO 1</code>) may help you spawn more parallel workers.</li></ul><p>Below is an example of the impact of parallelization on a 1,000,000 set of 768-dimensional vectors (note, to see parallel workers spawned in your session, you&rsquo;ll have to run <code>SET client_min_messages TO 'DEBUG1';</code>):</p><p>Serial build:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=cl><span class=k>CREATE</span><span class=w> </span><span class=k>INDEX</span><span class=w> </span><span class=k>ON</span><span class=w> </span><span class=n>vecs</span><span class=w> </span><span class=k>USING</span><span class=w> </span><span class=n>ivfflat</span><span class=p>(</span><span class=n>embedding</span><span class=p>)</span><span class=w> </span><span class=k>WITH</span><span class=w> </span><span class=p>(</span><span class=n>lists</span><span class=o>=</span><span class=mi>500</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>DEBUG</span><span class=p>:</span><span class=w>  </span><span class=n>building</span><span class=w> </span><span class=k>index</span><span class=w> </span><span class=s2>&#34;vecs_embedding_idx&#34;</span><span class=w> </span><span class=k>on</span><span class=w> </span><span class=k>table</span><span class=w> </span><span class=s2>&#34;vecs&#34;</span><span class=w> </span><span class=n>serially</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>CREATE</span><span class=w> </span><span class=k>INDEX</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>Time</span><span class=p>:</span><span class=w> </span><span class=mi>112836</span><span class=p>.</span><span class=mi>829</span><span class=w> </span><span class=n>ms</span><span class=w> </span><span class=p>(</span><span class=mi>01</span><span class=p>:</span><span class=mi>52</span><span class=p>.</span><span class=mi>837</span><span class=p>)</span><span class=w>
</span></span></span></code></pre></div><p>Parallel build with <code>2</code> workers (including the leader, so really &ldquo;3&rdquo; workers :):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=cl><span class=k>SET</span><span class=w> </span><span class=n>max_parallel_maintenance_workers</span><span class=w> </span><span class=k>TO</span><span class=w> </span><span class=mi>2</span><span class=p>;</span><span class=w>  
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>CREATE</span><span class=w> </span><span class=k>INDEX</span><span class=w> </span><span class=k>ON</span><span class=w> </span><span class=n>vecs</span><span class=w> </span><span class=k>USING</span><span class=w> </span><span class=n>ivfflat</span><span class=p>(</span><span class=n>embedding</span><span class=p>)</span><span class=w> </span><span class=k>WITH</span><span class=w> </span><span class=p>(</span><span class=n>lists</span><span class=o>=</span><span class=mi>500</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>DEBUG</span><span class=p>:</span><span class=w>  </span><span class=k>using</span><span class=w> </span><span class=mi>2</span><span class=w> </span><span class=n>parallel</span><span class=w> </span><span class=n>workers</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>DEBUG</span><span class=p>:</span><span class=w>  </span><span class=n>worker</span><span class=w> </span><span class=n>processed</span><span class=w> </span><span class=mi>331820</span><span class=w> </span><span class=n>tuples</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>DEBUG</span><span class=p>:</span><span class=w>  </span><span class=n>worker</span><span class=w> </span><span class=n>processed</span><span class=w> </span><span class=mi>331764</span><span class=w> </span><span class=n>tuples</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>DEBUG</span><span class=p>:</span><span class=w>  </span><span class=n>leader</span><span class=w> </span><span class=n>processed</span><span class=w> </span><span class=mi>336416</span><span class=w> </span><span class=n>tuples</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>CREATE</span><span class=w> </span><span class=k>INDEX</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>Time</span><span class=p>:</span><span class=w> </span><span class=mi>61849</span><span class=p>.</span><span class=mi>137</span><span class=w> </span><span class=n>ms</span><span class=w> </span><span class=p>(</span><span class=mi>01</span><span class=p>:</span><span class=mi>01</span><span class=p>.</span><span class=mi>849</span><span class=p>)</span><span class=w>
</span></span></span></code></pre></div><p>We can see that there is almost a 2x speedup with parallel builds. But how much is too much? Let&rsquo;s see what happens when we allow for PostgreSQL to use more parallel workers on this data set:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=cl><span class=k>SET</span><span class=w> </span><span class=n>max_parallel_maintenance_workers</span><span class=w> </span><span class=k>TO</span><span class=w> </span><span class=mi>8</span><span class=p>;</span><span class=w>  
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>CREATE</span><span class=w> </span><span class=k>INDEX</span><span class=w> </span><span class=k>ON</span><span class=w> </span><span class=n>vecs</span><span class=w> </span><span class=k>USING</span><span class=w> </span><span class=n>ivfflat</span><span class=p>(</span><span class=n>embedding</span><span class=p>)</span><span class=w> </span><span class=k>WITH</span><span class=w> </span><span class=p>(</span><span class=n>lists</span><span class=o>=</span><span class=mi>500</span><span class=p>);</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>DEBUG</span><span class=p>:</span><span class=w>  </span><span class=k>using</span><span class=w> </span><span class=mi>6</span><span class=w> </span><span class=n>parallel</span><span class=w> </span><span class=n>workers</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>DEBUG</span><span class=p>:</span><span class=w>  </span><span class=n>worker</span><span class=w> </span><span class=n>processed</span><span class=w> </span><span class=mi>142740</span><span class=w> </span><span class=n>tuples</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>DEBUG</span><span class=p>:</span><span class=w>  </span><span class=n>worker</span><span class=w> </span><span class=n>processed</span><span class=w> </span><span class=mi>142394</span><span class=w> </span><span class=n>tuples</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>DEBUG</span><span class=p>:</span><span class=w>  </span><span class=n>worker</span><span class=w> </span><span class=n>processed</span><span class=w> </span><span class=mi>142192</span><span class=w> </span><span class=n>tuples</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>DEBUG</span><span class=p>:</span><span class=w>  </span><span class=n>worker</span><span class=w> </span><span class=n>processed</span><span class=w> </span><span class=mi>142316</span><span class=w> </span><span class=n>tuples</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>DEBUG</span><span class=p>:</span><span class=w>  </span><span class=n>worker</span><span class=w> </span><span class=n>processed</span><span class=w> </span><span class=mi>142674</span><span class=w> </span><span class=n>tuples</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>DEBUG</span><span class=p>:</span><span class=w>  </span><span class=n>leader</span><span class=w> </span><span class=n>processed</span><span class=w> </span><span class=mi>142284</span><span class=w> </span><span class=n>tuples</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>DEBUG</span><span class=p>:</span><span class=w>  </span><span class=n>worker</span><span class=w> </span><span class=n>processed</span><span class=w> </span><span class=mi>145400</span><span class=w> </span><span class=n>tuples</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>CREATE</span><span class=w> </span><span class=k>INDEX</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=n>Time</span><span class=p>:</span><span class=w> </span><span class=mi>67140</span><span class=p>.</span><span class=mi>314</span><span class=w> </span><span class=n>ms</span><span class=w> </span><span class=p>(</span><span class=mi>01</span><span class=p>:</span><span class=mi>07</span><span class=p>.</span><span class=mi>140</span><span class=p>)</span><span class=w>
</span></span></span></code></pre></div><p>We can see that for this data set, we were able to achieve max performance with only <code>2</code> workers, and the leader.</p><p>Where does this feature shave off time? Let&rsquo;s look at a comparison between a few different runs and times between the different phases:</p><h3 id=1000000-768-dim-vectors-lists1000>1,000,000 768-dim vectors, lists=1000</h3><p>Time measured in <strong>seconds</strong>.</p><table><thead><tr><th>Build</th><th>Total</th><th>k-means</th><th>assignment</th><th>sort</th><th>write-to-disk</th></tr></thead><tbody><tr><td>Serial</td><td>185</td><td>32</td><td>130</td><td>0.04</td><td>23</td></tr><tr><td>Parallel</td><td>80</td><td>34</td><td>21</td><td>1.3</td><td>24</td></tr><tr><td>Speedup</td><td>2.3x</td><td></td><td>6.1x</td><td></td><td></td></tr></tbody></table><p>In the above experiment, we see that we get a 2x+ overall speedup, with a 6x improvement in the assignment phase, though note that the sort has a significant slowdown (albeit it&rsquo;s not noticeable). Let&rsquo;s add more vectors into the mix.</p><h3 id=5000000-768-dim-vectors-lists2000>5,000,000 768-dim vectors, lists=2000</h3><p>Time measured in <strong>seconds</strong>.</p><table><thead><tr><th>Build</th><th>Total</th><th>k-means</th><th>assignment</th><th>sort</th><th>write-to-disk</th></tr></thead><tbody><tr><td>Serial</td><td>1724</td><td>168</td><td>1430</td><td>0.04</td><td>126</td></tr><tr><td>Parallel</td><td>551</td><td>173</td><td>253</td><td>4</td><td>120</td></tr><tr><td>Speedup</td><td>3.1x</td><td></td><td>5.64x</td><td></td><td></td></tr></tbody></table><p>Again, in this experiment we see that we save the most time in the assignment phase, though note that the sort time increases roughly linearly as we add more vectors into the build.</p><p>Finally, let&rsquo;s look at one very large example!</p><h3 id=100000000-384-dim-vectors-lists1000>100,000,000 384-dim vectors, lists=1000</h3><p>A few things to note about this test:</p><ul><li>First, you&rsquo;d normally use a larger value of lists to drive a better performance/recall ratio across your data set.</li><li>I mainly experimented with different values of <code>max_parallel_maintenance_workers</code>, and for this data set, the optimal value was around <code>32</code>.</li><li>Finally, I stopped the tests before the <strong>write-to-disk</strong> phase completed, but everything was trending much faster than the 23 hours it took to build this serially!</li></ul><p>Time is in <strong>hours</strong>. All builds were in parallel:</p><table><thead><tr><th>Workers</th><th>Total</th><th>k-means</th><th>assignment</th><th>sort</th></tr></thead><tbody><tr><td>10</td><td>17.9</td><td>0.36</td><td>17.5</td><td>0.08</td></tr><tr><td>16</td><td>11.3</td><td>0.38</td><td>10.6</td><td>0.34</td></tr><tr><td>32</td><td>10.6</td><td>0.37</td><td>9.3</td><td>1.01</td></tr><tr><td>48</td><td>11.7</td><td>0.36</td><td>10.1</td><td>1.26</td></tr></tbody></table><p>Overall, if you have larger data sets, you should see improvements in <code>ivfflat</code> build times, particularly for larger data sets.</p><h2 id=other-features>Other features</h2><p>pgvector 0.5.0 is a pretty big release, so don&rsquo;t miss out on these other items:</p><ul><li><code>SUM</code> aggregates: You can now sum up all your vectors, e.g. <code>SELECT sum(embedding) FROM vecs;</code>.</li><li>Manhattan / Taxicab / L1 distance: This release adds the <code>l1_distance</code> function so you can find the <a href=https://en.wikipedia.org/wiki/Taxicab_geometry>Manhattan distance</a> between two vectors (which I <a href=/about/>personally find very helpful</a>).</li><li>Element-wise multiplication: You can now multiple two vectors by each other, e.g. <code>SELECT '[1,2,3]'::vector(3) * '[4,5,6]'::vector(3)</code>.</li></ul><h2 id=whats-next>What&rsquo;s next?</h2><p>Ready to try out or upgrade to pgvector 0.5.0? If you&rsquo;re already using pgvector, once you&rsquo;ve installed the new version into your database, you can upgrade to 0.5.0 using the following command:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=cl><span class=k>ALTER</span><span class=w> </span><span class=n>EXTENSION</span><span class=w> </span><span class=n>vector</span><span class=w> </span><span class=k>UPDATE</span><span class=p>;</span><span class=w>
</span></span></span></code></pre></div><p>or explicitly via:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=cl><span class=k>ALTER</span><span class=w> </span><span class=n>EXTENSION</span><span class=w> </span><span class=n>vector</span><span class=w> </span><span class=k>UPDATE</span><span class=w> </span><span class=k>TO</span><span class=w> </span><span class=s1>&#39;0.5.0&#39;</span><span class=p>;</span><span class=w>
</span></span></span></code></pre></div><p>There&rsquo;s still <a href=https://github.com/pgvector/pgvector/issues/27>more to come</a> in pgvector. The 0.5.0 release will help support more vector-driven workloads on PostgreSQL, and is a great setup for future releases that will further expand the capabilities of PostgreSQL as a vector database.</p><p>And last but not least, but a huge THANK YOU to everyone who&rsquo;s worked on pgvector to date, whether its coding, testing, providing feedback, and contributing ideas. A very special thanks to <a href=https://github.com/ankane>Andrew Kane</a> for his tireless work on the 0.5.0 release, and also many thanks to <a href=https://github.com/pashkinelfe>Pavel Borisov</a> who did the work unlocked the performance gains ont he distance operations.</p><p>As awesome as the 0.5.0 release of pgvector is, the best has yet to come. <a href=https://github.com/pgvector/pgvector>Stay tuned</a>!</p></div><div class=post-footer><div class=info><span class=separator><a class=tag href=/tags/postgres/>postgres</a><a class=tag href=/tags/postgresql/>postgresql</a><a class=tag href=/tags/pgvector/>pgvector</a></span></div></div></div></div></div></main></div><footer class="footer footer--base"><div class=by_farbox><ul class=footer__list><li class=footer__item>&copy;
2024 Jonathan Katz</li><li class=footer__item><a href=https://github.com/lxndrblz/anatole target=_blank rel="noopener noreferrer" title>Powered by the Anatole Hugo Theme</a></li><li class=footer__item><a href title>Opinions are my own.</a></li></ul></div></footer><script type=text/javascript src=/js/medium-zoom.min.44288fd315b6cda68c1f4743caad56535c0f81a5b5a672f385e82b3896575c1d.js integrity="sha256-RCiP0xW2zaaMH0dDyq1WU1wPgaW1pnLzhegrOJZXXB0=" crossorigin=anonymous></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NNLWC1035Y"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NNLWC1035Y")</script></body></html>