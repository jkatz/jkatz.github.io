<!doctype html><html dir=ltr lang=en data-theme><head><title>The 150x pgvector speedup: a year-in-review |
Jonathan Katz</title><meta charset=utf-8><meta name=generator content="Hugo 0.104.3"><meta name=viewport content="width=device-width,initial-scale=1,viewport-fit=cover"><meta name=description content="
      Reviewing a year of work to make pgvector much faster while supporting high-relevancy searches!


    "><link rel=stylesheet href=/css/main.min.861c630407a306e1a79890bffbab9b381c5a0b03cd0c3b79755cc5eeea04b6b0.css integrity="sha256-hhxjBAejBuGnmJC/+6ubOBxaCwPNDDt5dVzF7uoEtrA=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/css/markupHighlight.min.058b31f17db60602cc415fd63b0427e7932fbf35c70d8e341a4c39385f5f6f3e.css integrity="sha256-BYsx8X22BgLMQV/WOwQn55MvvzXHDY40Gkw5OF9fbz4=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/css/custom.min.3aab74359778ed24c5dcf7b0d36e04e4b55bbcb26e7d90240f41f946a075c1a5.css integrity="sha256-Oqt0NZd47STF3Pew024E5LVbvLJufZAkD0H5RqB1waU=" crossorigin=anonymous media=screen><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css integrity="sha512-+4zCK9k+qNFUR5X+cKL9EIR+ZOhtIloNl9GIKS57V1MyNsYpYcUrUeQc9vNfzsWfV28IaLL3i96P9sdNyeRssA==" crossorigin=anonymous><link rel="shortcut icon" href=/favicons/favicon.ico type=image/x-icon><link rel=apple-touch-icon sizes=180x180 href=/favicons/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicons/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicons/favicon-16x16.png><link rel=canonical href=https://jkatz.github.io/post/postgres/pgvector-performance-150x-speedup/><script type=text/javascript src=/js/anatole-header.min.d0408165d31a17f17bba83038bf54e86121f85021bdf936382e636f0f77a952f.js integrity="sha256-0ECBZdMaF/F7uoMDi/VOhhIfhQIb35NjguY28Pd6lS8=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/anatole-theme-switcher.min.ea8ebe268922ef9849261a1312cd65b640595e65251ce4c00534a176afd1ac0c.js integrity="sha256-6o6+Joki75hJJhoTEs1ltkBZXmUlHOTABTShdq/RrAw=" crossorigin=anonymous></script><meta name=twitter:card content="summary"><meta name=twitter:title content="The 150x pgvector speedup: a year-in-review"><meta name=twitter:description content="Reviewing a year of work to make pgvector much faster while supporting high-relevancy searches!"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"post","name":"The 150x pgvector speedup: a year-in-review","headline":"The 150x pgvector speedup: a year-in-review","alternativeHeadline":"","description":"
      Reviewing a year of work to make pgvector much faster while supporting high-relevancy searches!


    ","inLanguage":"en","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/jkatz.github.io\/post\/postgres\/pgvector-performance-150x-speedup\/"},"author":{"@type":"Person","name":"Jonathan Katz"},"creator":{"@type":"Person","name":"Jonathan Katz"},"accountablePerson":{"@type":"Person","name":"Jonathan Katz"},"copyrightHolder":{"@type":"Person","name":"Jonathan Katz"},"copyrightYear":"2024","dateCreated":"2024-04-30T00:00:00.00Z","datePublished":"2024-04-30T00:00:00.00Z","dateModified":"2024-04-30T00:00:00.00Z","publisher":{"@type":"Organization","name":"Jonathan Katz","url":"https://jkatz.github.io","logo":{"@type":"ImageObject","url":"https:\/\/jkatz.github.io\/favicons\/favicon-32x32.png","width":"32","height":"32"}},"image":[],"url":"https:\/\/jkatz.github.io\/post\/postgres\/pgvector-performance-150x-speedup\/","wordCount":"3520","genre":[],"keywords":["postgres","postgresql","pgvector"]}</script></head><body><header><div class="page-top
animated fadeInDown"><a role=button class=navbar-burger data-target=navMenu aria-label=menu aria-expanded=false><span aria-hidden=true></span>
<span aria-hidden=true></span>
<span aria-hidden=true></span></a><nav><ul class=nav__list id=navMenu><div class=nav__links><li><a href=/ title>Home</a></li><li><a href=/post/ title>Posts</a></li><li><a href=/talks/ title>Talks</a></li><li><a href=/about/ title>About</a></li></div><ul><li><a class=theme-switch title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></li></ul></ul></nav></div></header><div class=wrapper><aside><div class="sidebar
animated fadeInDown"><div class=sidebar__content><div class=logo-title><div class=title><img src=/images/profile.png alt="profile picture"><h3 title><a href=/>Jonathan Katz</a></h3><div class=description><p></p></div></div></div><ul class=social-links><li><a href=https://www.twitter.com/jkatz05 rel=me aria-label=Twitter title=Twitter><i class="fab fa-twitter fa-2x" aria-hidden=true></i></a></li><li><a href=https://www.linkedin.com/in/jonathan-katz-6495532/ rel=me aria-label=Linkedin title=Linkedin><i class="fab fa-linkedin fa-2x" aria-hidden=true></i></a></li><li><a href=https://github.com/jkatz/ rel=me aria-label=GitHub title=GitHub><i class="fab fa-github fa-2x" aria-hidden=true></i></a></li><li><a href=mailto:jkatz05@jkatz05.com rel=me aria-label=e-mail title=e-mail><i class="fas fa-envelope fa-2x" aria-hidden=true></i></a></li></ul></div><footer class="footer footer--sidebar"><div class=by_farbox><ul class=footer__list><li class=footer__item>&copy;
2024 Jonathan Katz</li><li class=footer__item><a href=https://github.com/lxndrblz/anatole target=_blank rel="noopener noreferrer" title>Powered by the Anatole Hugo Theme</a></li><li class=footer__item><a href title>Opinions are my own.</a></li></ul></div></footer><script type=text/javascript src=/js/medium-zoom.min.44288fd315b6cda68c1f4743caad56535c0f81a5b5a672f385e82b3896575c1d.js integrity="sha256-RCiP0xW2zaaMH0dDyq1WU1wPgaW1pnLzhegrOJZXXB0=" crossorigin=anonymous></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NNLWC1035Y"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NNLWC1035Y")</script></div></aside><main><div class=autopagerize_page_element><div class=content><div class="post
animated fadeInDown"><div class=post-content><div class=post-title><h1>The 150x pgvector speedup: a year-in-review</h1><div class=info><em class="fas fa-calendar-day"></em>
<span class=date>Tue, Apr 30, 2024</span>
<em class="fas fa-stopwatch"></em>
<span class=reading-time>17-minute read</span></div></div><p>I wanted to write a &ldquo;<a href=/post/postgres/vectors-json-postgresql/>year-in-review</a>&rdquo; covering all the performance <a href=https://github.com/pgvector/pgvector/>pgvector</a> has made (with significant credit to <a href=https://github.com/ankane>Andrew Kane</a>), highlighting specific areas where pgvector has improved (including one 150x improvement!) and areas where we can continue to do better.</p><p>A few weeks ago, I started outlining this post and began my data collection the data. While I was working on this over a two week period, no fewer than three competitive benchmarks against pgvector published. To me, this is a testament both how well pgvector is at handling vector workloads (and by extension, PostgreSQL too) that people are using it as the baseline to compare it to their vector search systems.</p><p>Some of these benchmarks did contain info that identified areas we can continue to improve both PostgreSQL and pgvector, but I was generally disappointed in the methodology used to make these comparisons. Of course I&rsquo;d like to see pgvector perform well in benchmarks, but it&rsquo;s important to position technologies fairly and be vocally self-critical on where your system can improve to build trust in what you&rsquo;re building.</p><p>I have a separate blog post planned for how to best present benchmark studies between different systems for vector similarity search (<a href=/post/postgres/pgvector-hnsw-performance/>it&rsquo;s a topic I&rsquo;m interested in</a>). Today though, I want to compare pgvector against itself, and highlight areas it&rsquo;s improved over the past year, and where the project can continue to go and grow.</p><h2 id=how-i-ran-these-tests>How I ran these tests</h2><p>An important aspect of any benchmark is transparency. First, I&rsquo;ll discuss the test methodology I used, describe the test environment setup (instances, storage, database configuration), and then discuss the results. If you&rsquo;re not interested in this part, you can skip ahead to &ldquo;The 150x pgvector speedup&rdquo;, but this information can help you with your own testing!</p><p>First, what are testing for? We&rsquo;ll be looking at these specific attributes in these tests:</p><ul><li><a href=https://en.wikipedia.org/wiki/Precision_and_recall><strong>Recall</strong></a>: A measurement of the relevancy of our results - what percentage of the expected results are returned during a vector search? Arguably, this is the most important measurement - it doesn&rsquo;t matter if you have the highest query throughput if your recall is poor.</li><li><strong>Storage size</strong>: This could be related to storing your original vector/associated data, and any data you store in a vector index. Because PostgreSQL is a database, at a minimum you&rsquo;ll have to store the vector in the table, and pay additional storage costs for building a vector index.</li><li><strong>Load time / index build time</strong>: How long does it take to load your vector data into an existing index? If your data is preloaded, how long does it take to build an index? Spending more time building your index can help improve both recall and query performance, but this is often the most expensive part of a vector database and can impact overall system performance.</li><li><strong>Latency (p99)</strong>: Specifically, how long it takes to return a single result, but representing the 99th percentile (&ldquo;very slow&rdquo;) queries. This serves as an &ldquo;upper bound&rdquo; on latency times.</li><li><strong>Single-connection Throughput / queries per second (QPS)</strong>: How many queries can be executed each second? This impacts how much load you can put on a single system.</li></ul><p>(More on the &ldquo;single-connection&rdquo; distinction in a future blog post).</p><p>This is a &ldquo;year-in-review&rdquo; post, so I ran tests against the following releases and configurations of pgvector. I&rsquo;m including the shorthand that I&rsquo;ll show in the tests results.</p><table><thead><tr><th>pgvector version</th><th>Index type</th><th>Test name (r7gd)</th><th>Test name (r7i)</th><th>Notes</th></tr></thead><tbody><tr><td>0.4.1</td><td>IVFFlat</td><td><code>r7gd.041</code></td><td><code>r7i.041</code></td><td></td></tr><tr><td>0.4.4</td><td>IVFFlat</td><td><code>r7gd.044</code></td><td><code>r7i.044</code></td><td></td></tr><tr><td>0.5.0</td><td>HNSW</td><td><code>r7gd.050</code></td><td><code>r7i.050</code></td><td></td></tr><tr><td>0.5.1</td><td>HNSW</td><td><code>r7gd.051</code></td><td><code>r7i.051</code></td><td></td></tr><tr><td>0.6.0</td><td>HNSW</td><td><code>r7gd.060</code></td><td><code>r7i.060</code></td><td></td></tr><tr><td>0.6.2</td><td>HNSW</td><td><code>r7gd.062</code></td><td><code>r7i.062</code></td><td></td></tr><tr><td>0.7.0</td><td>HNSW</td><td><code>r7gd.070</code></td><td><code>r7i.070</code></td><td></td></tr><tr><td>0.7.0</td><td>HNSW - SQ16</td><td><code>r7gd.070.fp16</code></td><td><code>r7i.070.fp16</code></td><td>Stores 2-byte float representation of vectors in the index</td></tr><tr><td>0.7.0</td><td>HNSW - BQ + Jaccard rerank</td><td><code>r7gd.070.bq-jaccard-rerank</code></td><td><code>r7i.070.bq-jaccard-rerank</code></td><td>Stores binary representation of vectors in index using Jaccard distance; results are re-ranked using original vector after the index search</td></tr><tr><td>0.7.0</td><td>HNSW - BQ + Hamming rerank</td><td><code>r7gd.070.bq-jaccard-rerank</code></td><td><code>r7i.070.bq-jaccard-rerank</code></td><td>Stores binary representation of vectors in index using Hamming distance; results are re-ranked using original vector after the index search</td></tr></tbody></table><h3 id=test-setup>Test setup</h3><p>To simplify the comparison, I kept the index build parameters the same for all of the tests. Adjusting build parameters can impact all five of the key metrics (please see <a href=/tags/pgvector/>previous posts</a> and <a href=/talks/>talks</a>), but the purpose of this blog post is to show how pgvector has evolved over the past year and choosing a fixed set of parameters does serve to show how it&rsquo;s improved and where it can grow. Below are the build parameters used for each index type:</p><table><thead><tr><th>Index type</th><th>Build parameters</th></tr></thead><tbody><tr><td>IVFFlat</td><td><code>lists</code>: <code>1000</code></td></tr><tr><td>HNSW</td><td><code>m</code>: <code>16</code>; <code>ef_construction</code>: <code>256</code></td></tr></tbody></table><p>For the testing, I used a <code>r7gd.16xlarge</code> and a <code>r7i.16xlarge</code>, both of which have 64 vCPU and 512GiB of RAM. I stored the data on the local NVMe on the <code>r7gd</code>, and on <code>gp3</code> storage for the <code>r7i</code>. If this test was looking at behaviors around storage, that fact would matter heavily, but these tests focused specifically on CPU and memory characteristics.</p><p>For these tests, I used PostgreSQL 16.2 (aside: the upcoming PostgreSQL 17 release is expected to have the ability to utilize AVX-512 SIMD instructions for the <code>pg_popcount</code> function, used by the Jaccard distance; this doesn&rsquo;t account for those optimizations) with the following configurations, using parallelism where available:</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>checkpoint_timeout</td><td>2h</td></tr><tr><td>effective_cache_size</td><td>256GB</td></tr><tr><td>jit</td><td>off</td></tr><tr><td>maintenance_work_mem</td><td>64GB</td></tr><tr><td>max_parallel_maintenance_workers</td><td>63</td></tr><tr><td>max_parallel_workers</td><td>64</td></tr><tr><td>max_parallel_workers_per_gather</td><td>64</td></tr><tr><td>max_wal_size</td><td>20GB</td></tr><tr><td>max_worker_processes</td><td>128</td></tr><tr><td>shared_buffers</td><td>128GB</td></tr><tr><td>wal_compression</td><td>zstd</td></tr><tr><td>work_mem</td><td>64MB</td></tr></tbody></table><p>I used the <a href=https://github.com/erikbern/ann-benchmarks>ANN Benchmark</a> framework to run the tests. I made the following modifications to the <code>pgvector</code> module:</p><ul><li>I commented out the <a href=https://github.com/erikbern/ann-benchmarks/blob/75043ab482d91afae82ac4033cbdd98997121d58/ann_benchmarks/algorithms/pgvector/module.py#L25><code>subprocess.run</code></a> line so I could run the test in a local process, not a container</li><li>I added modules to run both the new work <a href=/post/postgres/pgvector-scalar-binary-quantization/>scalar/binary quantization</a> code in v0.7.0 (<a href=/post/postgres/pgvector-scalar-binary-quantization/>see previous blog post for more details</a>). This followed the same format as the existing test.</li><li>Additionally, I revived the <a href=https://github.com/erikbern/ann-benchmarks/blob/fd389848e5706fdb0fbfc0e58bcb584e2ce7d4ee/ann_benchmarks/algorithms/pgvector/module.py>IVFFlat code</a> to run those tests. I did <a href=https://github.com/erikbern/ann-benchmarks/commit/c09127170291301c9da759cfe41294ade0d22652>include a recent commit to accelerate the loading of data into the table</a>.</li><li>I did make a small tweak to the timing of the index build. Instead of considering both the data load and the index build time, I only timed the index build. I&rsquo;m planning to propose this as a contribution to the upstream project, as the primary goal of the <code>fit</code> portion of ANN Benchmarks is to test the index build time.</li></ul><p>For each index type I used the following search parameters, which are the defaults for what&rsquo;s in the pgvector module for ANN Benchmarks:</p><table><thead><tr><th>Index type</th><th>Search parameters</th></tr></thead><tbody><tr><td>IVFFlat</td><td><code>ivfflat.probes</code>: <code>[1, 2, 4, 10, 20, 40, 100]</code></td></tr><tr><td>HNSW</td><td><code>hnsw.ef_search</code>: <code>[10, 20, 40, 80, 120, 200, 400, 800]</code></td></tr></tbody></table><p>Finally, the test results below will show the recall target (e.g. <code>0.90</code> or <code>90%</code>). The results are shown at the threshold that each test passed that recall level (if it passed that recall level). I could probably have fine tuned this further to find the exact <code>hnsw.ef_search</code> value where the test crossed the threshold, which would give a more accurate representation of the performance characteristics at a recall target, but again, the main goal is to show the growth and growth areas of pgvector over the past year.</p><p>And now it&rsquo;s time for&mldr;</p><h2 id=the-150x-pgvector-speedup>The 150x pgvector speedup</h2><p>For the first test, we&rsquo;ll review the results from the <code>dbpedia-openai-1000k-angular</code> benchmark at 99% recall. The results are below:</p><h3 id=dbpedia-openai-1000k-angular--99-recall-on-a-r7gd16xlarge><code>dbpedia-openai-1000k-angular</code> @ 99% recall on a r7gd.16xlarge</h3><table><thead><tr><th>Test</th><th>Recall</th><th>Single Connection Throughput (QPS)</th><th>QPS Speedup</th><th>p99 Latency (ms)</th><th>p99 Speedup</th><th>Index Build (s)</th><th>Index Build Speedup</th><th>Index Size (GiB)</th><th>Size Improvement</th></tr></thead><tbody><tr><td>r7gd.041</td><td>0.994</td><td>8</td><td>1</td><td>150.16</td><td>1</td><td>474</td><td>16</td><td>7.56</td><td>1</td></tr><tr><td>r7gd.044</td><td>0.994</td><td>8</td><td>1</td><td>155.25</td><td>1</td><td>476</td><td>16</td><td>7.56</td><td>1</td></tr><tr><td>r7gd.050</td><td>0.993</td><td>243</td><td>30.4</td><td>5.74</td><td>27</td><td>7479</td><td>1</td><td>7.55</td><td>1</td></tr><tr><td>r7gd.051</td><td>0.992</td><td>247</td><td>30.9</td><td>5.67</td><td>27.4</td><td>5088</td><td>2</td><td>7.55</td><td>1</td></tr><tr><td>r7gd.060</td><td>0.992</td><td>252</td><td>31.5</td><td>5.52</td><td>28.1</td><td>253</td><td>30</td><td>7.55</td><td>1</td></tr><tr><td>r7gd.062</td><td>0.992</td><td>253</td><td>31.6</td><td>5.54</td><td>28</td><td>252</td><td>30</td><td>7.55</td><td>1</td></tr><tr><td>r7gd.070</td><td>0.992</td><td>253</td><td>31.6</td><td>5.51</td><td>28.2</td><td>250</td><td>30</td><td>7.55</td><td>1</td></tr><tr><td>r7gd.070.fp16</td><td>0.993</td><td>263</td><td>32.9</td><td>5.3</td><td>29.3</td><td>146</td><td>51</td><td>3.78</td><td>2</td></tr><tr><td>r7gd.070.bq-hamming-rerank</td><td>0.99</td><td>236</td><td>29.5</td><td>5.4</td><td>28.8</td><td>49</td><td>153</td><td>0.46</td><td>16.4</td></tr><tr><td>r7gd.070.bq-jaccard-rerank</td><td>0.99</td><td>234</td><td>29.3</td><td>5.38</td><td>28.9</td><td>50</td><td>150</td><td>0.46</td><td>16.4</td></tr></tbody></table><p>And there it is: between pgvector 0.5.0 (where HNSW was introduced) and pgvector 0.7.0, we see that we can get a 150x speedup in the index build time when we use the &ldquo;<a href=/post/postgres/pgvector-scalar-binary-quantization/>binary quantization</a>&rdquo; methods. Note that <a href=/post/postgres/pgvector-scalar-binary-quantization/>we can&rsquo;t always use binary quantization</a> with our data, but we can see we can that scalar quantization to 2-byte floats show over a 50x speedup from the initial HNSW implementation in pgvector 0.5.0. A lot of this speedup is attributed to the use of parallel workers (in this case, 64) during the index build process. For fun, here&rsquo;s how this looks in a bar chart:</p><p><img src=/images/pgvector-150x-r7gd-dbpedia.png alt=pgvector-150x-r7gd-dbpedia.png></p><p>(Note: I do chuckle a bit, as it reminds of a time I fixed a query I wrote to get a 100x speedup. It was a recursive query, but I used <code>UNION ALL</code> when instead I wanted <code>UNION</code>. Unlike my goofy mistake, this I do take this work in pgvector to be a bona fide speedup due to all of the improvements in the pgvector implementation).</p><p>Additionally, we see that the addition of HNSW allows us to get a 30x QPS boost and an almost 30x p99 latency boost over IVFFlat at 99% recall. Queries were executed serially; we&rsquo;d need to run additional tests to see how pgvector scales with client concurrently querying the data.</p><h3 id=dbpedia-openai-1000k-angular--99-recall-on-a-r7i16xlarge><code>dbpedia-openai-1000k-angular</code> @ 99% recall on a r7i.16xlarge</h3><p>Different CPU families can impact the results from a test based upon the availability of acceleration instructions (e.g. SIMD). pgvector 0.7.0 added support for SIMD disaptching functions on x86-64 architecture, so it&rsquo;s important to test what impact this has on our test runs. For these tests, I used an Ubuntu 22.04, with the pgvector code compiled with gcc 12.3 and clang-15, and am showing the results from the <code>dbpedia-openai-1000k-angular</code> benchmark at 99% recall:</p><table><thead><tr><th>Test</th><th>Recall</th><th>Single Connection Throughput (QPS)</th><th>QPS Speedup</th><th>p99 Latency (ms)</th><th>p99 Speedup</th><th>Index Build (s)</th><th>Index Build Speedup</th><th>Index Size (GiB)</th><th>Size Improvement</th></tr></thead><tbody><tr><td>r7i.041</td><td>0.994</td><td>8</td><td>1</td><td>153.01</td><td>1</td><td>496</td><td>15.0</td><td>7.56</td><td>1</td></tr><tr><td>r7i.044</td><td>0.994</td><td>8</td><td>1</td><td>156.58</td><td>1</td><td>494</td><td>15.1</td><td>7.56</td><td>1</td></tr><tr><td>r7i.050</td><td>0.992</td><td>255</td><td>31.9</td><td>5.42</td><td>28.9</td><td>7443</td><td>1.0</td><td>7.55</td><td>1</td></tr><tr><td>r7i.051</td><td>0.992</td><td>245</td><td>30.6</td><td>5.66</td><td>27.7</td><td>5201</td><td>1.4</td><td>7.55</td><td>1</td></tr><tr><td>r7i.060</td><td>0.992</td><td>261</td><td>32.6</td><td>5.28</td><td>29.7</td><td>773</td><td>9.6</td><td>7.55</td><td>1</td></tr><tr><td>r7i.062</td><td>0.992</td><td>265</td><td>33.1</td><td>5.22</td><td>30.0</td><td>382</td><td>19.5</td><td>7.55</td><td>1</td></tr><tr><td>r7i.070</td><td>0.993</td><td>255</td><td>31.9</td><td>5.40</td><td>29.0</td><td>388</td><td>19.2</td><td>7.55</td><td>1</td></tr><tr><td>r7i.070.fp16</td><td>0.993</td><td>282</td><td>35.3</td><td>4.87</td><td>32.2</td><td>227</td><td>32.8</td><td>3.78</td><td>2</td></tr><tr><td>r7i.070.bq-hamming-rerank</td><td>0.99</td><td>269</td><td>33.6</td><td>4.78</td><td>32.8</td><td>64</td><td>116.3</td><td>0.46</td><td>16.4</td></tr><tr><td>r7i.070.bq-jaccard-rerank</td><td>0.99</td><td>267</td><td>33.4</td><td>4.77</td><td>32.8</td><td>66</td><td>112.8</td><td>0.46</td><td>16.4</td></tr></tbody></table><p>Again, we see a 100x+ speedup in index build time when using the &ldquo;<a href=/post/postgres/pgvector-scalar-binary-quantization/>binary quantization</a>&rdquo; methods, and comparable performance results overall to what we had with the r7gd family. We can also see a more than 30x improvement in both throughput and latency as well. Here is a chart that shows how the index build times have decreased on the r7i:</p><p><img src=/images/pgvector-150x-r7gd-dbpedia.png alt=pgvector-150x-r7i-dbpedia.png></p><p>(I&rsquo;ll note here I really need to level up my matplotlib skills; likely Excel too, as it was taking me awhile to get the data charted there. Anyway, this is all the charting I&rsquo;m doing in this blog post).</p><p>As explored in the previous blog post on <a href=post/postgres/pgvector-scalar-binary-quantization/>scalar and binary quantization</a>, we can&rsquo;t always use binary quantization and achieve our recall target due to lack of bit diversity in the indexed vectors. We saw this with both the <code>sift-128-euclidean</code> and <code>gist-960-euclidean</code> datasets. However, both still have nice speedups over the course of the year.</p><p>Below are the results from the <code>sift-128-euclidean</code> benchmark @ 99% recall on both architectures:</p><h3 id=sift-128-euclidean--99-recall-on-a-r7gd16xlarge><code>sift-128-euclidean</code> @ 99% recall on a r7gd.16xlarge</h3><table><thead><tr><th>Test</th><th>Recall</th><th>Single Connection Throughput (QPS)</th><th>QPS Speedup</th><th>p99 Latency (ms)</th><th>p99 Speedup</th><th>Index Build (s)</th><th>Index Build Speedup</th><th>Index Size (GiB)</th><th>Size Improvement</th></tr></thead><tbody><tr><td>r7gd.041</td><td>0.999</td><td>33</td><td>1.0</td><td>44.05</td><td>1.0</td><td>58</td><td>41.6</td><td>0.51</td><td>1.5</td></tr><tr><td>r7gd.044</td><td>0.999</td><td>33</td><td>1.0</td><td>42.39</td><td>1.0</td><td>59</td><td>40.9</td><td>0.51</td><td>1.5</td></tr><tr><td>r7gd.050</td><td>0.994</td><td>432</td><td>13.1</td><td>2.98</td><td>14.8</td><td>2411</td><td>1.0</td><td>0.76</td><td>1.0</td></tr><tr><td>r7gd.051</td><td>0.994</td><td>432</td><td>13.1</td><td>2.98</td><td>14.8</td><td>1933</td><td>1.2</td><td>0.76</td><td>1.0</td></tr><tr><td>r7gd.060</td><td>0.994</td><td>453</td><td>13.7</td><td>2.84</td><td>15.5</td><td>67</td><td>36.0</td><td>0.76</td><td>1.0</td></tr><tr><td>r7gd.062</td><td>0.994</td><td>458</td><td>13.9</td><td>2.81</td><td>15.7</td><td>57</td><td>42.3</td><td>0.76</td><td>1.0</td></tr><tr><td>r7gd.070</td><td>0.994</td><td>487</td><td>14.8</td><td>2.65</td><td>16.6</td><td>56</td><td>43.1</td><td>0.76</td><td>1.0</td></tr><tr><td>r7gd.070.fp16</td><td>0.994</td><td>482</td><td>14.6</td><td>2.68</td><td>16.4</td><td>48</td><td>50.2</td><td>0.52</td><td>1.5</td></tr></tbody></table><h3 id=sift-128-euclidean--99-recall-on-a-r7i16xlarge><code>sift-128-euclidean</code> @ 99% recall on a r7i.16xlarge</h3><table><thead><tr><th>Test</th><th>Recall</th><th>Single Connection Throughput (QPS)</th><th>QPS Speedup</th><th>p99 Latency (ms)</th><th>p99 Speedup</th><th>Index Build (s)</th><th>Index Build Speedup</th><th>Index Size (GiB)</th><th>Size Improvement</th></tr></thead><tbody><tr><td>r7i.041</td><td>0.999</td><td>31</td><td>1.1</td><td>48.57</td><td>1.1</td><td>43</td><td>51.3</td><td>0.51</td><td>1.5</td></tr><tr><td>r7i.044</td><td>0.999</td><td>29</td><td>1.0</td><td>51.34</td><td>1.0</td><td>44</td><td>50.2</td><td>0.51</td><td>1.5</td></tr><tr><td>r7i.050</td><td>0.994</td><td>436</td><td>15.0</td><td>2.96</td><td>17.3</td><td>2208</td><td>1.0</td><td>0.76</td><td>1.0</td></tr><tr><td>r7i.051</td><td>0.994</td><td>426</td><td>14.7</td><td>3.06</td><td>16.8</td><td>1722</td><td>1.3</td><td>0.76</td><td>1.0</td></tr><tr><td>r7i.060</td><td>0.994</td><td>503</td><td>17.3</td><td>2.57</td><td>20.0</td><td>581</td><td>3.8</td><td>0.76</td><td>1.0</td></tr><tr><td>r7i.062</td><td>0.994</td><td>497</td><td>17.1</td><td>2.57</td><td>20.0</td><td>74</td><td>29.8</td><td>0.76</td><td>1.0</td></tr><tr><td>r7i.070</td><td>0.994</td><td>492</td><td>17.0</td><td>2.60</td><td>19.7</td><td>74</td><td>29.8</td><td>0.76</td><td>1.0</td></tr><tr><td>r7i.070.fp16</td><td>0.994</td><td>544</td><td>18.8</td><td>2.36</td><td>21.8</td><td>62</td><td>35.6</td><td>0.52</td><td>1.5</td></tr></tbody></table><p>Across the board, there are some nice speedups, including the 50x index build time improvement for the quantized <code>halfvec</code> test (<code>r7gd.070.fp16</code>), similar to the <code>dbpedia-openai-1000k-angular</code> test.</p><p>Let&rsquo;s take a quick look at the <code>gist-960-euclidean</code> data. With the previous tests, we looked at the results targeting 99% recall, as the QPS/p99 speedups were more pronounced with those. However, <code>gist-960-euclidean</code> tends to be particularly challenging to get good throughput/performance results at high recall (though with binary quantization, I can get over 6,000 QPS at 0% recall!), and interestingly I observed the best speedups at 90% recall.</p><h3 id=gist-960-euclidean--90-recall-on-a-r7gd16xlarge><code>gist-960-euclidean</code> @ 90% recall on a r7gd.16xlarge</h3><table><thead><tr><th>Test</th><th>Recall</th><th>Single Connection Throughput (QPS)</th><th>QPS Speedup</th><th>p99 Latency (ms)</th><th>p99 Speedup</th><th>Index Build (s)</th><th>Index Build Speedup</th><th>Index Size (GiB)</th><th>Size Improvement</th></tr></thead><tbody><tr><td>r7gd.041</td><td>0.965</td><td>13</td><td>1.0</td><td>128.91</td><td>1.0</td><td>300</td><td>22.6</td><td>3.82</td><td>2.0</td></tr><tr><td>r7gd.044</td><td>0.968</td><td>14</td><td>1.1</td><td>123.66</td><td>1.0</td><td>297</td><td>22.9</td><td>3.82</td><td>2.0</td></tr><tr><td>r7gd.050</td><td>0.923</td><td>215</td><td>16.5</td><td>5.53</td><td>23.3</td><td>6787</td><td>1.0</td><td>7.50</td><td>1.0</td></tr><tr><td>r7gd.051</td><td>0.924</td><td>215</td><td>16.5</td><td>5.59</td><td>23.1</td><td>4687</td><td>1.4</td><td>7.50</td><td>1.0</td></tr><tr><td>r7gd.060</td><td>0.924</td><td>229</td><td>17.6</td><td>5.16</td><td>25.0</td><td>204</td><td>33.3</td><td>7.50</td><td>1.0</td></tr><tr><td>r7gd.062</td><td>0.923</td><td>224</td><td>17.2</td><td>5.31</td><td>24.3</td><td>198</td><td>34.3</td><td>7.50</td><td>1.0</td></tr><tr><td>r7gd.070</td><td>0.922</td><td>229</td><td>17.6</td><td>5.18</td><td>24.9</td><td>197</td><td>34.5</td><td>7.50</td><td>1.0</td></tr><tr><td>r7gd.070.fp16</td><td>0.921</td><td>248</td><td>19.1</td><td>4.83</td><td>26.7</td><td>137</td><td>49.5</td><td>2.50</td><td>3.0</td></tr></tbody></table><h3 id=gist-960-euclidean--90-recall-on-a-r7i16xlarge><code>gist-960-euclidean</code> @ 90% recall on a r7i.16xlarge</h3><table><thead><tr><th>Test</th><th>Recall</th><th>Single Connection Throughput (QPS)</th><th>QPS Speedup</th><th>p99 Latency (ms)</th><th>p99 Speedup</th><th>Index Build (s)</th><th>Index Build Speedup</th><th>Index Size (GiB)</th><th>Size Improvement</th></tr></thead><tbody><tr><td>r7i.041</td><td>0.966</td><td>16</td><td>1.1</td><td>111.47</td><td>1.1</td><td>282</td><td>22.2</td><td>3.82</td><td>2.0</td></tr><tr><td>r7i.044</td><td>0.965</td><td>15</td><td>1.0</td><td>120.90</td><td>1.0</td><td>289</td><td>21.7</td><td>3.82</td><td>2.0</td></tr><tr><td>r7i.050</td><td>0.923</td><td>226</td><td>15.1</td><td>5.20</td><td>23.3</td><td>6273</td><td>1.0</td><td>7.50</td><td>1.0</td></tr><tr><td>r7i.051</td><td>0.925</td><td>228</td><td>15.2</td><td>5.26</td><td>23.0</td><td>4212</td><td>1.5</td><td>7.50</td><td>1.0</td></tr><tr><td>r7i.060</td><td>0.924</td><td>246</td><td>16.4</td><td>4.84</td><td>25.0</td><td>1109</td><td>5.7</td><td>7.50</td><td>1.0</td></tr><tr><td>r7i.062</td><td>0.923</td><td>245</td><td>16.3</td><td>4.88</td><td>24.8</td><td>301</td><td>20.8</td><td>7.50</td><td>1.0</td></tr><tr><td>r7i.070</td><td>0.924</td><td>238</td><td>15.9</td><td>4.97</td><td>24.3</td><td>295</td><td>21.3</td><td>7.50</td><td>1.0</td></tr><tr><td>r7i.070.fp16</td><td>0.921</td><td>271</td><td>18.1</td><td>4.33</td><td>27.9</td><td>180</td><td>34.9</td><td>2.50</td><td>3.0</td></tr></tbody></table><p>Again, we can see the effects of parallelism on speeding up the HNSW builds, as well as the effects on shrinking the index size by using 2-byte floats. Also, similar to the <code>sift-128-euclidean</code> test, we&rsquo;re unable to use binary quantization to achieve 90% recall.</p><p>For completeness, here are a few more sets of results. I chose the &ldquo;recall&rdquo; values to optimize for where I saw the biggest performance gains:</p><h3 id=glove-25-angular--99-recall-on-a-r7gd16xlarge><code>glove-25-angular</code> @ 99% recall on a r7gd.16xlarge</h3><table><thead><tr><th>Test</th><th>Recall</th><th>Single Connection Throughput (QPS)</th><th>QPS Speedup</th><th>p99 Latency (ms)</th><th>p99 Speedup</th><th>Index Build (s)</th><th>Index Build Speedup</th><th>Index Size (GiB)</th><th>Size Improvement</th></tr></thead><tbody><tr><td>r7gd.041</td><td>0.997</td><td>26</td><td>1.0</td><td>53.50</td><td>1.0</td><td>31</td><td>81.9</td><td>0.14</td><td>3.2</td></tr><tr><td>r7gd.044</td><td>0.997</td><td>26</td><td>1.0</td><td>53.98</td><td>1.0</td><td>33</td><td>76.9</td><td>0.14</td><td>3.2</td></tr><tr><td>r7gd.050</td><td>0.995</td><td>493</td><td>19.0</td><td>2.64</td><td>20.4</td><td>2538</td><td>1.0</td><td>0.45</td><td>1.0</td></tr><tr><td>r7gd.051</td><td>0.995</td><td>495</td><td>19.0</td><td>2.64</td><td>20.4</td><td>1922</td><td>1.3</td><td>0.45</td><td>1.0</td></tr><tr><td>r7gd.060</td><td>0.995</td><td>514</td><td>19.8</td><td>2.55</td><td>21.2</td><td>53</td><td>47.9</td><td>0.45</td><td>1.0</td></tr><tr><td>r7gd.062</td><td>0.995</td><td>470</td><td>18.1</td><td>2.79</td><td>19.3</td><td>49</td><td>51.8</td><td>0.45</td><td>1.0</td></tr><tr><td>r7gd.070</td><td>0.995</td><td>522</td><td>20.1</td><td>2.50</td><td>21.6</td><td>48</td><td>52.9</td><td>0.45</td><td>1.0</td></tr><tr><td>r7gd.070.fp16</td><td>0.995</td><td>521</td><td>20.0</td><td>2.51</td><td>21.5</td><td>48</td><td>52.9</td><td>0.40</td><td>1.1</td></tr></tbody></table><h3 id=glove-25-angular--99-recall-on-a-r7i16xlarge><code>glove-25-angular</code> @ 99% recall on a r7i.16xlarge</h3><table><thead><tr><th>Test</th><th>Recall</th><th>Single Connection Throughput (QPS)</th><th>QPS Speedup</th><th>p99 Latency (ms)</th><th>p99 Speedup</th><th>Index Build (s)</th><th>Index Build Speedup</th><th>Index Size (GiB)</th><th>Size Improvement</th></tr></thead><tbody><tr><td>r7i.041</td><td>0.997</td><td>23</td><td>1.0</td><td>59.08</td><td>1.0</td><td>38</td><td>63.5</td><td>0.14</td><td>3.2</td></tr><tr><td>r7i.044</td><td>0.997</td><td>24</td><td>1.0</td><td>59.59</td><td>1.0</td><td>30</td><td>80.5</td><td>0.14</td><td>3.2</td></tr><tr><td>r7i.050</td><td>0.995</td><td>539</td><td>23.4</td><td>2.41</td><td>24.7</td><td>2414</td><td>1.0</td><td>0.45</td><td>1.0</td></tr><tr><td>r7i.051</td><td>0.995</td><td>545</td><td>23.7</td><td>2.39</td><td>24.9</td><td>1827</td><td>1.3</td><td>0.45</td><td>1.0</td></tr><tr><td>r7i.060</td><td>0.995</td><td>557</td><td>24.2</td><td>2.34</td><td>25.5</td><td>471</td><td>5.1</td><td>0.45</td><td>1.0</td></tr><tr><td>r7i.062</td><td>0.995</td><td>574</td><td>25.0</td><td>2.27</td><td>26.3</td><td>64</td><td>37.7</td><td>0.45</td><td>1.0</td></tr><tr><td>r7i.070</td><td>0.995</td><td>569</td><td>24.7</td><td>2.28</td><td>26.1</td><td>63</td><td>38.3</td><td>0.45</td><td>1.0</td></tr><tr><td>r7i.070.fp16</td><td>0.995</td><td>569</td><td>24.7</td><td>2.28</td><td>26.1</td><td>60</td><td>40.2</td><td>0.40</td><td>1.1</td></tr></tbody></table><p>The interesting thing about both of these tests is that the IVFFlat index builds are both faster and smaller than the HNSW index builds - and that is without using any parallelism during the IVFFlat build. However, the HNSW numbers show a sigificant boost in throughput and p99 latency.</p><p>Finally, here are the results from the <code>glove-100-angular</code> test. In my test, I wasn&rsquo;t able to get much above 95% recall. I would likely need to increase the <code>m</code> build parameter to get towards 99% recall, but as mentioned earlier, the goal of this testing was primarily to see how pgvector has improved over the course of the year and not optimize parameters for a particular dataset:</p><h3 id=glove-100-angular--95-recall-on-a-r7gd16xlarge><code>glove-100-angular</code> @ 95% recall on a r7gd.16xlarge</h3><table><thead><tr><th>Test</th><th>Recall</th><th>Single Connection Throughput (QPS)</th><th>QPS Speedup</th><th>p99 Latency (ms)</th><th>p99 Speedup</th><th>Index Build (s)</th><th>Index Build Speedup</th><th>Index Size (GiB)</th><th>Size Improvement</th></tr></thead><tbody><tr><td>r7gd.041</td><td>0.963</td><td>29</td><td>1.0</td><td>47.10</td><td>1.0</td><td>68</td><td>58.0</td><td>0.48</td><td>1.7</td></tr><tr><td>r7gd.044</td><td>0.963</td><td>29</td><td>1.0</td><td>46.20</td><td>1.0</td><td>69</td><td>57.1</td><td>0.48</td><td>1.7</td></tr><tr><td>r7gd.050</td><td>0.965</td><td>65</td><td>2.2</td><td>21.05</td><td>2.2</td><td>3941</td><td>1.0</td><td>0.82</td><td>1.0</td></tr><tr><td>r7gd.051</td><td>0.965</td><td>65</td><td>2.2</td><td>20.90</td><td>2.3</td><td>2965</td><td>1.3</td><td>0.82</td><td>1.0</td></tr><tr><td>r7gd.060</td><td>0.965</td><td>63</td><td>2.2</td><td>21.22</td><td>2.2</td><td>83</td><td>47.5</td><td>0.82</td><td>1.0</td></tr><tr><td>r7gd.062</td><td>0.965</td><td>62</td><td>2.1</td><td>21.68</td><td>2.2</td><td>78</td><td>50.5</td><td>0.82</td><td>1.0</td></tr><tr><td>r7gd.070</td><td>0.965</td><td>66</td><td>2.3</td><td>20.07</td><td>2.3</td><td>77</td><td>51.2</td><td>0.82</td><td>1.0</td></tr><tr><td>r7gd.070.fp16</td><td>0.965</td><td>67</td><td>2.3</td><td>19.97</td><td>2.4</td><td>68</td><td>58.0</td><td>0.57</td><td>1.4</td></tr></tbody></table><h3 id=glove-100-angular--95-recall-on-a-r7i16xlarge><code>glove-100-angular</code> @ 95% recall on a r7i.16xlarge</h3><table><thead><tr><th>Test</th><th>Recall</th><th>Single Connection Throughput (QPS)</th><th>QPS Speedup</th><th>p99 Latency (ms)</th><th>p99 Speedup</th><th>Index Build (s)</th><th>Index Build Speedup</th><th>Index Size (GiB)</th><th>Size Improvement</th></tr></thead><tbody><tr><td>r7i.041</td><td>0.963</td><td>27</td><td>1.0</td><td>50.43</td><td>1.0</td><td>53</td><td>66.8</td><td>0.48</td><td>1.7</td></tr><tr><td>r7i.044</td><td>0.962</td><td>26</td><td>1.0</td><td>52.13</td><td>1.0</td><td>56</td><td>63.3</td><td>0.48</td><td>1.7</td></tr><tr><td>r7i.050</td><td>0.965</td><td>81</td><td>3.1</td><td>16.70</td><td>3.1</td><td>3543</td><td>1.0</td><td>0.82</td><td>1.0</td></tr><tr><td>r7i.051</td><td>0.965</td><td>82</td><td>3.2</td><td>16.49</td><td>3.2</td><td>2517</td><td>1.4</td><td>0.82</td><td>1.0</td></tr><tr><td>r7i.060</td><td>0.965</td><td>79</td><td>3.0</td><td>16.64</td><td>3.1</td><td>692</td><td>5.1</td><td>0.82</td><td>1.0</td></tr><tr><td>r7i.062</td><td>0.965</td><td>83</td><td>3.2</td><td>15.90</td><td>3.3</td><td>98</td><td>36.2</td><td>0.82</td><td>1.0</td></tr><tr><td>r7i.070</td><td>0.965</td><td>81</td><td>3.1</td><td>16.27</td><td>3.2</td><td>95</td><td>37.3</td><td>0.82</td><td>1.0</td></tr><tr><td>r7i.070.fp16</td><td>0.965</td><td>86</td><td>3.3</td><td>15.27</td><td>3.4</td><td>84</td><td>42.2</td><td>0.57</td><td>1.4</td></tr></tbody></table><p>Overall with <code>glove-100-angular</code> on the selected build parameters, there are definite speedups on build times for HNSW indexes, and we do see improvements in throughput/latency. For this specific dataset, I&rsquo;d recommend rerunning it with different HNSW build parameters to see if we can improve query performance numbers at higher levels of recall, but that&rsquo;s an experiment for another day.</p><h2 id=where-do-we-go-from-here>Where do we go from here?</h2><p>It&rsquo;s been quite a year for pgvector on many fronts, not to say the least the many people who are already building amazing apps with it today! A &ldquo;billion-scale&rdquo; vector storage problem is attainable with pgvector today, much of this attributed to the work of the last year. And while I can&rsquo;t say enough about the work <a href=https://github.com/ankane>Andrew Kane</a> has done on pgvector, I do want to give mentions to <a href=https://github.com/hlinnaka>Heikki Linnakangas</a>, <a href=https://github.com/nathan-bossart>Nathan Bossart</a>, <a href=https://github.com/pashkinelfe>Pavel Borisov</a>, and <a href=https://github.com/aytekinar>Arda Aytekin</a> who all made contributions to improve pgvector performance (and apologies if I missed someone).</p><p>However, much like the <a href=https://jkatz05.com/post/postgres/postgresql-2024/><em>almost</em> 40-year-old database PostgreSQL</a>, there are still ways pgvector can continue to grow. I&rsquo;m going to talk more in depth about some <a href=https://www.pgevents.ca/events/pgconfdev2024/schedule/session/1-vectors-how-to-better-support-a-nasty-data-type-in-postgresql/>longer term goals to better support vector workloads with pgvector and PostgreSQL</a> at <a href=https://2024.pgconf.dev/>PGConf.dev 2024</a>, but I&rsquo;ll give a brief preview here.</p><p>Over the past year, pgvector has made significant gains across the board in index build times, index sizes, throughput, and latency, particularly on vector queries over an entire vector data set. Simplifying <a href="https://github.com/pgvector/pgvector/?tab=readme-ov-file#filtering">filtering</a> (aka the <code>WHERE</code> clause) - pgvector and PostgreSQL already support this, but there are some areas we can make it easier and more efficient. Additionally, there are other search patterns that are gaining popularity, such as &ldquo;hybrid search&rdquo; like using simultaneously vector similarity search and fulltext search to return results. Again, this is something already supported by PostgreSQL natively, but there are areas we can simplify this process with pgvector. We&rsquo;re seeing more work in pgvector to support hardware acceleration; this combined with further optimizations on And finally, there are some areas of PostgresQL we can prove to better support <a href=/post/postgres/distributed-pgvector/>distributed pgvector workloads</a>, but I&rsquo;ll still emphasize that most workloads that involve PostgreSQL and pgvector will scale vertically (which means showing more concurrency testing!).</p><p>We&rsquo;ll also have to see how vector search workloads evolve, as that will also dictate what new features we&rsquo;ll see in pgvector. Please keep giving feedback on what you&rsquo;re building with pgvector and how your experience is - as that is how we can continue to make the project better!</p></div><div class=post-footer><div class=info><span class=separator><a class=tag href=/tags/postgres/>postgres</a><a class=tag href=/tags/postgresql/>postgresql</a><a class=tag href=/tags/pgvector/>pgvector</a></span></div></div></div></div></div></main></div><footer class="footer footer--base"><div class=by_farbox><ul class=footer__list><li class=footer__item>&copy;
2024 Jonathan Katz</li><li class=footer__item><a href=https://github.com/lxndrblz/anatole target=_blank rel="noopener noreferrer" title>Powered by the Anatole Hugo Theme</a></li><li class=footer__item><a href title>Opinions are my own.</a></li></ul></div></footer><script type=text/javascript src=/js/medium-zoom.min.44288fd315b6cda68c1f4743caad56535c0f81a5b5a672f385e82b3896575c1d.js integrity="sha256-RCiP0xW2zaaMH0dDyq1WU1wPgaW1pnLzhegrOJZXXB0=" crossorigin=anonymous></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NNLWC1035Y"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NNLWC1035Y")</script></body></html>